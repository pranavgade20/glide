{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [
    {
     "data": {
      "text/plain": "<All keys matched successfully>"
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append(\"./reference\")\n",
    "import torch as t\n",
    "from reference.glide_text2im.download import load_checkpoint\n",
    "from reference.glide_text2im.model_creation import (\n",
    "    create_model_and_diffusion,\n",
    "    model_and_diffusion_defaults,\n",
    ")\n",
    "\n",
    "device = t.device('cpu')\n",
    "options = model_and_diffusion_defaults()\n",
    "options['timestep_respacing'] = '100'\n",
    "ref_model, diffusion = create_model_and_diffusion(**options)\n",
    "ref_model.load_state_dict(load_checkpoint('base', device))\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "outputs": [],
   "source": [
    "from unet import Text2Im\n",
    "from reference.glide_text2im.tokenizer.bpe import get_encoder"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[768, 768, 768, 576, 576, 576, 576, 384, 384, 384, 384, 192, 192, 192, 192, 192]\n",
      "1536\n",
      "1536\n",
      "1536\n",
      "1344\n",
      "1152\n",
      "1152\n",
      "1152\n",
      "960\n",
      "768\n",
      "768\n",
      "768\n",
      "576\n",
      "384\n",
      "384\n",
      "384\n",
      "384\n"
     ]
    }
   ],
   "source": [
    "tokenizer = get_encoder()\n",
    "model = Text2Im(tokenizer)\n",
    "# model"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "783 783\n"
     ]
    },
    {
     "data": {
      "text/plain": "[('positional_embedding', 'positional_embedding'),\n ('padding_embedding', 'padding_embedding'),\n ('time_embed.0.weight', 'time_embed.0.weight'),\n ('time_embed.0.bias', 'time_embed.0.bias'),\n ('time_embed.2.weight', 'time_embed.2.weight'),\n ('time_embed.2.bias', 'time_embed.2.bias'),\n ('in_layers.0.weight', 'input_blocks.0.0.weight'),\n ('in_layers.0.bias', 'input_blocks.0.0.bias'),\n ('in_layers.1.in_layers.0.weight', 'input_blocks.1.0.in_layers.0.weight'),\n ('in_layers.1.in_layers.0.bias', 'input_blocks.1.0.in_layers.0.bias'),\n ('in_layers.1.in_layers.2.weight', 'input_blocks.1.0.in_layers.2.weight'),\n ('in_layers.1.in_layers.2.bias', 'input_blocks.1.0.in_layers.2.bias'),\n ('in_layers.1.emb_layers.1.weight', 'input_blocks.1.0.emb_layers.1.weight'),\n ('in_layers.1.emb_layers.1.bias', 'input_blocks.1.0.emb_layers.1.bias'),\n ('in_layers.1.group_norm.weight', 'input_blocks.1.0.out_layers.0.weight'),\n ('in_layers.1.group_norm.bias', 'input_blocks.1.0.out_layers.0.bias'),\n ('in_layers.1.out_layers.2.weight', 'input_blocks.1.0.out_layers.3.weight'),\n ('in_layers.1.out_layers.2.bias', 'input_blocks.1.0.out_layers.3.bias'),\n ('in_layers.2.in_layers.0.weight', 'input_blocks.2.0.in_layers.0.weight'),\n ('in_layers.2.in_layers.0.bias', 'input_blocks.2.0.in_layers.0.bias'),\n ('in_layers.2.in_layers.2.weight', 'input_blocks.2.0.in_layers.2.weight'),\n ('in_layers.2.in_layers.2.bias', 'input_blocks.2.0.in_layers.2.bias'),\n ('in_layers.2.emb_layers.1.weight', 'input_blocks.2.0.emb_layers.1.weight'),\n ('in_layers.2.emb_layers.1.bias', 'input_blocks.2.0.emb_layers.1.bias'),\n ('in_layers.2.group_norm.weight', 'input_blocks.2.0.out_layers.0.weight'),\n ('in_layers.2.group_norm.bias', 'input_blocks.2.0.out_layers.0.bias'),\n ('in_layers.2.out_layers.2.weight', 'input_blocks.2.0.out_layers.3.weight'),\n ('in_layers.2.out_layers.2.bias', 'input_blocks.2.0.out_layers.3.bias'),\n ('in_layers.3.in_layers.0.weight', 'input_blocks.3.0.in_layers.0.weight'),\n ('in_layers.3.in_layers.0.bias', 'input_blocks.3.0.in_layers.0.bias'),\n ('in_layers.3.in_layers.2.weight', 'input_blocks.3.0.in_layers.2.weight'),\n ('in_layers.3.in_layers.2.bias', 'input_blocks.3.0.in_layers.2.bias'),\n ('in_layers.3.emb_layers.1.weight', 'input_blocks.3.0.emb_layers.1.weight'),\n ('in_layers.3.emb_layers.1.bias', 'input_blocks.3.0.emb_layers.1.bias'),\n ('in_layers.3.group_norm.weight', 'input_blocks.3.0.out_layers.0.weight'),\n ('in_layers.3.group_norm.bias', 'input_blocks.3.0.out_layers.0.bias'),\n ('in_layers.3.out_layers.2.weight', 'input_blocks.3.0.out_layers.3.weight'),\n ('in_layers.3.out_layers.2.bias', 'input_blocks.3.0.out_layers.3.bias'),\n ('in_layers.4.in_layers.0.weight', 'input_blocks.4.0.in_layers.0.weight'),\n ('in_layers.4.in_layers.0.bias', 'input_blocks.4.0.in_layers.0.bias'),\n ('in_layers.4.in_layers.2.weight', 'input_blocks.4.0.in_layers.2.weight'),\n ('in_layers.4.in_layers.2.bias', 'input_blocks.4.0.in_layers.2.bias'),\n ('in_layers.4.emb_layers.1.weight', 'input_blocks.4.0.emb_layers.1.weight'),\n ('in_layers.4.emb_layers.1.bias', 'input_blocks.4.0.emb_layers.1.bias'),\n ('in_layers.4.group_norm.weight', 'input_blocks.4.0.out_layers.0.weight'),\n ('in_layers.4.group_norm.bias', 'input_blocks.4.0.out_layers.0.bias'),\n ('in_layers.4.out_layers.2.weight', 'input_blocks.4.0.out_layers.3.weight'),\n ('in_layers.4.out_layers.2.bias', 'input_blocks.4.0.out_layers.3.bias'),\n ('in_layers.5.in_layers.0.weight', 'input_blocks.5.0.in_layers.0.weight'),\n ('in_layers.5.in_layers.0.bias', 'input_blocks.5.0.in_layers.0.bias'),\n ('in_layers.5.in_layers.2.weight', 'input_blocks.5.0.in_layers.2.weight'),\n ('in_layers.5.in_layers.2.bias', 'input_blocks.5.0.in_layers.2.bias'),\n ('in_layers.5.emb_layers.1.weight', 'input_blocks.5.0.emb_layers.1.weight'),\n ('in_layers.5.emb_layers.1.bias', 'input_blocks.5.0.emb_layers.1.bias'),\n ('in_layers.5.group_norm.weight', 'input_blocks.5.0.out_layers.0.weight'),\n ('in_layers.5.group_norm.bias', 'input_blocks.5.0.out_layers.0.bias'),\n ('in_layers.5.out_layers.2.weight', 'input_blocks.5.0.out_layers.3.weight'),\n ('in_layers.5.out_layers.2.bias', 'input_blocks.5.0.out_layers.3.bias'),\n ('in_layers.5.skip_connection.weight',\n  'input_blocks.5.0.skip_connection.weight'),\n ('in_layers.5.skip_connection.bias', 'input_blocks.5.0.skip_connection.bias'),\n ('in_layers.5.attention_layer.group_norm.weight',\n  'input_blocks.5.1.norm.weight'),\n ('in_layers.5.attention_layer.group_norm.bias', 'input_blocks.5.1.norm.bias'),\n ('in_layers.5.attention_layer.qkv.weight', 'input_blocks.5.1.qkv.weight'),\n ('in_layers.5.attention_layer.qkv.bias', 'input_blocks.5.1.qkv.bias'),\n ('in_layers.5.attention_layer.encoder_kv.weight',\n  'input_blocks.5.1.encoder_kv.weight'),\n ('in_layers.5.attention_layer.encoder_kv.bias',\n  'input_blocks.5.1.encoder_kv.bias'),\n ('in_layers.5.attention_layer.proj_out.weight',\n  'input_blocks.5.1.proj_out.weight'),\n ('in_layers.5.attention_layer.proj_out.bias',\n  'input_blocks.5.1.proj_out.bias'),\n ('in_layers.6.in_layers.0.weight', 'input_blocks.6.0.in_layers.0.weight'),\n ('in_layers.6.in_layers.0.bias', 'input_blocks.6.0.in_layers.0.bias'),\n ('in_layers.6.in_layers.2.weight', 'input_blocks.6.0.in_layers.2.weight'),\n ('in_layers.6.in_layers.2.bias', 'input_blocks.6.0.in_layers.2.bias'),\n ('in_layers.6.emb_layers.1.weight', 'input_blocks.6.0.emb_layers.1.weight'),\n ('in_layers.6.emb_layers.1.bias', 'input_blocks.6.0.emb_layers.1.bias'),\n ('in_layers.6.group_norm.weight', 'input_blocks.6.0.out_layers.0.weight'),\n ('in_layers.6.group_norm.bias', 'input_blocks.6.0.out_layers.0.bias'),\n ('in_layers.6.out_layers.2.weight', 'input_blocks.6.0.out_layers.3.weight'),\n ('in_layers.6.out_layers.2.bias', 'input_blocks.6.0.out_layers.3.bias'),\n ('in_layers.6.attention_layer.group_norm.weight',\n  'input_blocks.6.1.norm.weight'),\n ('in_layers.6.attention_layer.group_norm.bias', 'input_blocks.6.1.norm.bias'),\n ('in_layers.6.attention_layer.qkv.weight', 'input_blocks.6.1.qkv.weight'),\n ('in_layers.6.attention_layer.qkv.bias', 'input_blocks.6.1.qkv.bias'),\n ('in_layers.6.attention_layer.encoder_kv.weight',\n  'input_blocks.6.1.encoder_kv.weight'),\n ('in_layers.6.attention_layer.encoder_kv.bias',\n  'input_blocks.6.1.encoder_kv.bias'),\n ('in_layers.6.attention_layer.proj_out.weight',\n  'input_blocks.6.1.proj_out.weight'),\n ('in_layers.6.attention_layer.proj_out.bias',\n  'input_blocks.6.1.proj_out.bias'),\n ('in_layers.7.in_layers.0.weight', 'input_blocks.7.0.in_layers.0.weight'),\n ('in_layers.7.in_layers.0.bias', 'input_blocks.7.0.in_layers.0.bias'),\n ('in_layers.7.in_layers.2.weight', 'input_blocks.7.0.in_layers.2.weight'),\n ('in_layers.7.in_layers.2.bias', 'input_blocks.7.0.in_layers.2.bias'),\n ('in_layers.7.emb_layers.1.weight', 'input_blocks.7.0.emb_layers.1.weight'),\n ('in_layers.7.emb_layers.1.bias', 'input_blocks.7.0.emb_layers.1.bias'),\n ('in_layers.7.group_norm.weight', 'input_blocks.7.0.out_layers.0.weight'),\n ('in_layers.7.group_norm.bias', 'input_blocks.7.0.out_layers.0.bias'),\n ('in_layers.7.out_layers.2.weight', 'input_blocks.7.0.out_layers.3.weight'),\n ('in_layers.7.out_layers.2.bias', 'input_blocks.7.0.out_layers.3.bias'),\n ('in_layers.7.attention_layer.group_norm.weight',\n  'input_blocks.7.1.norm.weight'),\n ('in_layers.7.attention_layer.group_norm.bias', 'input_blocks.7.1.norm.bias'),\n ('in_layers.7.attention_layer.qkv.weight', 'input_blocks.7.1.qkv.weight'),\n ('in_layers.7.attention_layer.qkv.bias', 'input_blocks.7.1.qkv.bias'),\n ('in_layers.7.attention_layer.encoder_kv.weight',\n  'input_blocks.7.1.encoder_kv.weight'),\n ('in_layers.7.attention_layer.encoder_kv.bias',\n  'input_blocks.7.1.encoder_kv.bias'),\n ('in_layers.7.attention_layer.proj_out.weight',\n  'input_blocks.7.1.proj_out.weight'),\n ('in_layers.7.attention_layer.proj_out.bias',\n  'input_blocks.7.1.proj_out.bias'),\n ('in_layers.8.in_layers.0.weight', 'input_blocks.8.0.in_layers.0.weight'),\n ('in_layers.8.in_layers.0.bias', 'input_blocks.8.0.in_layers.0.bias'),\n ('in_layers.8.in_layers.2.weight', 'input_blocks.8.0.in_layers.2.weight'),\n ('in_layers.8.in_layers.2.bias', 'input_blocks.8.0.in_layers.2.bias'),\n ('in_layers.8.emb_layers.1.weight', 'input_blocks.8.0.emb_layers.1.weight'),\n ('in_layers.8.emb_layers.1.bias', 'input_blocks.8.0.emb_layers.1.bias'),\n ('in_layers.8.group_norm.weight', 'input_blocks.8.0.out_layers.0.weight'),\n ('in_layers.8.group_norm.bias', 'input_blocks.8.0.out_layers.0.bias'),\n ('in_layers.8.out_layers.2.weight', 'input_blocks.8.0.out_layers.3.weight'),\n ('in_layers.8.out_layers.2.bias', 'input_blocks.8.0.out_layers.3.bias'),\n ('in_layers.9.in_layers.0.weight', 'input_blocks.9.0.in_layers.0.weight'),\n ('in_layers.9.in_layers.0.bias', 'input_blocks.9.0.in_layers.0.bias'),\n ('in_layers.9.in_layers.2.weight', 'input_blocks.9.0.in_layers.2.weight'),\n ('in_layers.9.in_layers.2.bias', 'input_blocks.9.0.in_layers.2.bias'),\n ('in_layers.9.emb_layers.1.weight', 'input_blocks.9.0.emb_layers.1.weight'),\n ('in_layers.9.emb_layers.1.bias', 'input_blocks.9.0.emb_layers.1.bias'),\n ('in_layers.9.group_norm.weight', 'input_blocks.9.0.out_layers.0.weight'),\n ('in_layers.9.group_norm.bias', 'input_blocks.9.0.out_layers.0.bias'),\n ('in_layers.9.out_layers.2.weight', 'input_blocks.9.0.out_layers.3.weight'),\n ('in_layers.9.out_layers.2.bias', 'input_blocks.9.0.out_layers.3.bias'),\n ('in_layers.9.skip_connection.weight',\n  'input_blocks.9.0.skip_connection.weight'),\n ('in_layers.9.skip_connection.bias', 'input_blocks.9.0.skip_connection.bias'),\n ('in_layers.9.attention_layer.group_norm.weight',\n  'input_blocks.9.1.norm.weight'),\n ('in_layers.9.attention_layer.group_norm.bias', 'input_blocks.9.1.norm.bias'),\n ('in_layers.9.attention_layer.qkv.weight', 'input_blocks.9.1.qkv.weight'),\n ('in_layers.9.attention_layer.qkv.bias', 'input_blocks.9.1.qkv.bias'),\n ('in_layers.9.attention_layer.encoder_kv.weight',\n  'input_blocks.9.1.encoder_kv.weight'),\n ('in_layers.9.attention_layer.encoder_kv.bias',\n  'input_blocks.9.1.encoder_kv.bias'),\n ('in_layers.9.attention_layer.proj_out.weight',\n  'input_blocks.9.1.proj_out.weight'),\n ('in_layers.9.attention_layer.proj_out.bias',\n  'input_blocks.9.1.proj_out.bias'),\n ('in_layers.10.in_layers.0.weight', 'input_blocks.10.0.in_layers.0.weight'),\n ('in_layers.10.in_layers.0.bias', 'input_blocks.10.0.in_layers.0.bias'),\n ('in_layers.10.in_layers.2.weight', 'input_blocks.10.0.in_layers.2.weight'),\n ('in_layers.10.in_layers.2.bias', 'input_blocks.10.0.in_layers.2.bias'),\n ('in_layers.10.emb_layers.1.weight', 'input_blocks.10.0.emb_layers.1.weight'),\n ('in_layers.10.emb_layers.1.bias', 'input_blocks.10.0.emb_layers.1.bias'),\n ('in_layers.10.group_norm.weight', 'input_blocks.10.0.out_layers.0.weight'),\n ('in_layers.10.group_norm.bias', 'input_blocks.10.0.out_layers.0.bias'),\n ('in_layers.10.out_layers.2.weight', 'input_blocks.10.0.out_layers.3.weight'),\n ('in_layers.10.out_layers.2.bias', 'input_blocks.10.0.out_layers.3.bias'),\n ('in_layers.10.attention_layer.group_norm.weight',\n  'input_blocks.10.1.norm.weight'),\n ('in_layers.10.attention_layer.group_norm.bias',\n  'input_blocks.10.1.norm.bias'),\n ('in_layers.10.attention_layer.qkv.weight', 'input_blocks.10.1.qkv.weight'),\n ('in_layers.10.attention_layer.qkv.bias', 'input_blocks.10.1.qkv.bias'),\n ('in_layers.10.attention_layer.encoder_kv.weight',\n  'input_blocks.10.1.encoder_kv.weight'),\n ('in_layers.10.attention_layer.encoder_kv.bias',\n  'input_blocks.10.1.encoder_kv.bias'),\n ('in_layers.10.attention_layer.proj_out.weight',\n  'input_blocks.10.1.proj_out.weight'),\n ('in_layers.10.attention_layer.proj_out.bias',\n  'input_blocks.10.1.proj_out.bias'),\n ('in_layers.11.in_layers.0.weight', 'input_blocks.11.0.in_layers.0.weight'),\n ('in_layers.11.in_layers.0.bias', 'input_blocks.11.0.in_layers.0.bias'),\n ('in_layers.11.in_layers.2.weight', 'input_blocks.11.0.in_layers.2.weight'),\n ('in_layers.11.in_layers.2.bias', 'input_blocks.11.0.in_layers.2.bias'),\n ('in_layers.11.emb_layers.1.weight', 'input_blocks.11.0.emb_layers.1.weight'),\n ('in_layers.11.emb_layers.1.bias', 'input_blocks.11.0.emb_layers.1.bias'),\n ('in_layers.11.group_norm.weight', 'input_blocks.11.0.out_layers.0.weight'),\n ('in_layers.11.group_norm.bias', 'input_blocks.11.0.out_layers.0.bias'),\n ('in_layers.11.out_layers.2.weight', 'input_blocks.11.0.out_layers.3.weight'),\n ('in_layers.11.out_layers.2.bias', 'input_blocks.11.0.out_layers.3.bias'),\n ('in_layers.11.attention_layer.group_norm.weight',\n  'input_blocks.11.1.norm.weight'),\n ('in_layers.11.attention_layer.group_norm.bias',\n  'input_blocks.11.1.norm.bias'),\n ('in_layers.11.attention_layer.qkv.weight', 'input_blocks.11.1.qkv.weight'),\n ('in_layers.11.attention_layer.qkv.bias', 'input_blocks.11.1.qkv.bias'),\n ('in_layers.11.attention_layer.encoder_kv.weight',\n  'input_blocks.11.1.encoder_kv.weight'),\n ('in_layers.11.attention_layer.encoder_kv.bias',\n  'input_blocks.11.1.encoder_kv.bias'),\n ('in_layers.11.attention_layer.proj_out.weight',\n  'input_blocks.11.1.proj_out.weight'),\n ('in_layers.11.attention_layer.proj_out.bias',\n  'input_blocks.11.1.proj_out.bias'),\n ('in_layers.12.in_layers.0.weight', 'input_blocks.12.0.in_layers.0.weight'),\n ('in_layers.12.in_layers.0.bias', 'input_blocks.12.0.in_layers.0.bias'),\n ('in_layers.12.in_layers.2.weight', 'input_blocks.12.0.in_layers.2.weight'),\n ('in_layers.12.in_layers.2.bias', 'input_blocks.12.0.in_layers.2.bias'),\n ('in_layers.12.emb_layers.1.weight', 'input_blocks.12.0.emb_layers.1.weight'),\n ('in_layers.12.emb_layers.1.bias', 'input_blocks.12.0.emb_layers.1.bias'),\n ('in_layers.12.group_norm.weight', 'input_blocks.12.0.out_layers.0.weight'),\n ('in_layers.12.group_norm.bias', 'input_blocks.12.0.out_layers.0.bias'),\n ('in_layers.12.out_layers.2.weight', 'input_blocks.12.0.out_layers.3.weight'),\n ('in_layers.12.out_layers.2.bias', 'input_blocks.12.0.out_layers.3.bias'),\n ('in_layers.13.in_layers.0.weight', 'input_blocks.13.0.in_layers.0.weight'),\n ('in_layers.13.in_layers.0.bias', 'input_blocks.13.0.in_layers.0.bias'),\n ('in_layers.13.in_layers.2.weight', 'input_blocks.13.0.in_layers.2.weight'),\n ('in_layers.13.in_layers.2.bias', 'input_blocks.13.0.in_layers.2.bias'),\n ('in_layers.13.emb_layers.1.weight', 'input_blocks.13.0.emb_layers.1.weight'),\n ('in_layers.13.emb_layers.1.bias', 'input_blocks.13.0.emb_layers.1.bias'),\n ('in_layers.13.group_norm.weight', 'input_blocks.13.0.out_layers.0.weight'),\n ('in_layers.13.group_norm.bias', 'input_blocks.13.0.out_layers.0.bias'),\n ('in_layers.13.out_layers.2.weight', 'input_blocks.13.0.out_layers.3.weight'),\n ('in_layers.13.out_layers.2.bias', 'input_blocks.13.0.out_layers.3.bias'),\n ('in_layers.13.skip_connection.weight',\n  'input_blocks.13.0.skip_connection.weight'),\n ('in_layers.13.skip_connection.bias',\n  'input_blocks.13.0.skip_connection.bias'),\n ('in_layers.13.attention_layer.group_norm.weight',\n  'input_blocks.13.1.norm.weight'),\n ('in_layers.13.attention_layer.group_norm.bias',\n  'input_blocks.13.1.norm.bias'),\n ('in_layers.13.attention_layer.qkv.weight', 'input_blocks.13.1.qkv.weight'),\n ('in_layers.13.attention_layer.qkv.bias', 'input_blocks.13.1.qkv.bias'),\n ('in_layers.13.attention_layer.encoder_kv.weight',\n  'input_blocks.13.1.encoder_kv.weight'),\n ('in_layers.13.attention_layer.encoder_kv.bias',\n  'input_blocks.13.1.encoder_kv.bias'),\n ('in_layers.13.attention_layer.proj_out.weight',\n  'input_blocks.13.1.proj_out.weight'),\n ('in_layers.13.attention_layer.proj_out.bias',\n  'input_blocks.13.1.proj_out.bias'),\n ('in_layers.14.in_layers.0.weight', 'input_blocks.14.0.in_layers.0.weight'),\n ('in_layers.14.in_layers.0.bias', 'input_blocks.14.0.in_layers.0.bias'),\n ('in_layers.14.in_layers.2.weight', 'input_blocks.14.0.in_layers.2.weight'),\n ('in_layers.14.in_layers.2.bias', 'input_blocks.14.0.in_layers.2.bias'),\n ('in_layers.14.emb_layers.1.weight', 'input_blocks.14.0.emb_layers.1.weight'),\n ('in_layers.14.emb_layers.1.bias', 'input_blocks.14.0.emb_layers.1.bias'),\n ('in_layers.14.group_norm.weight', 'input_blocks.14.0.out_layers.0.weight'),\n ('in_layers.14.group_norm.bias', 'input_blocks.14.0.out_layers.0.bias'),\n ('in_layers.14.out_layers.2.weight', 'input_blocks.14.0.out_layers.3.weight'),\n ('in_layers.14.out_layers.2.bias', 'input_blocks.14.0.out_layers.3.bias'),\n ('in_layers.14.attention_layer.group_norm.weight',\n  'input_blocks.14.1.norm.weight'),\n ('in_layers.14.attention_layer.group_norm.bias',\n  'input_blocks.14.1.norm.bias'),\n ('in_layers.14.attention_layer.qkv.weight', 'input_blocks.14.1.qkv.weight'),\n ('in_layers.14.attention_layer.qkv.bias', 'input_blocks.14.1.qkv.bias'),\n ('in_layers.14.attention_layer.encoder_kv.weight',\n  'input_blocks.14.1.encoder_kv.weight'),\n ('in_layers.14.attention_layer.encoder_kv.bias',\n  'input_blocks.14.1.encoder_kv.bias'),\n ('in_layers.14.attention_layer.proj_out.weight',\n  'input_blocks.14.1.proj_out.weight'),\n ('in_layers.14.attention_layer.proj_out.bias',\n  'input_blocks.14.1.proj_out.bias'),\n ('in_layers.15.in_layers.0.weight', 'input_blocks.15.0.in_layers.0.weight'),\n ('in_layers.15.in_layers.0.bias', 'input_blocks.15.0.in_layers.0.bias'),\n ('in_layers.15.in_layers.2.weight', 'input_blocks.15.0.in_layers.2.weight'),\n ('in_layers.15.in_layers.2.bias', 'input_blocks.15.0.in_layers.2.bias'),\n ('in_layers.15.emb_layers.1.weight', 'input_blocks.15.0.emb_layers.1.weight'),\n ('in_layers.15.emb_layers.1.bias', 'input_blocks.15.0.emb_layers.1.bias'),\n ('in_layers.15.group_norm.weight', 'input_blocks.15.0.out_layers.0.weight'),\n ('in_layers.15.group_norm.bias', 'input_blocks.15.0.out_layers.0.bias'),\n ('in_layers.15.out_layers.2.weight', 'input_blocks.15.0.out_layers.3.weight'),\n ('in_layers.15.out_layers.2.bias', 'input_blocks.15.0.out_layers.3.bias'),\n ('in_layers.15.attention_layer.group_norm.weight',\n  'input_blocks.15.1.norm.weight'),\n ('in_layers.15.attention_layer.group_norm.bias',\n  'input_blocks.15.1.norm.bias'),\n ('in_layers.15.attention_layer.qkv.weight', 'input_blocks.15.1.qkv.weight'),\n ('in_layers.15.attention_layer.qkv.bias', 'input_blocks.15.1.qkv.bias'),\n ('in_layers.15.attention_layer.encoder_kv.weight',\n  'input_blocks.15.1.encoder_kv.weight'),\n ('in_layers.15.attention_layer.encoder_kv.bias',\n  'input_blocks.15.1.encoder_kv.bias'),\n ('in_layers.15.attention_layer.proj_out.weight',\n  'input_blocks.15.1.proj_out.weight'),\n ('in_layers.15.attention_layer.proj_out.bias',\n  'input_blocks.15.1.proj_out.bias'),\n ('middle_layers.0.in_layers.0.weight', 'middle_block.0.in_layers.0.weight'),\n ('middle_layers.0.in_layers.0.bias', 'middle_block.0.in_layers.0.bias'),\n ('middle_layers.0.in_layers.2.weight', 'middle_block.0.in_layers.2.weight'),\n ('middle_layers.0.in_layers.2.bias', 'middle_block.0.in_layers.2.bias'),\n ('middle_layers.0.emb_layers.1.weight', 'middle_block.0.emb_layers.1.weight'),\n ('middle_layers.0.emb_layers.1.bias', 'middle_block.0.emb_layers.1.bias'),\n ('middle_layers.0.group_norm.weight', 'middle_block.0.out_layers.0.weight'),\n ('middle_layers.0.group_norm.bias', 'middle_block.0.out_layers.0.bias'),\n ('middle_layers.0.out_layers.2.weight', 'middle_block.0.out_layers.3.weight'),\n ('middle_layers.0.out_layers.2.bias', 'middle_block.0.out_layers.3.bias'),\n ('middle_layers.1.group_norm.weight', 'middle_block.1.norm.weight'),\n ('middle_layers.1.group_norm.bias', 'middle_block.1.norm.bias'),\n ('middle_layers.1.qkv.weight', 'middle_block.1.qkv.weight'),\n ('middle_layers.1.qkv.bias', 'middle_block.1.qkv.bias'),\n ('middle_layers.1.encoder_kv.weight', 'middle_block.1.encoder_kv.weight'),\n ('middle_layers.1.encoder_kv.bias', 'middle_block.1.encoder_kv.bias'),\n ('middle_layers.1.proj_out.weight', 'middle_block.1.proj_out.weight'),\n ('middle_layers.1.proj_out.bias', 'middle_block.1.proj_out.bias'),\n ('middle_layers.2.in_layers.0.weight', 'middle_block.2.in_layers.0.weight'),\n ('middle_layers.2.in_layers.0.bias', 'middle_block.2.in_layers.0.bias'),\n ('middle_layers.2.in_layers.2.weight', 'middle_block.2.in_layers.2.weight'),\n ('middle_layers.2.in_layers.2.bias', 'middle_block.2.in_layers.2.bias'),\n ('middle_layers.2.emb_layers.1.weight', 'middle_block.2.emb_layers.1.weight'),\n ('middle_layers.2.emb_layers.1.bias', 'middle_block.2.emb_layers.1.bias'),\n ('middle_layers.2.group_norm.weight', 'middle_block.2.out_layers.0.weight'),\n ('middle_layers.2.group_norm.bias', 'middle_block.2.out_layers.0.bias'),\n ('middle_layers.2.out_layers.2.weight', 'middle_block.2.out_layers.3.weight'),\n ('middle_layers.2.out_layers.2.bias', 'middle_block.2.out_layers.3.bias'),\n ('out_layers.0.in_layers.0.weight', 'output_blocks.0.0.in_layers.0.weight'),\n ('out_layers.0.in_layers.0.bias', 'output_blocks.0.0.in_layers.0.bias'),\n ('out_layers.0.in_layers.2.weight', 'output_blocks.0.0.in_layers.2.weight'),\n ('out_layers.0.in_layers.2.bias', 'output_blocks.0.0.in_layers.2.bias'),\n ('out_layers.0.emb_layers.1.weight', 'output_blocks.0.0.emb_layers.1.weight'),\n ('out_layers.0.emb_layers.1.bias', 'output_blocks.0.0.emb_layers.1.bias'),\n ('out_layers.0.group_norm.weight', 'output_blocks.0.0.out_layers.0.weight'),\n ('out_layers.0.group_norm.bias', 'output_blocks.0.0.out_layers.0.bias'),\n ('out_layers.0.out_layers.2.weight', 'output_blocks.0.0.out_layers.3.weight'),\n ('out_layers.0.out_layers.2.bias', 'output_blocks.0.0.out_layers.3.bias'),\n ('out_layers.0.skip_connection.weight',\n  'output_blocks.0.0.skip_connection.weight'),\n ('out_layers.0.skip_connection.bias',\n  'output_blocks.0.0.skip_connection.bias'),\n ('out_layers.0.attention_layer.group_norm.weight',\n  'output_blocks.0.1.norm.weight'),\n ('out_layers.0.attention_layer.group_norm.bias',\n  'output_blocks.0.1.norm.bias'),\n ('out_layers.0.attention_layer.qkv.weight', 'output_blocks.0.1.qkv.weight'),\n ('out_layers.0.attention_layer.qkv.bias', 'output_blocks.0.1.qkv.bias'),\n ('out_layers.0.attention_layer.encoder_kv.weight',\n  'output_blocks.0.1.encoder_kv.weight'),\n ('out_layers.0.attention_layer.encoder_kv.bias',\n  'output_blocks.0.1.encoder_kv.bias'),\n ('out_layers.0.attention_layer.proj_out.weight',\n  'output_blocks.0.1.proj_out.weight'),\n ('out_layers.0.attention_layer.proj_out.bias',\n  'output_blocks.0.1.proj_out.bias'),\n ('out_layers.1.in_layers.0.weight', 'output_blocks.1.0.in_layers.0.weight'),\n ('out_layers.1.in_layers.0.bias', 'output_blocks.1.0.in_layers.0.bias'),\n ('out_layers.1.in_layers.2.weight', 'output_blocks.1.0.in_layers.2.weight'),\n ('out_layers.1.in_layers.2.bias', 'output_blocks.1.0.in_layers.2.bias'),\n ('out_layers.1.emb_layers.1.weight', 'output_blocks.1.0.emb_layers.1.weight'),\n ('out_layers.1.emb_layers.1.bias', 'output_blocks.1.0.emb_layers.1.bias'),\n ('out_layers.1.group_norm.weight', 'output_blocks.1.0.out_layers.0.weight'),\n ('out_layers.1.group_norm.bias', 'output_blocks.1.0.out_layers.0.bias'),\n ('out_layers.1.out_layers.2.weight', 'output_blocks.1.0.out_layers.3.weight'),\n ('out_layers.1.out_layers.2.bias', 'output_blocks.1.0.out_layers.3.bias'),\n ('out_layers.1.skip_connection.weight',\n  'output_blocks.1.0.skip_connection.weight'),\n ('out_layers.1.skip_connection.bias',\n  'output_blocks.1.0.skip_connection.bias'),\n ('out_layers.1.attention_layer.group_norm.weight',\n  'output_blocks.1.1.norm.weight'),\n ('out_layers.1.attention_layer.group_norm.bias',\n  'output_blocks.1.1.norm.bias'),\n ('out_layers.1.attention_layer.qkv.weight', 'output_blocks.1.1.qkv.weight'),\n ('out_layers.1.attention_layer.qkv.bias', 'output_blocks.1.1.qkv.bias'),\n ('out_layers.1.attention_layer.encoder_kv.weight',\n  'output_blocks.1.1.encoder_kv.weight'),\n ('out_layers.1.attention_layer.encoder_kv.bias',\n  'output_blocks.1.1.encoder_kv.bias'),\n ('out_layers.1.attention_layer.proj_out.weight',\n  'output_blocks.1.1.proj_out.weight'),\n ('out_layers.1.attention_layer.proj_out.bias',\n  'output_blocks.1.1.proj_out.bias'),\n ('out_layers.2.in_layers.0.weight', 'output_blocks.2.0.in_layers.0.weight'),\n ('out_layers.2.in_layers.0.bias', 'output_blocks.2.0.in_layers.0.bias'),\n ('out_layers.2.in_layers.2.weight', 'output_blocks.2.0.in_layers.2.weight'),\n ('out_layers.2.in_layers.2.bias', 'output_blocks.2.0.in_layers.2.bias'),\n ('out_layers.2.emb_layers.1.weight', 'output_blocks.2.0.emb_layers.1.weight'),\n ('out_layers.2.emb_layers.1.bias', 'output_blocks.2.0.emb_layers.1.bias'),\n ('out_layers.2.group_norm.weight', 'output_blocks.2.0.out_layers.0.weight'),\n ('out_layers.2.group_norm.bias', 'output_blocks.2.0.out_layers.0.bias'),\n ('out_layers.2.out_layers.2.weight', 'output_blocks.2.0.out_layers.3.weight'),\n ('out_layers.2.out_layers.2.bias', 'output_blocks.2.0.out_layers.3.bias'),\n ('out_layers.2.skip_connection.weight',\n  'output_blocks.2.0.skip_connection.weight'),\n ('out_layers.2.skip_connection.bias',\n  'output_blocks.2.0.skip_connection.bias'),\n ('out_layers.2.attention_layer.group_norm.weight',\n  'output_blocks.2.1.norm.weight'),\n ('out_layers.2.attention_layer.group_norm.bias',\n  'output_blocks.2.1.norm.bias'),\n ('out_layers.2.attention_layer.qkv.weight', 'output_blocks.2.1.qkv.weight'),\n ('out_layers.2.attention_layer.qkv.bias', 'output_blocks.2.1.qkv.bias'),\n ('out_layers.2.attention_layer.encoder_kv.weight',\n  'output_blocks.2.1.encoder_kv.weight'),\n ('out_layers.2.attention_layer.encoder_kv.bias',\n  'output_blocks.2.1.encoder_kv.bias'),\n ('out_layers.2.attention_layer.proj_out.weight',\n  'output_blocks.2.1.proj_out.weight'),\n ('out_layers.2.attention_layer.proj_out.bias',\n  'output_blocks.2.1.proj_out.bias'),\n ('out_layers.3.in_layers.0.weight', 'output_blocks.3.0.in_layers.0.weight'),\n ('out_layers.3.in_layers.0.bias', 'output_blocks.3.0.in_layers.0.bias'),\n ('out_layers.3.in_layers.2.weight', 'output_blocks.3.0.in_layers.2.weight'),\n ('out_layers.3.in_layers.2.bias', 'output_blocks.3.0.in_layers.2.bias'),\n ('out_layers.3.emb_layers.1.weight', 'output_blocks.3.0.emb_layers.1.weight'),\n ('out_layers.3.emb_layers.1.bias', 'output_blocks.3.0.emb_layers.1.bias'),\n ('out_layers.3.group_norm.weight', 'output_blocks.3.0.out_layers.0.weight'),\n ('out_layers.3.group_norm.bias', 'output_blocks.3.0.out_layers.0.bias'),\n ('out_layers.3.out_layers.2.weight', 'output_blocks.3.0.out_layers.3.weight'),\n ('out_layers.3.out_layers.2.bias', 'output_blocks.3.0.out_layers.3.bias'),\n ('out_layers.3.skip_connection.weight',\n  'output_blocks.3.0.skip_connection.weight'),\n ('out_layers.3.skip_connection.bias',\n  'output_blocks.3.0.skip_connection.bias'),\n ('out_layers.3.attention_layer.group_norm.weight',\n  'output_blocks.3.1.norm.weight'),\n ('out_layers.3.attention_layer.group_norm.bias',\n  'output_blocks.3.1.norm.bias'),\n ('out_layers.3.attention_layer.qkv.weight', 'output_blocks.3.1.qkv.weight'),\n ('out_layers.3.attention_layer.qkv.bias', 'output_blocks.3.1.qkv.bias'),\n ('out_layers.3.attention_layer.encoder_kv.weight',\n  'output_blocks.3.1.encoder_kv.weight'),\n ('out_layers.3.attention_layer.encoder_kv.bias',\n  'output_blocks.3.1.encoder_kv.bias'),\n ('out_layers.3.attention_layer.proj_out.weight',\n  'output_blocks.3.1.proj_out.weight'),\n ('out_layers.3.attention_layer.proj_out.bias',\n  'output_blocks.3.1.proj_out.bias'),\n ('out_layers.4.0.in_layers.0.weight', 'output_blocks.3.2.in_layers.0.weight'),\n ('out_layers.4.0.in_layers.0.bias', 'output_blocks.3.2.in_layers.0.bias'),\n ('out_layers.4.0.in_layers.2.weight', 'output_blocks.3.2.in_layers.2.weight'),\n ('out_layers.4.0.in_layers.2.bias', 'output_blocks.3.2.in_layers.2.bias'),\n ('out_layers.4.0.emb_layers.1.weight',\n  'output_blocks.3.2.emb_layers.1.weight'),\n ('out_layers.4.0.emb_layers.1.bias', 'output_blocks.3.2.emb_layers.1.bias'),\n ('out_layers.4.0.group_norm.weight', 'output_blocks.3.2.out_layers.0.weight'),\n ('out_layers.4.0.group_norm.bias', 'output_blocks.3.2.out_layers.0.bias'),\n ('out_layers.4.0.out_layers.2.weight',\n  'output_blocks.3.2.out_layers.3.weight'),\n ('out_layers.4.0.out_layers.2.bias', 'output_blocks.3.2.out_layers.3.bias'),\n ('out_layers.4.0.skip_connection.weight',\n  'output_blocks.4.0.in_layers.0.weight'),\n ('out_layers.4.0.skip_connection.bias', 'output_blocks.4.0.in_layers.0.bias'),\n ('out_layers.4.0.attention_layer.group_norm.weight',\n  'output_blocks.4.0.in_layers.2.weight'),\n ('out_layers.4.0.attention_layer.group_norm.bias',\n  'output_blocks.4.0.in_layers.2.bias'),\n ('out_layers.4.0.attention_layer.qkv.weight',\n  'output_blocks.4.0.emb_layers.1.weight'),\n ('out_layers.4.0.attention_layer.qkv.bias',\n  'output_blocks.4.0.emb_layers.1.bias'),\n ('out_layers.4.0.attention_layer.encoder_kv.weight',\n  'output_blocks.4.0.out_layers.0.weight'),\n ('out_layers.4.0.attention_layer.encoder_kv.bias',\n  'output_blocks.4.0.out_layers.0.bias'),\n ('out_layers.4.0.attention_layer.proj_out.weight',\n  'output_blocks.4.0.out_layers.3.weight'),\n ('out_layers.4.0.attention_layer.proj_out.bias',\n  'output_blocks.4.0.out_layers.3.bias'),\n ('out_layers.4.1.in_layers.0.weight',\n  'output_blocks.4.0.skip_connection.weight'),\n ('out_layers.4.1.in_layers.0.bias', 'output_blocks.4.0.skip_connection.bias'),\n ('out_layers.4.1.in_layers.2.weight', 'output_blocks.4.1.norm.weight'),\n ('out_layers.4.1.in_layers.2.bias', 'output_blocks.4.1.norm.bias'),\n ('out_layers.4.1.emb_layers.1.weight', 'output_blocks.4.1.qkv.weight'),\n ('out_layers.4.1.emb_layers.1.bias', 'output_blocks.4.1.qkv.bias'),\n ('out_layers.4.1.group_norm.weight', 'output_blocks.4.1.encoder_kv.weight'),\n ('out_layers.4.1.group_norm.bias', 'output_blocks.4.1.encoder_kv.bias'),\n ('out_layers.4.1.out_layers.2.weight', 'output_blocks.4.1.proj_out.weight'),\n ('out_layers.4.1.out_layers.2.bias', 'output_blocks.4.1.proj_out.bias'),\n ('out_layers.5.in_layers.0.weight', 'output_blocks.5.0.in_layers.0.weight'),\n ('out_layers.5.in_layers.0.bias', 'output_blocks.5.0.in_layers.0.bias'),\n ('out_layers.5.in_layers.2.weight', 'output_blocks.5.0.in_layers.2.weight'),\n ('out_layers.5.in_layers.2.bias', 'output_blocks.5.0.in_layers.2.bias'),\n ('out_layers.5.emb_layers.1.weight', 'output_blocks.5.0.emb_layers.1.weight'),\n ('out_layers.5.emb_layers.1.bias', 'output_blocks.5.0.emb_layers.1.bias'),\n ('out_layers.5.group_norm.weight', 'output_blocks.5.0.out_layers.0.weight'),\n ('out_layers.5.group_norm.bias', 'output_blocks.5.0.out_layers.0.bias'),\n ('out_layers.5.out_layers.2.weight', 'output_blocks.5.0.out_layers.3.weight'),\n ('out_layers.5.out_layers.2.bias', 'output_blocks.5.0.out_layers.3.bias'),\n ('out_layers.5.skip_connection.weight',\n  'output_blocks.5.0.skip_connection.weight'),\n ('out_layers.5.skip_connection.bias',\n  'output_blocks.5.0.skip_connection.bias'),\n ('out_layers.5.attention_layer.group_norm.weight',\n  'output_blocks.5.1.norm.weight'),\n ('out_layers.5.attention_layer.group_norm.bias',\n  'output_blocks.5.1.norm.bias'),\n ('out_layers.5.attention_layer.qkv.weight', 'output_blocks.5.1.qkv.weight'),\n ('out_layers.5.attention_layer.qkv.bias', 'output_blocks.5.1.qkv.bias'),\n ('out_layers.5.attention_layer.encoder_kv.weight',\n  'output_blocks.5.1.encoder_kv.weight'),\n ('out_layers.5.attention_layer.encoder_kv.bias',\n  'output_blocks.5.1.encoder_kv.bias'),\n ('out_layers.5.attention_layer.proj_out.weight',\n  'output_blocks.5.1.proj_out.weight'),\n ('out_layers.5.attention_layer.proj_out.bias',\n  'output_blocks.5.1.proj_out.bias'),\n ('out_layers.6.in_layers.0.weight', 'output_blocks.6.0.in_layers.0.weight'),\n ('out_layers.6.in_layers.0.bias', 'output_blocks.6.0.in_layers.0.bias'),\n ('out_layers.6.in_layers.2.weight', 'output_blocks.6.0.in_layers.2.weight'),\n ('out_layers.6.in_layers.2.bias', 'output_blocks.6.0.in_layers.2.bias'),\n ('out_layers.6.emb_layers.1.weight', 'output_blocks.6.0.emb_layers.1.weight'),\n ('out_layers.6.emb_layers.1.bias', 'output_blocks.6.0.emb_layers.1.bias'),\n ('out_layers.6.group_norm.weight', 'output_blocks.6.0.out_layers.0.weight'),\n ('out_layers.6.group_norm.bias', 'output_blocks.6.0.out_layers.0.bias'),\n ('out_layers.6.out_layers.2.weight', 'output_blocks.6.0.out_layers.3.weight'),\n ('out_layers.6.out_layers.2.bias', 'output_blocks.6.0.out_layers.3.bias'),\n ('out_layers.6.skip_connection.weight',\n  'output_blocks.6.0.skip_connection.weight'),\n ('out_layers.6.skip_connection.bias',\n  'output_blocks.6.0.skip_connection.bias'),\n ('out_layers.6.attention_layer.group_norm.weight',\n  'output_blocks.6.1.norm.weight'),\n ('out_layers.6.attention_layer.group_norm.bias',\n  'output_blocks.6.1.norm.bias'),\n ('out_layers.6.attention_layer.qkv.weight', 'output_blocks.6.1.qkv.weight'),\n ('out_layers.6.attention_layer.qkv.bias', 'output_blocks.6.1.qkv.bias'),\n ('out_layers.6.attention_layer.encoder_kv.weight',\n  'output_blocks.6.1.encoder_kv.weight'),\n ('out_layers.6.attention_layer.encoder_kv.bias',\n  'output_blocks.6.1.encoder_kv.bias'),\n ('out_layers.6.attention_layer.proj_out.weight',\n  'output_blocks.6.1.proj_out.weight'),\n ('out_layers.6.attention_layer.proj_out.bias',\n  'output_blocks.6.1.proj_out.bias'),\n ('out_layers.7.in_layers.0.weight', 'output_blocks.7.0.in_layers.0.weight'),\n ('out_layers.7.in_layers.0.bias', 'output_blocks.7.0.in_layers.0.bias'),\n ('out_layers.7.in_layers.2.weight', 'output_blocks.7.0.in_layers.2.weight'),\n ('out_layers.7.in_layers.2.bias', 'output_blocks.7.0.in_layers.2.bias'),\n ('out_layers.7.emb_layers.1.weight', 'output_blocks.7.0.emb_layers.1.weight'),\n ('out_layers.7.emb_layers.1.bias', 'output_blocks.7.0.emb_layers.1.bias'),\n ('out_layers.7.group_norm.weight', 'output_blocks.7.0.out_layers.0.weight'),\n ('out_layers.7.group_norm.bias', 'output_blocks.7.0.out_layers.0.bias'),\n ('out_layers.7.out_layers.2.weight', 'output_blocks.7.0.out_layers.3.weight'),\n ('out_layers.7.out_layers.2.bias', 'output_blocks.7.0.out_layers.3.bias'),\n ('out_layers.7.skip_connection.weight',\n  'output_blocks.7.0.skip_connection.weight'),\n ('out_layers.7.skip_connection.bias',\n  'output_blocks.7.0.skip_connection.bias'),\n ('out_layers.7.attention_layer.group_norm.weight',\n  'output_blocks.7.1.norm.weight'),\n ('out_layers.7.attention_layer.group_norm.bias',\n  'output_blocks.7.1.norm.bias'),\n ('out_layers.7.attention_layer.qkv.weight', 'output_blocks.7.1.qkv.weight'),\n ('out_layers.7.attention_layer.qkv.bias', 'output_blocks.7.1.qkv.bias'),\n ('out_layers.7.attention_layer.encoder_kv.weight',\n  'output_blocks.7.1.encoder_kv.weight'),\n ('out_layers.7.attention_layer.encoder_kv.bias',\n  'output_blocks.7.1.encoder_kv.bias'),\n ('out_layers.7.attention_layer.proj_out.weight',\n  'output_blocks.7.1.proj_out.weight'),\n ('out_layers.7.attention_layer.proj_out.bias',\n  'output_blocks.7.1.proj_out.bias'),\n ('out_layers.8.0.in_layers.0.weight', 'output_blocks.7.2.in_layers.0.weight'),\n ('out_layers.8.0.in_layers.0.bias', 'output_blocks.7.2.in_layers.0.bias'),\n ('out_layers.8.0.in_layers.2.weight', 'output_blocks.7.2.in_layers.2.weight'),\n ('out_layers.8.0.in_layers.2.bias', 'output_blocks.7.2.in_layers.2.bias'),\n ('out_layers.8.0.emb_layers.1.weight',\n  'output_blocks.7.2.emb_layers.1.weight'),\n ('out_layers.8.0.emb_layers.1.bias', 'output_blocks.7.2.emb_layers.1.bias'),\n ('out_layers.8.0.group_norm.weight', 'output_blocks.7.2.out_layers.0.weight'),\n ('out_layers.8.0.group_norm.bias', 'output_blocks.7.2.out_layers.0.bias'),\n ('out_layers.8.0.out_layers.2.weight',\n  'output_blocks.7.2.out_layers.3.weight'),\n ('out_layers.8.0.out_layers.2.bias', 'output_blocks.7.2.out_layers.3.bias'),\n ('out_layers.8.0.skip_connection.weight',\n  'output_blocks.8.0.in_layers.0.weight'),\n ('out_layers.8.0.skip_connection.bias', 'output_blocks.8.0.in_layers.0.bias'),\n ('out_layers.8.0.attention_layer.group_norm.weight',\n  'output_blocks.8.0.in_layers.2.weight'),\n ('out_layers.8.0.attention_layer.group_norm.bias',\n  'output_blocks.8.0.in_layers.2.bias'),\n ('out_layers.8.0.attention_layer.qkv.weight',\n  'output_blocks.8.0.emb_layers.1.weight'),\n ('out_layers.8.0.attention_layer.qkv.bias',\n  'output_blocks.8.0.emb_layers.1.bias'),\n ('out_layers.8.0.attention_layer.encoder_kv.weight',\n  'output_blocks.8.0.out_layers.0.weight'),\n ('out_layers.8.0.attention_layer.encoder_kv.bias',\n  'output_blocks.8.0.out_layers.0.bias'),\n ('out_layers.8.0.attention_layer.proj_out.weight',\n  'output_blocks.8.0.out_layers.3.weight'),\n ('out_layers.8.0.attention_layer.proj_out.bias',\n  'output_blocks.8.0.out_layers.3.bias'),\n ('out_layers.8.1.in_layers.0.weight',\n  'output_blocks.8.0.skip_connection.weight'),\n ('out_layers.8.1.in_layers.0.bias', 'output_blocks.8.0.skip_connection.bias'),\n ('out_layers.8.1.in_layers.2.weight', 'output_blocks.8.1.norm.weight'),\n ('out_layers.8.1.in_layers.2.bias', 'output_blocks.8.1.norm.bias'),\n ('out_layers.8.1.emb_layers.1.weight', 'output_blocks.8.1.qkv.weight'),\n ('out_layers.8.1.emb_layers.1.bias', 'output_blocks.8.1.qkv.bias'),\n ('out_layers.8.1.group_norm.weight', 'output_blocks.8.1.encoder_kv.weight'),\n ('out_layers.8.1.group_norm.bias', 'output_blocks.8.1.encoder_kv.bias'),\n ('out_layers.8.1.out_layers.2.weight', 'output_blocks.8.1.proj_out.weight'),\n ('out_layers.8.1.out_layers.2.bias', 'output_blocks.8.1.proj_out.bias'),\n ('out_layers.9.in_layers.0.weight', 'output_blocks.9.0.in_layers.0.weight'),\n ('out_layers.9.in_layers.0.bias', 'output_blocks.9.0.in_layers.0.bias'),\n ('out_layers.9.in_layers.2.weight', 'output_blocks.9.0.in_layers.2.weight'),\n ('out_layers.9.in_layers.2.bias', 'output_blocks.9.0.in_layers.2.bias'),\n ('out_layers.9.emb_layers.1.weight', 'output_blocks.9.0.emb_layers.1.weight'),\n ('out_layers.9.emb_layers.1.bias', 'output_blocks.9.0.emb_layers.1.bias'),\n ('out_layers.9.group_norm.weight', 'output_blocks.9.0.out_layers.0.weight'),\n ('out_layers.9.group_norm.bias', 'output_blocks.9.0.out_layers.0.bias'),\n ('out_layers.9.out_layers.2.weight', 'output_blocks.9.0.out_layers.3.weight'),\n ('out_layers.9.out_layers.2.bias', 'output_blocks.9.0.out_layers.3.bias'),\n ('out_layers.9.skip_connection.weight',\n  'output_blocks.9.0.skip_connection.weight'),\n ('out_layers.9.skip_connection.bias',\n  'output_blocks.9.0.skip_connection.bias'),\n ('out_layers.9.attention_layer.group_norm.weight',\n  'output_blocks.9.1.norm.weight'),\n ('out_layers.9.attention_layer.group_norm.bias',\n  'output_blocks.9.1.norm.bias'),\n ('out_layers.9.attention_layer.qkv.weight', 'output_blocks.9.1.qkv.weight'),\n ('out_layers.9.attention_layer.qkv.bias', 'output_blocks.9.1.qkv.bias'),\n ('out_layers.9.attention_layer.encoder_kv.weight',\n  'output_blocks.9.1.encoder_kv.weight'),\n ('out_layers.9.attention_layer.encoder_kv.bias',\n  'output_blocks.9.1.encoder_kv.bias'),\n ('out_layers.9.attention_layer.proj_out.weight',\n  'output_blocks.9.1.proj_out.weight'),\n ('out_layers.9.attention_layer.proj_out.bias',\n  'output_blocks.9.1.proj_out.bias'),\n ('out_layers.10.in_layers.0.weight', 'output_blocks.10.0.in_layers.0.weight'),\n ('out_layers.10.in_layers.0.bias', 'output_blocks.10.0.in_layers.0.bias'),\n ('out_layers.10.in_layers.2.weight', 'output_blocks.10.0.in_layers.2.weight'),\n ('out_layers.10.in_layers.2.bias', 'output_blocks.10.0.in_layers.2.bias'),\n ('out_layers.10.emb_layers.1.weight',\n  'output_blocks.10.0.emb_layers.1.weight'),\n ('out_layers.10.emb_layers.1.bias', 'output_blocks.10.0.emb_layers.1.bias'),\n ('out_layers.10.group_norm.weight', 'output_blocks.10.0.out_layers.0.weight'),\n ('out_layers.10.group_norm.bias', 'output_blocks.10.0.out_layers.0.bias'),\n ('out_layers.10.out_layers.2.weight',\n  'output_blocks.10.0.out_layers.3.weight'),\n ('out_layers.10.out_layers.2.bias', 'output_blocks.10.0.out_layers.3.bias'),\n ('out_layers.10.skip_connection.weight',\n  'output_blocks.10.0.skip_connection.weight'),\n ('out_layers.10.skip_connection.bias',\n  'output_blocks.10.0.skip_connection.bias'),\n ('out_layers.10.attention_layer.group_norm.weight',\n  'output_blocks.10.1.norm.weight'),\n ('out_layers.10.attention_layer.group_norm.bias',\n  'output_blocks.10.1.norm.bias'),\n ('out_layers.10.attention_layer.qkv.weight', 'output_blocks.10.1.qkv.weight'),\n ('out_layers.10.attention_layer.qkv.bias', 'output_blocks.10.1.qkv.bias'),\n ('out_layers.10.attention_layer.encoder_kv.weight',\n  'output_blocks.10.1.encoder_kv.weight'),\n ('out_layers.10.attention_layer.encoder_kv.bias',\n  'output_blocks.10.1.encoder_kv.bias'),\n ('out_layers.10.attention_layer.proj_out.weight',\n  'output_blocks.10.1.proj_out.weight'),\n ('out_layers.10.attention_layer.proj_out.bias',\n  'output_blocks.10.1.proj_out.bias'),\n ('out_layers.11.in_layers.0.weight', 'output_blocks.11.0.in_layers.0.weight'),\n ('out_layers.11.in_layers.0.bias', 'output_blocks.11.0.in_layers.0.bias'),\n ('out_layers.11.in_layers.2.weight', 'output_blocks.11.0.in_layers.2.weight'),\n ('out_layers.11.in_layers.2.bias', 'output_blocks.11.0.in_layers.2.bias'),\n ('out_layers.11.emb_layers.1.weight',\n  'output_blocks.11.0.emb_layers.1.weight'),\n ('out_layers.11.emb_layers.1.bias', 'output_blocks.11.0.emb_layers.1.bias'),\n ('out_layers.11.group_norm.weight', 'output_blocks.11.0.out_layers.0.weight'),\n ('out_layers.11.group_norm.bias', 'output_blocks.11.0.out_layers.0.bias'),\n ('out_layers.11.out_layers.2.weight',\n  'output_blocks.11.0.out_layers.3.weight'),\n ('out_layers.11.out_layers.2.bias', 'output_blocks.11.0.out_layers.3.bias'),\n ('out_layers.11.skip_connection.weight',\n  'output_blocks.11.0.skip_connection.weight'),\n ('out_layers.11.skip_connection.bias',\n  'output_blocks.11.0.skip_connection.bias'),\n ('out_layers.11.attention_layer.group_norm.weight',\n  'output_blocks.11.1.norm.weight'),\n ('out_layers.11.attention_layer.group_norm.bias',\n  'output_blocks.11.1.norm.bias'),\n ('out_layers.11.attention_layer.qkv.weight', 'output_blocks.11.1.qkv.weight'),\n ('out_layers.11.attention_layer.qkv.bias', 'output_blocks.11.1.qkv.bias'),\n ('out_layers.11.attention_layer.encoder_kv.weight',\n  'output_blocks.11.1.encoder_kv.weight'),\n ('out_layers.11.attention_layer.encoder_kv.bias',\n  'output_blocks.11.1.encoder_kv.bias'),\n ('out_layers.11.attention_layer.proj_out.weight',\n  'output_blocks.11.1.proj_out.weight'),\n ('out_layers.11.attention_layer.proj_out.bias',\n  'output_blocks.11.1.proj_out.bias'),\n ('out_layers.12.0.in_layers.0.weight',\n  'output_blocks.11.2.in_layers.0.weight'),\n ('out_layers.12.0.in_layers.0.bias', 'output_blocks.11.2.in_layers.0.bias'),\n ('out_layers.12.0.in_layers.2.weight',\n  'output_blocks.11.2.in_layers.2.weight'),\n ('out_layers.12.0.in_layers.2.bias', 'output_blocks.11.2.in_layers.2.bias'),\n ('out_layers.12.0.emb_layers.1.weight',\n  'output_blocks.11.2.emb_layers.1.weight'),\n ('out_layers.12.0.emb_layers.1.bias', 'output_blocks.11.2.emb_layers.1.bias'),\n ('out_layers.12.0.group_norm.weight',\n  'output_blocks.11.2.out_layers.0.weight'),\n ('out_layers.12.0.group_norm.bias', 'output_blocks.11.2.out_layers.0.bias'),\n ('out_layers.12.0.out_layers.2.weight',\n  'output_blocks.11.2.out_layers.3.weight'),\n ('out_layers.12.0.out_layers.2.bias', 'output_blocks.11.2.out_layers.3.bias'),\n ('out_layers.12.0.skip_connection.weight',\n  'output_blocks.12.0.in_layers.0.weight'),\n ('out_layers.12.0.skip_connection.bias',\n  'output_blocks.12.0.in_layers.0.bias'),\n ('out_layers.12.1.in_layers.0.weight',\n  'output_blocks.12.0.in_layers.2.weight'),\n ('out_layers.12.1.in_layers.0.bias', 'output_blocks.12.0.in_layers.2.bias'),\n ('out_layers.12.1.in_layers.2.weight',\n  'output_blocks.12.0.emb_layers.1.weight'),\n ('out_layers.12.1.in_layers.2.bias', 'output_blocks.12.0.emb_layers.1.bias'),\n ('out_layers.12.1.emb_layers.1.weight',\n  'output_blocks.12.0.out_layers.0.weight'),\n ('out_layers.12.1.emb_layers.1.bias', 'output_blocks.12.0.out_layers.0.bias'),\n ('out_layers.12.1.group_norm.weight',\n  'output_blocks.12.0.out_layers.3.weight'),\n ('out_layers.12.1.group_norm.bias', 'output_blocks.12.0.out_layers.3.bias'),\n ('out_layers.12.1.out_layers.2.weight',\n  'output_blocks.12.0.skip_connection.weight'),\n ('out_layers.12.1.out_layers.2.bias',\n  'output_blocks.12.0.skip_connection.bias'),\n ('out_layers.13.in_layers.0.weight', 'output_blocks.13.0.in_layers.0.weight'),\n ('out_layers.13.in_layers.0.bias', 'output_blocks.13.0.in_layers.0.bias'),\n ('out_layers.13.in_layers.2.weight', 'output_blocks.13.0.in_layers.2.weight'),\n ('out_layers.13.in_layers.2.bias', 'output_blocks.13.0.in_layers.2.bias'),\n ('out_layers.13.emb_layers.1.weight',\n  'output_blocks.13.0.emb_layers.1.weight'),\n ('out_layers.13.emb_layers.1.bias', 'output_blocks.13.0.emb_layers.1.bias'),\n ('out_layers.13.group_norm.weight', 'output_blocks.13.0.out_layers.0.weight'),\n ('out_layers.13.group_norm.bias', 'output_blocks.13.0.out_layers.0.bias'),\n ('out_layers.13.out_layers.2.weight',\n  'output_blocks.13.0.out_layers.3.weight'),\n ('out_layers.13.out_layers.2.bias', 'output_blocks.13.0.out_layers.3.bias'),\n ('out_layers.13.skip_connection.weight',\n  'output_blocks.13.0.skip_connection.weight'),\n ('out_layers.13.skip_connection.bias',\n  'output_blocks.13.0.skip_connection.bias'),\n ('out_layers.14.in_layers.0.weight', 'output_blocks.14.0.in_layers.0.weight'),\n ('out_layers.14.in_layers.0.bias', 'output_blocks.14.0.in_layers.0.bias'),\n ('out_layers.14.in_layers.2.weight', 'output_blocks.14.0.in_layers.2.weight'),\n ('out_layers.14.in_layers.2.bias', 'output_blocks.14.0.in_layers.2.bias'),\n ('out_layers.14.emb_layers.1.weight',\n  'output_blocks.14.0.emb_layers.1.weight'),\n ('out_layers.14.emb_layers.1.bias', 'output_blocks.14.0.emb_layers.1.bias'),\n ('out_layers.14.group_norm.weight', 'output_blocks.14.0.out_layers.0.weight'),\n ('out_layers.14.group_norm.bias', 'output_blocks.14.0.out_layers.0.bias'),\n ('out_layers.14.out_layers.2.weight',\n  'output_blocks.14.0.out_layers.3.weight'),\n ('out_layers.14.out_layers.2.bias', 'output_blocks.14.0.out_layers.3.bias'),\n ('out_layers.14.skip_connection.weight',\n  'output_blocks.14.0.skip_connection.weight'),\n ('out_layers.14.skip_connection.bias',\n  'output_blocks.14.0.skip_connection.bias'),\n ('out_layers.15.in_layers.0.weight', 'output_blocks.15.0.in_layers.0.weight'),\n ('out_layers.15.in_layers.0.bias', 'output_blocks.15.0.in_layers.0.bias'),\n ('out_layers.15.in_layers.2.weight', 'output_blocks.15.0.in_layers.2.weight'),\n ('out_layers.15.in_layers.2.bias', 'output_blocks.15.0.in_layers.2.bias'),\n ('out_layers.15.emb_layers.1.weight',\n  'output_blocks.15.0.emb_layers.1.weight'),\n ('out_layers.15.emb_layers.1.bias', 'output_blocks.15.0.emb_layers.1.bias'),\n ('out_layers.15.group_norm.weight', 'output_blocks.15.0.out_layers.0.weight'),\n ('out_layers.15.group_norm.bias', 'output_blocks.15.0.out_layers.0.bias'),\n ('out_layers.15.out_layers.2.weight',\n  'output_blocks.15.0.out_layers.3.weight'),\n ('out_layers.15.out_layers.2.bias', 'output_blocks.15.0.out_layers.3.bias'),\n ('out_layers.15.skip_connection.weight',\n  'output_blocks.15.0.skip_connection.weight'),\n ('out_layers.15.skip_connection.bias',\n  'output_blocks.15.0.skip_connection.bias'),\n ('out.0.weight', 'out.0.weight'),\n ('out.0.bias', 'out.0.bias'),\n ('out.1.weight', 'out.2.weight'),\n ('out.1.bias', 'out.2.bias'),\n ('transformer.resblocks.0.attn.c_qkv.weight',\n  'transformer.resblocks.0.attn.c_qkv.weight'),\n ('transformer.resblocks.0.attn.c_qkv.bias',\n  'transformer.resblocks.0.attn.c_qkv.bias'),\n ('transformer.resblocks.0.attn.c_proj.weight',\n  'transformer.resblocks.0.attn.c_proj.weight'),\n ('transformer.resblocks.0.attn.c_proj.bias',\n  'transformer.resblocks.0.attn.c_proj.bias'),\n ('transformer.resblocks.0.ln_1.weight',\n  'transformer.resblocks.0.ln_1.weight'),\n ('transformer.resblocks.0.ln_1.bias', 'transformer.resblocks.0.ln_1.bias'),\n ('transformer.resblocks.0.mlp.c_fc.weight',\n  'transformer.resblocks.0.mlp.c_fc.weight'),\n ('transformer.resblocks.0.mlp.c_fc.bias',\n  'transformer.resblocks.0.mlp.c_fc.bias'),\n ('transformer.resblocks.0.mlp.c_proj.weight',\n  'transformer.resblocks.0.mlp.c_proj.weight'),\n ('transformer.resblocks.0.mlp.c_proj.bias',\n  'transformer.resblocks.0.mlp.c_proj.bias'),\n ('transformer.resblocks.0.ln_2.weight',\n  'transformer.resblocks.0.ln_2.weight'),\n ('transformer.resblocks.0.ln_2.bias', 'transformer.resblocks.0.ln_2.bias'),\n ('transformer.resblocks.1.attn.c_qkv.weight',\n  'transformer.resblocks.1.attn.c_qkv.weight'),\n ('transformer.resblocks.1.attn.c_qkv.bias',\n  'transformer.resblocks.1.attn.c_qkv.bias'),\n ('transformer.resblocks.1.attn.c_proj.weight',\n  'transformer.resblocks.1.attn.c_proj.weight'),\n ('transformer.resblocks.1.attn.c_proj.bias',\n  'transformer.resblocks.1.attn.c_proj.bias'),\n ('transformer.resblocks.1.ln_1.weight',\n  'transformer.resblocks.1.ln_1.weight'),\n ('transformer.resblocks.1.ln_1.bias', 'transformer.resblocks.1.ln_1.bias'),\n ('transformer.resblocks.1.mlp.c_fc.weight',\n  'transformer.resblocks.1.mlp.c_fc.weight'),\n ('transformer.resblocks.1.mlp.c_fc.bias',\n  'transformer.resblocks.1.mlp.c_fc.bias'),\n ('transformer.resblocks.1.mlp.c_proj.weight',\n  'transformer.resblocks.1.mlp.c_proj.weight'),\n ('transformer.resblocks.1.mlp.c_proj.bias',\n  'transformer.resblocks.1.mlp.c_proj.bias'),\n ('transformer.resblocks.1.ln_2.weight',\n  'transformer.resblocks.1.ln_2.weight'),\n ('transformer.resblocks.1.ln_2.bias', 'transformer.resblocks.1.ln_2.bias'),\n ('transformer.resblocks.2.attn.c_qkv.weight',\n  'transformer.resblocks.2.attn.c_qkv.weight'),\n ('transformer.resblocks.2.attn.c_qkv.bias',\n  'transformer.resblocks.2.attn.c_qkv.bias'),\n ('transformer.resblocks.2.attn.c_proj.weight',\n  'transformer.resblocks.2.attn.c_proj.weight'),\n ('transformer.resblocks.2.attn.c_proj.bias',\n  'transformer.resblocks.2.attn.c_proj.bias'),\n ('transformer.resblocks.2.ln_1.weight',\n  'transformer.resblocks.2.ln_1.weight'),\n ('transformer.resblocks.2.ln_1.bias', 'transformer.resblocks.2.ln_1.bias'),\n ('transformer.resblocks.2.mlp.c_fc.weight',\n  'transformer.resblocks.2.mlp.c_fc.weight'),\n ('transformer.resblocks.2.mlp.c_fc.bias',\n  'transformer.resblocks.2.mlp.c_fc.bias'),\n ('transformer.resblocks.2.mlp.c_proj.weight',\n  'transformer.resblocks.2.mlp.c_proj.weight'),\n ('transformer.resblocks.2.mlp.c_proj.bias',\n  'transformer.resblocks.2.mlp.c_proj.bias'),\n ('transformer.resblocks.2.ln_2.weight',\n  'transformer.resblocks.2.ln_2.weight'),\n ('transformer.resblocks.2.ln_2.bias', 'transformer.resblocks.2.ln_2.bias'),\n ('transformer.resblocks.3.attn.c_qkv.weight',\n  'transformer.resblocks.3.attn.c_qkv.weight'),\n ('transformer.resblocks.3.attn.c_qkv.bias',\n  'transformer.resblocks.3.attn.c_qkv.bias'),\n ('transformer.resblocks.3.attn.c_proj.weight',\n  'transformer.resblocks.3.attn.c_proj.weight'),\n ('transformer.resblocks.3.attn.c_proj.bias',\n  'transformer.resblocks.3.attn.c_proj.bias'),\n ('transformer.resblocks.3.ln_1.weight',\n  'transformer.resblocks.3.ln_1.weight'),\n ('transformer.resblocks.3.ln_1.bias', 'transformer.resblocks.3.ln_1.bias'),\n ('transformer.resblocks.3.mlp.c_fc.weight',\n  'transformer.resblocks.3.mlp.c_fc.weight'),\n ('transformer.resblocks.3.mlp.c_fc.bias',\n  'transformer.resblocks.3.mlp.c_fc.bias'),\n ('transformer.resblocks.3.mlp.c_proj.weight',\n  'transformer.resblocks.3.mlp.c_proj.weight'),\n ('transformer.resblocks.3.mlp.c_proj.bias',\n  'transformer.resblocks.3.mlp.c_proj.bias'),\n ('transformer.resblocks.3.ln_2.weight',\n  'transformer.resblocks.3.ln_2.weight'),\n ('transformer.resblocks.3.ln_2.bias', 'transformer.resblocks.3.ln_2.bias'),\n ('transformer.resblocks.4.attn.c_qkv.weight',\n  'transformer.resblocks.4.attn.c_qkv.weight'),\n ('transformer.resblocks.4.attn.c_qkv.bias',\n  'transformer.resblocks.4.attn.c_qkv.bias'),\n ('transformer.resblocks.4.attn.c_proj.weight',\n  'transformer.resblocks.4.attn.c_proj.weight'),\n ('transformer.resblocks.4.attn.c_proj.bias',\n  'transformer.resblocks.4.attn.c_proj.bias'),\n ('transformer.resblocks.4.ln_1.weight',\n  'transformer.resblocks.4.ln_1.weight'),\n ('transformer.resblocks.4.ln_1.bias', 'transformer.resblocks.4.ln_1.bias'),\n ('transformer.resblocks.4.mlp.c_fc.weight',\n  'transformer.resblocks.4.mlp.c_fc.weight'),\n ('transformer.resblocks.4.mlp.c_fc.bias',\n  'transformer.resblocks.4.mlp.c_fc.bias'),\n ('transformer.resblocks.4.mlp.c_proj.weight',\n  'transformer.resblocks.4.mlp.c_proj.weight'),\n ('transformer.resblocks.4.mlp.c_proj.bias',\n  'transformer.resblocks.4.mlp.c_proj.bias'),\n ('transformer.resblocks.4.ln_2.weight',\n  'transformer.resblocks.4.ln_2.weight'),\n ('transformer.resblocks.4.ln_2.bias', 'transformer.resblocks.4.ln_2.bias'),\n ('transformer.resblocks.5.attn.c_qkv.weight',\n  'transformer.resblocks.5.attn.c_qkv.weight'),\n ('transformer.resblocks.5.attn.c_qkv.bias',\n  'transformer.resblocks.5.attn.c_qkv.bias'),\n ('transformer.resblocks.5.attn.c_proj.weight',\n  'transformer.resblocks.5.attn.c_proj.weight'),\n ('transformer.resblocks.5.attn.c_proj.bias',\n  'transformer.resblocks.5.attn.c_proj.bias'),\n ('transformer.resblocks.5.ln_1.weight',\n  'transformer.resblocks.5.ln_1.weight'),\n ('transformer.resblocks.5.ln_1.bias', 'transformer.resblocks.5.ln_1.bias'),\n ('transformer.resblocks.5.mlp.c_fc.weight',\n  'transformer.resblocks.5.mlp.c_fc.weight'),\n ('transformer.resblocks.5.mlp.c_fc.bias',\n  'transformer.resblocks.5.mlp.c_fc.bias'),\n ('transformer.resblocks.5.mlp.c_proj.weight',\n  'transformer.resblocks.5.mlp.c_proj.weight'),\n ('transformer.resblocks.5.mlp.c_proj.bias',\n  'transformer.resblocks.5.mlp.c_proj.bias'),\n ('transformer.resblocks.5.ln_2.weight',\n  'transformer.resblocks.5.ln_2.weight'),\n ('transformer.resblocks.5.ln_2.bias', 'transformer.resblocks.5.ln_2.bias'),\n ('transformer.resblocks.6.attn.c_qkv.weight',\n  'transformer.resblocks.6.attn.c_qkv.weight'),\n ('transformer.resblocks.6.attn.c_qkv.bias',\n  'transformer.resblocks.6.attn.c_qkv.bias'),\n ('transformer.resblocks.6.attn.c_proj.weight',\n  'transformer.resblocks.6.attn.c_proj.weight'),\n ('transformer.resblocks.6.attn.c_proj.bias',\n  'transformer.resblocks.6.attn.c_proj.bias'),\n ('transformer.resblocks.6.ln_1.weight',\n  'transformer.resblocks.6.ln_1.weight'),\n ('transformer.resblocks.6.ln_1.bias', 'transformer.resblocks.6.ln_1.bias'),\n ('transformer.resblocks.6.mlp.c_fc.weight',\n  'transformer.resblocks.6.mlp.c_fc.weight'),\n ('transformer.resblocks.6.mlp.c_fc.bias',\n  'transformer.resblocks.6.mlp.c_fc.bias'),\n ('transformer.resblocks.6.mlp.c_proj.weight',\n  'transformer.resblocks.6.mlp.c_proj.weight'),\n ('transformer.resblocks.6.mlp.c_proj.bias',\n  'transformer.resblocks.6.mlp.c_proj.bias'),\n ('transformer.resblocks.6.ln_2.weight',\n  'transformer.resblocks.6.ln_2.weight'),\n ('transformer.resblocks.6.ln_2.bias', 'transformer.resblocks.6.ln_2.bias'),\n ('transformer.resblocks.7.attn.c_qkv.weight',\n  'transformer.resblocks.7.attn.c_qkv.weight'),\n ('transformer.resblocks.7.attn.c_qkv.bias',\n  'transformer.resblocks.7.attn.c_qkv.bias'),\n ('transformer.resblocks.7.attn.c_proj.weight',\n  'transformer.resblocks.7.attn.c_proj.weight'),\n ('transformer.resblocks.7.attn.c_proj.bias',\n  'transformer.resblocks.7.attn.c_proj.bias'),\n ('transformer.resblocks.7.ln_1.weight',\n  'transformer.resblocks.7.ln_1.weight'),\n ('transformer.resblocks.7.ln_1.bias', 'transformer.resblocks.7.ln_1.bias'),\n ('transformer.resblocks.7.mlp.c_fc.weight',\n  'transformer.resblocks.7.mlp.c_fc.weight'),\n ('transformer.resblocks.7.mlp.c_fc.bias',\n  'transformer.resblocks.7.mlp.c_fc.bias'),\n ('transformer.resblocks.7.mlp.c_proj.weight',\n  'transformer.resblocks.7.mlp.c_proj.weight'),\n ('transformer.resblocks.7.mlp.c_proj.bias',\n  'transformer.resblocks.7.mlp.c_proj.bias'),\n ('transformer.resblocks.7.ln_2.weight',\n  'transformer.resblocks.7.ln_2.weight'),\n ('transformer.resblocks.7.ln_2.bias', 'transformer.resblocks.7.ln_2.bias'),\n ('transformer.resblocks.8.attn.c_qkv.weight',\n  'transformer.resblocks.8.attn.c_qkv.weight'),\n ('transformer.resblocks.8.attn.c_qkv.bias',\n  'transformer.resblocks.8.attn.c_qkv.bias'),\n ('transformer.resblocks.8.attn.c_proj.weight',\n  'transformer.resblocks.8.attn.c_proj.weight'),\n ('transformer.resblocks.8.attn.c_proj.bias',\n  'transformer.resblocks.8.attn.c_proj.bias'),\n ('transformer.resblocks.8.ln_1.weight',\n  'transformer.resblocks.8.ln_1.weight'),\n ('transformer.resblocks.8.ln_1.bias', 'transformer.resblocks.8.ln_1.bias'),\n ('transformer.resblocks.8.mlp.c_fc.weight',\n  'transformer.resblocks.8.mlp.c_fc.weight'),\n ('transformer.resblocks.8.mlp.c_fc.bias',\n  'transformer.resblocks.8.mlp.c_fc.bias'),\n ('transformer.resblocks.8.mlp.c_proj.weight',\n  'transformer.resblocks.8.mlp.c_proj.weight'),\n ('transformer.resblocks.8.mlp.c_proj.bias',\n  'transformer.resblocks.8.mlp.c_proj.bias'),\n ('transformer.resblocks.8.ln_2.weight',\n  'transformer.resblocks.8.ln_2.weight'),\n ('transformer.resblocks.8.ln_2.bias', 'transformer.resblocks.8.ln_2.bias'),\n ('transformer.resblocks.9.attn.c_qkv.weight',\n  'transformer.resblocks.9.attn.c_qkv.weight'),\n ('transformer.resblocks.9.attn.c_qkv.bias',\n  'transformer.resblocks.9.attn.c_qkv.bias'),\n ('transformer.resblocks.9.attn.c_proj.weight',\n  'transformer.resblocks.9.attn.c_proj.weight'),\n ('transformer.resblocks.9.attn.c_proj.bias',\n  'transformer.resblocks.9.attn.c_proj.bias'),\n ('transformer.resblocks.9.ln_1.weight',\n  'transformer.resblocks.9.ln_1.weight'),\n ('transformer.resblocks.9.ln_1.bias', 'transformer.resblocks.9.ln_1.bias'),\n ('transformer.resblocks.9.mlp.c_fc.weight',\n  'transformer.resblocks.9.mlp.c_fc.weight'),\n ('transformer.resblocks.9.mlp.c_fc.bias',\n  'transformer.resblocks.9.mlp.c_fc.bias'),\n ('transformer.resblocks.9.mlp.c_proj.weight',\n  'transformer.resblocks.9.mlp.c_proj.weight'),\n ('transformer.resblocks.9.mlp.c_proj.bias',\n  'transformer.resblocks.9.mlp.c_proj.bias'),\n ('transformer.resblocks.9.ln_2.weight',\n  'transformer.resblocks.9.ln_2.weight'),\n ('transformer.resblocks.9.ln_2.bias', 'transformer.resblocks.9.ln_2.bias'),\n ('transformer.resblocks.10.attn.c_qkv.weight',\n  'transformer.resblocks.10.attn.c_qkv.weight'),\n ('transformer.resblocks.10.attn.c_qkv.bias',\n  'transformer.resblocks.10.attn.c_qkv.bias'),\n ('transformer.resblocks.10.attn.c_proj.weight',\n  'transformer.resblocks.10.attn.c_proj.weight'),\n ('transformer.resblocks.10.attn.c_proj.bias',\n  'transformer.resblocks.10.attn.c_proj.bias'),\n ('transformer.resblocks.10.ln_1.weight',\n  'transformer.resblocks.10.ln_1.weight'),\n ('transformer.resblocks.10.ln_1.bias', 'transformer.resblocks.10.ln_1.bias'),\n ('transformer.resblocks.10.mlp.c_fc.weight',\n  'transformer.resblocks.10.mlp.c_fc.weight'),\n ('transformer.resblocks.10.mlp.c_fc.bias',\n  'transformer.resblocks.10.mlp.c_fc.bias'),\n ('transformer.resblocks.10.mlp.c_proj.weight',\n  'transformer.resblocks.10.mlp.c_proj.weight'),\n ('transformer.resblocks.10.mlp.c_proj.bias',\n  'transformer.resblocks.10.mlp.c_proj.bias'),\n ('transformer.resblocks.10.ln_2.weight',\n  'transformer.resblocks.10.ln_2.weight'),\n ('transformer.resblocks.10.ln_2.bias', 'transformer.resblocks.10.ln_2.bias'),\n ('transformer.resblocks.11.attn.c_qkv.weight',\n  'transformer.resblocks.11.attn.c_qkv.weight'),\n ('transformer.resblocks.11.attn.c_qkv.bias',\n  'transformer.resblocks.11.attn.c_qkv.bias'),\n ('transformer.resblocks.11.attn.c_proj.weight',\n  'transformer.resblocks.11.attn.c_proj.weight'),\n ('transformer.resblocks.11.attn.c_proj.bias',\n  'transformer.resblocks.11.attn.c_proj.bias'),\n ('transformer.resblocks.11.ln_1.weight',\n  'transformer.resblocks.11.ln_1.weight'),\n ('transformer.resblocks.11.ln_1.bias', 'transformer.resblocks.11.ln_1.bias'),\n ('transformer.resblocks.11.mlp.c_fc.weight',\n  'transformer.resblocks.11.mlp.c_fc.weight'),\n ('transformer.resblocks.11.mlp.c_fc.bias',\n  'transformer.resblocks.11.mlp.c_fc.bias'),\n ('transformer.resblocks.11.mlp.c_proj.weight',\n  'transformer.resblocks.11.mlp.c_proj.weight'),\n ('transformer.resblocks.11.mlp.c_proj.bias',\n  'transformer.resblocks.11.mlp.c_proj.bias'),\n ('transformer.resblocks.11.ln_2.weight',\n  'transformer.resblocks.11.ln_2.weight'),\n ('transformer.resblocks.11.ln_2.bias', 'transformer.resblocks.11.ln_2.bias'),\n ('transformer.resblocks.12.attn.c_qkv.weight',\n  'transformer.resblocks.12.attn.c_qkv.weight'),\n ('transformer.resblocks.12.attn.c_qkv.bias',\n  'transformer.resblocks.12.attn.c_qkv.bias'),\n ('transformer.resblocks.12.attn.c_proj.weight',\n  'transformer.resblocks.12.attn.c_proj.weight'),\n ('transformer.resblocks.12.attn.c_proj.bias',\n  'transformer.resblocks.12.attn.c_proj.bias'),\n ('transformer.resblocks.12.ln_1.weight',\n  'transformer.resblocks.12.ln_1.weight'),\n ('transformer.resblocks.12.ln_1.bias', 'transformer.resblocks.12.ln_1.bias'),\n ('transformer.resblocks.12.mlp.c_fc.weight',\n  'transformer.resblocks.12.mlp.c_fc.weight'),\n ('transformer.resblocks.12.mlp.c_fc.bias',\n  'transformer.resblocks.12.mlp.c_fc.bias'),\n ('transformer.resblocks.12.mlp.c_proj.weight',\n  'transformer.resblocks.12.mlp.c_proj.weight'),\n ('transformer.resblocks.12.mlp.c_proj.bias',\n  'transformer.resblocks.12.mlp.c_proj.bias'),\n ('transformer.resblocks.12.ln_2.weight',\n  'transformer.resblocks.12.ln_2.weight'),\n ('transformer.resblocks.12.ln_2.bias', 'transformer.resblocks.12.ln_2.bias'),\n ('transformer.resblocks.13.attn.c_qkv.weight',\n  'transformer.resblocks.13.attn.c_qkv.weight'),\n ('transformer.resblocks.13.attn.c_qkv.bias',\n  'transformer.resblocks.13.attn.c_qkv.bias'),\n ('transformer.resblocks.13.attn.c_proj.weight',\n  'transformer.resblocks.13.attn.c_proj.weight'),\n ('transformer.resblocks.13.attn.c_proj.bias',\n  'transformer.resblocks.13.attn.c_proj.bias'),\n ('transformer.resblocks.13.ln_1.weight',\n  'transformer.resblocks.13.ln_1.weight'),\n ('transformer.resblocks.13.ln_1.bias', 'transformer.resblocks.13.ln_1.bias'),\n ('transformer.resblocks.13.mlp.c_fc.weight',\n  'transformer.resblocks.13.mlp.c_fc.weight'),\n ('transformer.resblocks.13.mlp.c_fc.bias',\n  'transformer.resblocks.13.mlp.c_fc.bias'),\n ('transformer.resblocks.13.mlp.c_proj.weight',\n  'transformer.resblocks.13.mlp.c_proj.weight'),\n ('transformer.resblocks.13.mlp.c_proj.bias',\n  'transformer.resblocks.13.mlp.c_proj.bias'),\n ('transformer.resblocks.13.ln_2.weight',\n  'transformer.resblocks.13.ln_2.weight'),\n ('transformer.resblocks.13.ln_2.bias', 'transformer.resblocks.13.ln_2.bias'),\n ('transformer.resblocks.14.attn.c_qkv.weight',\n  'transformer.resblocks.14.attn.c_qkv.weight'),\n ('transformer.resblocks.14.attn.c_qkv.bias',\n  'transformer.resblocks.14.attn.c_qkv.bias'),\n ('transformer.resblocks.14.attn.c_proj.weight',\n  'transformer.resblocks.14.attn.c_proj.weight'),\n ('transformer.resblocks.14.attn.c_proj.bias',\n  'transformer.resblocks.14.attn.c_proj.bias'),\n ('transformer.resblocks.14.ln_1.weight',\n  'transformer.resblocks.14.ln_1.weight'),\n ('transformer.resblocks.14.ln_1.bias', 'transformer.resblocks.14.ln_1.bias'),\n ('transformer.resblocks.14.mlp.c_fc.weight',\n  'transformer.resblocks.14.mlp.c_fc.weight'),\n ('transformer.resblocks.14.mlp.c_fc.bias',\n  'transformer.resblocks.14.mlp.c_fc.bias'),\n ('transformer.resblocks.14.mlp.c_proj.weight',\n  'transformer.resblocks.14.mlp.c_proj.weight'),\n ('transformer.resblocks.14.mlp.c_proj.bias',\n  'transformer.resblocks.14.mlp.c_proj.bias'),\n ('transformer.resblocks.14.ln_2.weight',\n  'transformer.resblocks.14.ln_2.weight'),\n ('transformer.resblocks.14.ln_2.bias', 'transformer.resblocks.14.ln_2.bias'),\n ('transformer.resblocks.15.attn.c_qkv.weight',\n  'transformer.resblocks.15.attn.c_qkv.weight'),\n ('transformer.resblocks.15.attn.c_qkv.bias',\n  'transformer.resblocks.15.attn.c_qkv.bias'),\n ('transformer.resblocks.15.attn.c_proj.weight',\n  'transformer.resblocks.15.attn.c_proj.weight'),\n ('transformer.resblocks.15.attn.c_proj.bias',\n  'transformer.resblocks.15.attn.c_proj.bias'),\n ('transformer.resblocks.15.ln_1.weight',\n  'transformer.resblocks.15.ln_1.weight'),\n ('transformer.resblocks.15.ln_1.bias', 'transformer.resblocks.15.ln_1.bias'),\n ('transformer.resblocks.15.mlp.c_fc.weight',\n  'transformer.resblocks.15.mlp.c_fc.weight'),\n ('transformer.resblocks.15.mlp.c_fc.bias',\n  'transformer.resblocks.15.mlp.c_fc.bias'),\n ('transformer.resblocks.15.mlp.c_proj.weight',\n  'transformer.resblocks.15.mlp.c_proj.weight'),\n ('transformer.resblocks.15.mlp.c_proj.bias',\n  'transformer.resblocks.15.mlp.c_proj.bias'),\n ('transformer.resblocks.15.ln_2.weight',\n  'transformer.resblocks.15.ln_2.weight'),\n ('transformer.resblocks.15.ln_2.bias', 'transformer.resblocks.15.ln_2.bias'),\n ('final_ln.weight', 'final_ln.weight'),\n ('final_ln.bias', 'final_ln.bias'),\n ('token_embedding.weight', 'token_embedding.weight'),\n ('transformer_proj.weight', 'transformer_proj.weight'),\n ('transformer_proj.bias', 'transformer_proj.bias')]"
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(len(model.state_dict().keys()), len(ref_model.state_dict().keys()))\n",
    "list(zip(model.state_dict().keys(), ref_model.state_dict().keys()))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Error(s) in loading state_dict for Text2Im:\n\tsize mismatch for out_layers.3.in_layers.2.weight: copying a param with shape torch.Size([768, 1344, 3, 3]) from checkpoint, the shape in current model is torch.Size([576, 1344, 3, 3]).\n\tsize mismatch for out_layers.3.in_layers.2.bias: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([576]).\n\tsize mismatch for out_layers.3.emb_layers.1.weight: copying a param with shape torch.Size([1536, 768]) from checkpoint, the shape in current model is torch.Size([1152, 768]).\n\tsize mismatch for out_layers.3.emb_layers.1.bias: copying a param with shape torch.Size([1536]) from checkpoint, the shape in current model is torch.Size([1152]).\n\tsize mismatch for out_layers.3.group_norm.weight: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([576]).\n\tsize mismatch for out_layers.3.group_norm.bias: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([576]).\n\tsize mismatch for out_layers.3.out_layers.2.weight: copying a param with shape torch.Size([768, 768, 3, 3]) from checkpoint, the shape in current model is torch.Size([576, 576, 3, 3]).\n\tsize mismatch for out_layers.3.out_layers.2.bias: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([576]).\n\tsize mismatch for out_layers.3.skip_connection.weight: copying a param with shape torch.Size([768, 1344, 1, 1]) from checkpoint, the shape in current model is torch.Size([576, 1344, 1, 1]).\n\tsize mismatch for out_layers.3.skip_connection.bias: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([576]).\n\tsize mismatch for out_layers.3.attention_layer.group_norm.weight: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([576]).\n\tsize mismatch for out_layers.3.attention_layer.group_norm.bias: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([576]).\n\tsize mismatch for out_layers.3.attention_layer.qkv.weight: copying a param with shape torch.Size([2304, 768, 1]) from checkpoint, the shape in current model is torch.Size([1728, 576, 1]).\n\tsize mismatch for out_layers.3.attention_layer.qkv.bias: copying a param with shape torch.Size([2304]) from checkpoint, the shape in current model is torch.Size([1728]).\n\tsize mismatch for out_layers.3.attention_layer.encoder_kv.weight: copying a param with shape torch.Size([1536, 512, 1]) from checkpoint, the shape in current model is torch.Size([1152, 512, 1]).\n\tsize mismatch for out_layers.3.attention_layer.encoder_kv.bias: copying a param with shape torch.Size([1536]) from checkpoint, the shape in current model is torch.Size([1152]).\n\tsize mismatch for out_layers.3.attention_layer.proj_out.weight: copying a param with shape torch.Size([768, 768, 1]) from checkpoint, the shape in current model is torch.Size([576, 576, 1]).\n\tsize mismatch for out_layers.3.attention_layer.proj_out.bias: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([576]).\n\tsize mismatch for out_layers.4.0.in_layers.0.weight: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([1152]).\n\tsize mismatch for out_layers.4.0.in_layers.0.bias: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([1152]).\n\tsize mismatch for out_layers.4.0.in_layers.2.weight: copying a param with shape torch.Size([768, 768, 3, 3]) from checkpoint, the shape in current model is torch.Size([576, 1152, 3, 3]).\n\tsize mismatch for out_layers.4.0.in_layers.2.bias: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([576]).\n\tsize mismatch for out_layers.4.0.emb_layers.1.weight: copying a param with shape torch.Size([1536, 768]) from checkpoint, the shape in current model is torch.Size([1152, 768]).\n\tsize mismatch for out_layers.4.0.emb_layers.1.bias: copying a param with shape torch.Size([1536]) from checkpoint, the shape in current model is torch.Size([1152]).\n\tsize mismatch for out_layers.4.0.group_norm.weight: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([576]).\n\tsize mismatch for out_layers.4.0.group_norm.bias: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([576]).\n\tsize mismatch for out_layers.4.0.out_layers.2.weight: copying a param with shape torch.Size([768, 768, 3, 3]) from checkpoint, the shape in current model is torch.Size([576, 576, 3, 3]).\n\tsize mismatch for out_layers.4.0.out_layers.2.bias: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([576]).\n\tsize mismatch for out_layers.4.0.skip_connection.weight: copying a param with shape torch.Size([1344]) from checkpoint, the shape in current model is torch.Size([576, 1152, 1, 1]).\n\tsize mismatch for out_layers.4.0.skip_connection.bias: copying a param with shape torch.Size([1344]) from checkpoint, the shape in current model is torch.Size([576]).\n\tsize mismatch for out_layers.4.0.attention_layer.group_norm.weight: copying a param with shape torch.Size([576, 1344, 3, 3]) from checkpoint, the shape in current model is torch.Size([576]).\n\tsize mismatch for out_layers.4.0.attention_layer.qkv.weight: copying a param with shape torch.Size([1152, 768]) from checkpoint, the shape in current model is torch.Size([1728, 576, 1]).\n\tsize mismatch for out_layers.4.0.attention_layer.qkv.bias: copying a param with shape torch.Size([1152]) from checkpoint, the shape in current model is torch.Size([1728]).\n\tsize mismatch for out_layers.4.0.attention_layer.encoder_kv.weight: copying a param with shape torch.Size([576]) from checkpoint, the shape in current model is torch.Size([1152, 512, 1]).\n\tsize mismatch for out_layers.4.0.attention_layer.encoder_kv.bias: copying a param with shape torch.Size([576]) from checkpoint, the shape in current model is torch.Size([1152]).\n\tsize mismatch for out_layers.4.0.attention_layer.proj_out.weight: copying a param with shape torch.Size([576, 576, 3, 3]) from checkpoint, the shape in current model is torch.Size([576, 576, 1]).\n\tsize mismatch for out_layers.4.1.in_layers.0.weight: copying a param with shape torch.Size([576, 1344, 1, 1]) from checkpoint, the shape in current model is torch.Size([576]).\n\tsize mismatch for out_layers.4.1.in_layers.2.weight: copying a param with shape torch.Size([576]) from checkpoint, the shape in current model is torch.Size([576, 576, 3, 3]).\n\tsize mismatch for out_layers.4.1.emb_layers.1.weight: copying a param with shape torch.Size([1728, 576, 1]) from checkpoint, the shape in current model is torch.Size([1152, 768]).\n\tsize mismatch for out_layers.4.1.emb_layers.1.bias: copying a param with shape torch.Size([1728]) from checkpoint, the shape in current model is torch.Size([1152]).\n\tsize mismatch for out_layers.4.1.group_norm.weight: copying a param with shape torch.Size([1152, 512, 1]) from checkpoint, the shape in current model is torch.Size([576]).\n\tsize mismatch for out_layers.4.1.group_norm.bias: copying a param with shape torch.Size([1152]) from checkpoint, the shape in current model is torch.Size([576]).\n\tsize mismatch for out_layers.4.1.out_layers.2.weight: copying a param with shape torch.Size([576, 576, 1]) from checkpoint, the shape in current model is torch.Size([576, 576, 3, 3]).\n\tsize mismatch for out_layers.7.in_layers.2.weight: copying a param with shape torch.Size([576, 960, 3, 3]) from checkpoint, the shape in current model is torch.Size([384, 960, 3, 3]).\n\tsize mismatch for out_layers.7.in_layers.2.bias: copying a param with shape torch.Size([576]) from checkpoint, the shape in current model is torch.Size([384]).\n\tsize mismatch for out_layers.7.emb_layers.1.weight: copying a param with shape torch.Size([1152, 768]) from checkpoint, the shape in current model is torch.Size([768, 768]).\n\tsize mismatch for out_layers.7.emb_layers.1.bias: copying a param with shape torch.Size([1152]) from checkpoint, the shape in current model is torch.Size([768]).\n\tsize mismatch for out_layers.7.group_norm.weight: copying a param with shape torch.Size([576]) from checkpoint, the shape in current model is torch.Size([384]).\n\tsize mismatch for out_layers.7.group_norm.bias: copying a param with shape torch.Size([576]) from checkpoint, the shape in current model is torch.Size([384]).\n\tsize mismatch for out_layers.7.out_layers.2.weight: copying a param with shape torch.Size([576, 576, 3, 3]) from checkpoint, the shape in current model is torch.Size([384, 384, 3, 3]).\n\tsize mismatch for out_layers.7.out_layers.2.bias: copying a param with shape torch.Size([576]) from checkpoint, the shape in current model is torch.Size([384]).\n\tsize mismatch for out_layers.7.skip_connection.weight: copying a param with shape torch.Size([576, 960, 1, 1]) from checkpoint, the shape in current model is torch.Size([384, 960, 1, 1]).\n\tsize mismatch for out_layers.7.skip_connection.bias: copying a param with shape torch.Size([576]) from checkpoint, the shape in current model is torch.Size([384]).\n\tsize mismatch for out_layers.7.attention_layer.group_norm.weight: copying a param with shape torch.Size([576]) from checkpoint, the shape in current model is torch.Size([384]).\n\tsize mismatch for out_layers.7.attention_layer.group_norm.bias: copying a param with shape torch.Size([576]) from checkpoint, the shape in current model is torch.Size([384]).\n\tsize mismatch for out_layers.7.attention_layer.qkv.weight: copying a param with shape torch.Size([1728, 576, 1]) from checkpoint, the shape in current model is torch.Size([1152, 384, 1]).\n\tsize mismatch for out_layers.7.attention_layer.qkv.bias: copying a param with shape torch.Size([1728]) from checkpoint, the shape in current model is torch.Size([1152]).\n\tsize mismatch for out_layers.7.attention_layer.encoder_kv.weight: copying a param with shape torch.Size([1152, 512, 1]) from checkpoint, the shape in current model is torch.Size([768, 512, 1]).\n\tsize mismatch for out_layers.7.attention_layer.encoder_kv.bias: copying a param with shape torch.Size([1152]) from checkpoint, the shape in current model is torch.Size([768]).\n\tsize mismatch for out_layers.7.attention_layer.proj_out.weight: copying a param with shape torch.Size([576, 576, 1]) from checkpoint, the shape in current model is torch.Size([384, 384, 1]).\n\tsize mismatch for out_layers.7.attention_layer.proj_out.bias: copying a param with shape torch.Size([576]) from checkpoint, the shape in current model is torch.Size([384]).\n\tsize mismatch for out_layers.8.0.in_layers.0.weight: copying a param with shape torch.Size([576]) from checkpoint, the shape in current model is torch.Size([768]).\n\tsize mismatch for out_layers.8.0.in_layers.0.bias: copying a param with shape torch.Size([576]) from checkpoint, the shape in current model is torch.Size([768]).\n\tsize mismatch for out_layers.8.0.in_layers.2.weight: copying a param with shape torch.Size([576, 576, 3, 3]) from checkpoint, the shape in current model is torch.Size([384, 768, 3, 3]).\n\tsize mismatch for out_layers.8.0.in_layers.2.bias: copying a param with shape torch.Size([576]) from checkpoint, the shape in current model is torch.Size([384]).\n\tsize mismatch for out_layers.8.0.emb_layers.1.weight: copying a param with shape torch.Size([1152, 768]) from checkpoint, the shape in current model is torch.Size([768, 768]).\n\tsize mismatch for out_layers.8.0.emb_layers.1.bias: copying a param with shape torch.Size([1152]) from checkpoint, the shape in current model is torch.Size([768]).\n\tsize mismatch for out_layers.8.0.group_norm.weight: copying a param with shape torch.Size([576]) from checkpoint, the shape in current model is torch.Size([384]).\n\tsize mismatch for out_layers.8.0.group_norm.bias: copying a param with shape torch.Size([576]) from checkpoint, the shape in current model is torch.Size([384]).\n\tsize mismatch for out_layers.8.0.out_layers.2.weight: copying a param with shape torch.Size([576, 576, 3, 3]) from checkpoint, the shape in current model is torch.Size([384, 384, 3, 3]).\n\tsize mismatch for out_layers.8.0.out_layers.2.bias: copying a param with shape torch.Size([576]) from checkpoint, the shape in current model is torch.Size([384]).\n\tsize mismatch for out_layers.8.0.skip_connection.weight: copying a param with shape torch.Size([960]) from checkpoint, the shape in current model is torch.Size([384, 768, 1, 1]).\n\tsize mismatch for out_layers.8.0.skip_connection.bias: copying a param with shape torch.Size([960]) from checkpoint, the shape in current model is torch.Size([384]).\n\tsize mismatch for out_layers.8.0.attention_layer.group_norm.weight: copying a param with shape torch.Size([384, 960, 3, 3]) from checkpoint, the shape in current model is torch.Size([384]).\n\tsize mismatch for out_layers.8.0.attention_layer.qkv.weight: copying a param with shape torch.Size([768, 768]) from checkpoint, the shape in current model is torch.Size([1152, 384, 1]).\n\tsize mismatch for out_layers.8.0.attention_layer.qkv.bias: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([1152]).\n\tsize mismatch for out_layers.8.0.attention_layer.encoder_kv.weight: copying a param with shape torch.Size([384]) from checkpoint, the shape in current model is torch.Size([768, 512, 1]).\n\tsize mismatch for out_layers.8.0.attention_layer.encoder_kv.bias: copying a param with shape torch.Size([384]) from checkpoint, the shape in current model is torch.Size([768]).\n\tsize mismatch for out_layers.8.0.attention_layer.proj_out.weight: copying a param with shape torch.Size([384, 384, 3, 3]) from checkpoint, the shape in current model is torch.Size([384, 384, 1]).\n\tsize mismatch for out_layers.8.1.in_layers.0.weight: copying a param with shape torch.Size([384, 960, 1, 1]) from checkpoint, the shape in current model is torch.Size([384]).\n\tsize mismatch for out_layers.8.1.in_layers.2.weight: copying a param with shape torch.Size([384]) from checkpoint, the shape in current model is torch.Size([384, 384, 3, 3]).\n\tsize mismatch for out_layers.8.1.emb_layers.1.weight: copying a param with shape torch.Size([1152, 384, 1]) from checkpoint, the shape in current model is torch.Size([768, 768]).\n\tsize mismatch for out_layers.8.1.emb_layers.1.bias: copying a param with shape torch.Size([1152]) from checkpoint, the shape in current model is torch.Size([768]).\n\tsize mismatch for out_layers.8.1.group_norm.weight: copying a param with shape torch.Size([768, 512, 1]) from checkpoint, the shape in current model is torch.Size([384]).\n\tsize mismatch for out_layers.8.1.group_norm.bias: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([384]).\n\tsize mismatch for out_layers.8.1.out_layers.2.weight: copying a param with shape torch.Size([384, 384, 1]) from checkpoint, the shape in current model is torch.Size([384, 384, 3, 3]).\n\tsize mismatch for out_layers.11.in_layers.2.weight: copying a param with shape torch.Size([384, 576, 3, 3]) from checkpoint, the shape in current model is torch.Size([192, 576, 3, 3]).\n\tsize mismatch for out_layers.11.in_layers.2.bias: copying a param with shape torch.Size([384]) from checkpoint, the shape in current model is torch.Size([192]).\n\tsize mismatch for out_layers.11.emb_layers.1.weight: copying a param with shape torch.Size([768, 768]) from checkpoint, the shape in current model is torch.Size([384, 768]).\n\tsize mismatch for out_layers.11.emb_layers.1.bias: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([384]).\n\tsize mismatch for out_layers.11.group_norm.weight: copying a param with shape torch.Size([384]) from checkpoint, the shape in current model is torch.Size([192]).\n\tsize mismatch for out_layers.11.group_norm.bias: copying a param with shape torch.Size([384]) from checkpoint, the shape in current model is torch.Size([192]).\n\tsize mismatch for out_layers.11.out_layers.2.weight: copying a param with shape torch.Size([384, 384, 3, 3]) from checkpoint, the shape in current model is torch.Size([192, 192, 3, 3]).\n\tsize mismatch for out_layers.11.out_layers.2.bias: copying a param with shape torch.Size([384]) from checkpoint, the shape in current model is torch.Size([192]).\n\tsize mismatch for out_layers.11.skip_connection.weight: copying a param with shape torch.Size([384, 576, 1, 1]) from checkpoint, the shape in current model is torch.Size([192, 576, 1, 1]).\n\tsize mismatch for out_layers.11.skip_connection.bias: copying a param with shape torch.Size([384]) from checkpoint, the shape in current model is torch.Size([192]).\n\tsize mismatch for out_layers.11.attention_layer.group_norm.weight: copying a param with shape torch.Size([384]) from checkpoint, the shape in current model is torch.Size([192]).\n\tsize mismatch for out_layers.11.attention_layer.group_norm.bias: copying a param with shape torch.Size([384]) from checkpoint, the shape in current model is torch.Size([192]).\n\tsize mismatch for out_layers.11.attention_layer.qkv.weight: copying a param with shape torch.Size([1152, 384, 1]) from checkpoint, the shape in current model is torch.Size([576, 192, 1]).\n\tsize mismatch for out_layers.11.attention_layer.qkv.bias: copying a param with shape torch.Size([1152]) from checkpoint, the shape in current model is torch.Size([576]).\n\tsize mismatch for out_layers.11.attention_layer.encoder_kv.weight: copying a param with shape torch.Size([768, 512, 1]) from checkpoint, the shape in current model is torch.Size([384, 512, 1]).\n\tsize mismatch for out_layers.11.attention_layer.encoder_kv.bias: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([384]).\n\tsize mismatch for out_layers.11.attention_layer.proj_out.weight: copying a param with shape torch.Size([384, 384, 1]) from checkpoint, the shape in current model is torch.Size([192, 192, 1]).\n\tsize mismatch for out_layers.11.attention_layer.proj_out.bias: copying a param with shape torch.Size([384]) from checkpoint, the shape in current model is torch.Size([192]).\n\tsize mismatch for out_layers.12.0.in_layers.2.weight: copying a param with shape torch.Size([384, 384, 3, 3]) from checkpoint, the shape in current model is torch.Size([192, 384, 3, 3]).\n\tsize mismatch for out_layers.12.0.in_layers.2.bias: copying a param with shape torch.Size([384]) from checkpoint, the shape in current model is torch.Size([192]).\n\tsize mismatch for out_layers.12.0.emb_layers.1.weight: copying a param with shape torch.Size([768, 768]) from checkpoint, the shape in current model is torch.Size([384, 768]).\n\tsize mismatch for out_layers.12.0.emb_layers.1.bias: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([384]).\n\tsize mismatch for out_layers.12.0.group_norm.weight: copying a param with shape torch.Size([384]) from checkpoint, the shape in current model is torch.Size([192]).\n\tsize mismatch for out_layers.12.0.group_norm.bias: copying a param with shape torch.Size([384]) from checkpoint, the shape in current model is torch.Size([192]).\n\tsize mismatch for out_layers.12.0.out_layers.2.weight: copying a param with shape torch.Size([384, 384, 3, 3]) from checkpoint, the shape in current model is torch.Size([192, 192, 3, 3]).\n\tsize mismatch for out_layers.12.0.out_layers.2.bias: copying a param with shape torch.Size([384]) from checkpoint, the shape in current model is torch.Size([192]).\n\tsize mismatch for out_layers.12.0.skip_connection.weight: copying a param with shape torch.Size([576]) from checkpoint, the shape in current model is torch.Size([192, 384, 1, 1]).\n\tsize mismatch for out_layers.12.0.skip_connection.bias: copying a param with shape torch.Size([576]) from checkpoint, the shape in current model is torch.Size([192]).\n\tsize mismatch for out_layers.12.1.in_layers.0.weight: copying a param with shape torch.Size([192, 576, 3, 3]) from checkpoint, the shape in current model is torch.Size([192]).\n\tsize mismatch for out_layers.12.1.in_layers.2.weight: copying a param with shape torch.Size([384, 768]) from checkpoint, the shape in current model is torch.Size([192, 192, 3, 3]).\n\tsize mismatch for out_layers.12.1.in_layers.2.bias: copying a param with shape torch.Size([384]) from checkpoint, the shape in current model is torch.Size([192]).\n\tsize mismatch for out_layers.12.1.emb_layers.1.weight: copying a param with shape torch.Size([192]) from checkpoint, the shape in current model is torch.Size([384, 768]).\n\tsize mismatch for out_layers.12.1.emb_layers.1.bias: copying a param with shape torch.Size([192]) from checkpoint, the shape in current model is torch.Size([384]).\n\tsize mismatch for out_layers.12.1.group_norm.weight: copying a param with shape torch.Size([192, 192, 3, 3]) from checkpoint, the shape in current model is torch.Size([192]).\n\tsize mismatch for out_layers.12.1.out_layers.2.weight: copying a param with shape torch.Size([192, 576, 1, 1]) from checkpoint, the shape in current model is torch.Size([192, 192, 3, 3]).",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mRuntimeError\u001B[0m                              Traceback (most recent call last)",
      "\u001B[0;32m/tmp/ipykernel_49155/1057984657.py\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[1;32m      2\u001B[0m \u001B[0mmapped\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0;34m{\u001B[0m\u001B[0mours\u001B[0m\u001B[0;34m:\u001B[0m \u001B[0mstate\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0mtheirs\u001B[0m\u001B[0;34m]\u001B[0m \u001B[0;32mfor\u001B[0m \u001B[0mours\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mtheirs\u001B[0m \u001B[0;32min\u001B[0m \u001B[0mzip\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mmodel\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mstate_dict\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mkeys\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mref_model\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mstate_dict\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mkeys\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m}\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      3\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m----> 4\u001B[0;31m \u001B[0mmodel\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mload_state_dict\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mmapped\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m",
      "\u001B[0;32m/usr/lib/python3.10/site-packages/torch/nn/modules/module.py\u001B[0m in \u001B[0;36mload_state_dict\u001B[0;34m(self, state_dict, strict)\u001B[0m\n\u001B[1;32m   1480\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1481\u001B[0m         \u001B[0;32mif\u001B[0m \u001B[0mlen\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0merror_msgs\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;34m>\u001B[0m \u001B[0;36m0\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m-> 1482\u001B[0;31m             raise RuntimeError('Error(s) in loading state_dict for {}:\\n\\t{}'.format(\n\u001B[0m\u001B[1;32m   1483\u001B[0m                                self.__class__.__name__, \"\\n\\t\".join(error_msgs)))\n\u001B[1;32m   1484\u001B[0m         \u001B[0;32mreturn\u001B[0m \u001B[0m_IncompatibleKeys\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mmissing_keys\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0munexpected_keys\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;31mRuntimeError\u001B[0m: Error(s) in loading state_dict for Text2Im:\n\tsize mismatch for out_layers.3.in_layers.2.weight: copying a param with shape torch.Size([768, 1344, 3, 3]) from checkpoint, the shape in current model is torch.Size([576, 1344, 3, 3]).\n\tsize mismatch for out_layers.3.in_layers.2.bias: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([576]).\n\tsize mismatch for out_layers.3.emb_layers.1.weight: copying a param with shape torch.Size([1536, 768]) from checkpoint, the shape in current model is torch.Size([1152, 768]).\n\tsize mismatch for out_layers.3.emb_layers.1.bias: copying a param with shape torch.Size([1536]) from checkpoint, the shape in current model is torch.Size([1152]).\n\tsize mismatch for out_layers.3.group_norm.weight: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([576]).\n\tsize mismatch for out_layers.3.group_norm.bias: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([576]).\n\tsize mismatch for out_layers.3.out_layers.2.weight: copying a param with shape torch.Size([768, 768, 3, 3]) from checkpoint, the shape in current model is torch.Size([576, 576, 3, 3]).\n\tsize mismatch for out_layers.3.out_layers.2.bias: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([576]).\n\tsize mismatch for out_layers.3.skip_connection.weight: copying a param with shape torch.Size([768, 1344, 1, 1]) from checkpoint, the shape in current model is torch.Size([576, 1344, 1, 1]).\n\tsize mismatch for out_layers.3.skip_connection.bias: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([576]).\n\tsize mismatch for out_layers.3.attention_layer.group_norm.weight: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([576]).\n\tsize mismatch for out_layers.3.attention_layer.group_norm.bias: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([576]).\n\tsize mismatch for out_layers.3.attention_layer.qkv.weight: copying a param with shape torch.Size([2304, 768, 1]) from checkpoint, the shape in current model is torch.Size([1728, 576, 1]).\n\tsize mismatch for out_layers.3.attention_layer.qkv.bias: copying a param with shape torch.Size([2304]) from checkpoint, the shape in current model is torch.Size([1728]).\n\tsize mismatch for out_layers.3.attention_layer.encoder_kv.weight: copying a param with shape torch.Size([1536, 512, 1]) from checkpoint, the shape in current model is torch.Size([1152, 512, 1]).\n\tsize mismatch for out_layers.3.attention_layer.encoder_kv.bias: copying a param with shape torch.Size([1536]) from checkpoint, the shape in current model is torch.Size([1152]).\n\tsize mismatch for out_layers.3.attention_layer.proj_out.weight: copying a param with shape torch.Size([768, 768, 1]) from checkpoint, the shape in current model is torch.Size([576, 576, 1]).\n\tsize mismatch for out_layers.3.attention_layer.proj_out.bias: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([576]).\n\tsize mismatch for out_layers.4.0.in_layers.0.weight: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([1152]).\n\tsize mismatch for out_layers.4.0.in_layers.0.bias: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([1152]).\n\tsize mismatch for out_layers.4.0.in_layers.2.weight: copying a param with shape torch.Size([768, 768, 3, 3]) from checkpoint, the shape in current model is torch.Size([576, 1152, 3, 3]).\n\tsize mismatch for out_layers.4.0.in_layers.2.bias: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([576]).\n\tsize mismatch for out_layers.4.0.emb_layers.1.weight: copying a param with shape torch.Size([1536, 768]) from checkpoint, the shape in current model is torch.Size([1152, 768]).\n\tsize mismatch for out_layers.4.0.emb_layers.1.bias: copying a param with shape torch.Size([1536]) from checkpoint, the shape in current model is torch.Size([1152]).\n\tsize mismatch for out_layers.4.0.group_norm.weight: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([576]).\n\tsize mismatch for out_layers.4.0.group_norm.bias: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([576]).\n\tsize mismatch for out_layers.4.0.out_layers.2.weight: copying a param with shape torch.Size([768, 768, 3, 3]) from checkpoint, the shape in current model is torch.Size([576, 576, 3, 3]).\n\tsize mismatch for out_layers.4.0.out_layers.2.bias: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([576]).\n\tsize mismatch for out_layers.4.0.skip_connection.weight: copying a param with shape torch.Size([1344]) from checkpoint, the shape in current model is torch.Size([576, 1152, 1, 1]).\n\tsize mismatch for out_layers.4.0.skip_connection.bias: copying a param with shape torch.Size([1344]) from checkpoint, the shape in current model is torch.Size([576]).\n\tsize mismatch for out_layers.4.0.attention_layer.group_norm.weight: copying a param with shape torch.Size([576, 1344, 3, 3]) from checkpoint, the shape in current model is torch.Size([576]).\n\tsize mismatch for out_layers.4.0.attention_layer.qkv.weight: copying a param with shape torch.Size([1152, 768]) from checkpoint, the shape in current model is torch.Size([1728, 576, 1]).\n\tsize mismatch for out_layers.4.0.attention_layer.qkv.bias: copying a param with shape torch.Size([1152]) from checkpoint, the shape in current model is torch.Size([1728]).\n\tsize mismatch for out_layers.4.0.attention_layer.encoder_kv.weight: copying a param with shape torch.Size([576]) from checkpoint, the shape in current model is torch.Size([1152, 512, 1]).\n\tsize mismatch for out_layers.4.0.attention_layer.encoder_kv.bias: copying a param with shape torch.Size([576]) from checkpoint, the shape in current model is torch.Size([1152]).\n\tsize mismatch for out_layers.4.0.attention_layer.proj_out.weight: copying a param with shape torch.Size([576, 576, 3, 3]) from checkpoint, the shape in current model is torch.Size([576, 576, 1]).\n\tsize mismatch for out_layers.4.1.in_layers.0.weight: copying a param with shape torch.Size([576, 1344, 1, 1]) from checkpoint, the shape in current model is torch.Size([576]).\n\tsize mismatch for out_layers.4.1.in_layers.2.weight: copying a param with shape torch.Size([576]) from checkpoint, the shape in current model is torch.Size([576, 576, 3, 3]).\n\tsize mismatch for out_layers.4.1.emb_layers.1.weight: copying a param with shape torch.Size([1728, 576, 1]) from checkpoint, the shape in current model is torch.Size([1152, 768]).\n\tsize mismatch for out_layers.4.1.emb_layers.1.bias: copying a param with shape torch.Size([1728]) from checkpoint, the shape in current model is torch.Size([1152]).\n\tsize mismatch for out_layers.4.1.group_norm.weight: copying a param with shape torch.Size([1152, 512, 1]) from checkpoint, the shape in current model is torch.Size([576]).\n\tsize mismatch for out_layers.4.1.group_norm.bias: copying a param with shape torch.Size([1152]) from checkpoint, the shape in current model is torch.Size([576]).\n\tsize mismatch for out_layers.4.1.out_layers.2.weight: copying a param with shape torch.Size([576, 576, 1]) from checkpoint, the shape in current model is torch.Size([576, 576, 3, 3]).\n\tsize mismatch for out_layers.7.in_layers.2.weight: copying a param with shape torch.Size([576, 960, 3, 3]) from checkpoint, the shape in current model is torch.Size([384, 960, 3, 3]).\n\tsize mismatch for out_layers.7.in_layers.2.bias: copying a param with shape torch.Size([576]) from checkpoint, the shape in current model is torch.Size([384]).\n\tsize mismatch for out_layers.7.emb_layers.1.weight: copying a param with shape torch.Size([1152, 768]) from checkpoint, the shape in current model is torch.Size([768, 768]).\n\tsize mismatch for out_layers.7.emb_layers.1.bias: copying a param with shape torch.Size([1152]) from checkpoint, the shape in current model is torch.Size([768]).\n\tsize mismatch for out_layers.7.group_norm.weight: copying a param with shape torch.Size([576]) from checkpoint, the shape in current model is torch.Size([384]).\n\tsize mismatch for out_layers.7.group_norm.bias: copying a param with shape torch.Size([576]) from checkpoint, the shape in current model is torch.Size([384]).\n\tsize mismatch for out_layers.7.out_layers.2.weight: copying a param with shape torch.Size([576, 576, 3, 3]) from checkpoint, the shape in current model is torch.Size([384, 384, 3, 3]).\n\tsize mismatch for out_layers.7.out_layers.2.bias: copying a param with shape torch.Size([576]) from checkpoint, the shape in current model is torch.Size([384]).\n\tsize mismatch for out_layers.7.skip_connection.weight: copying a param with shape torch.Size([576, 960, 1, 1]) from checkpoint, the shape in current model is torch.Size([384, 960, 1, 1]).\n\tsize mismatch for out_layers.7.skip_connection.bias: copying a param with shape torch.Size([576]) from checkpoint, the shape in current model is torch.Size([384]).\n\tsize mismatch for out_layers.7.attention_layer.group_norm.weight: copying a param with shape torch.Size([576]) from checkpoint, the shape in current model is torch.Size([384]).\n\tsize mismatch for out_layers.7.attention_layer.group_norm.bias: copying a param with shape torch.Size([576]) from checkpoint, the shape in current model is torch.Size([384]).\n\tsize mismatch for out_layers.7.attention_layer.qkv.weight: copying a param with shape torch.Size([1728, 576, 1]) from checkpoint, the shape in current model is torch.Size([1152, 384, 1]).\n\tsize mismatch for out_layers.7.attention_layer.qkv.bias: copying a param with shape torch.Size([1728]) from checkpoint, the shape in current model is torch.Size([1152]).\n\tsize mismatch for out_layers.7.attention_layer.encoder_kv.weight: copying a param with shape torch.Size([1152, 512, 1]) from checkpoint, the shape in current model is torch.Size([768, 512, 1]).\n\tsize mismatch for out_layers.7.attention_layer.encoder_kv.bias: copying a param with shape torch.Size([1152]) from checkpoint, the shape in current model is torch.Size([768]).\n\tsize mismatch for out_layers.7.attention_layer.proj_out.weight: copying a param with shape torch.Size([576, 576, 1]) from checkpoint, the shape in current model is torch.Size([384, 384, 1]).\n\tsize mismatch for out_layers.7.attention_layer.proj_out.bias: copying a param with shape torch.Size([576]) from checkpoint, the shape in current model is torch.Size([384]).\n\tsize mismatch for out_layers.8.0.in_layers.0.weight: copying a param with shape torch.Size([576]) from checkpoint, the shape in current model is torch.Size([768]).\n\tsize mismatch for out_layers.8.0.in_layers.0.bias: copying a param with shape torch.Size([576]) from checkpoint, the shape in current model is torch.Size([768]).\n\tsize mismatch for out_layers.8.0.in_layers.2.weight: copying a param with shape torch.Size([576, 576, 3, 3]) from checkpoint, the shape in current model is torch.Size([384, 768, 3, 3]).\n\tsize mismatch for out_layers.8.0.in_layers.2.bias: copying a param with shape torch.Size([576]) from checkpoint, the shape in current model is torch.Size([384]).\n\tsize mismatch for out_layers.8.0.emb_layers.1.weight: copying a param with shape torch.Size([1152, 768]) from checkpoint, the shape in current model is torch.Size([768, 768]).\n\tsize mismatch for out_layers.8.0.emb_layers.1.bias: copying a param with shape torch.Size([1152]) from checkpoint, the shape in current model is torch.Size([768]).\n\tsize mismatch for out_layers.8.0.group_norm.weight: copying a param with shape torch.Size([576]) from checkpoint, the shape in current model is torch.Size([384]).\n\tsize mismatch for out_layers.8.0.group_norm.bias: copying a param with shape torch.Size([576]) from checkpoint, the shape in current model is torch.Size([384]).\n\tsize mismatch for out_layers.8.0.out_layers.2.weight: copying a param with shape torch.Size([576, 576, 3, 3]) from checkpoint, the shape in current model is torch.Size([384, 384, 3, 3]).\n\tsize mismatch for out_layers.8.0.out_layers.2.bias: copying a param with shape torch.Size([576]) from checkpoint, the shape in current model is torch.Size([384]).\n\tsize mismatch for out_layers.8.0.skip_connection.weight: copying a param with shape torch.Size([960]) from checkpoint, the shape in current model is torch.Size([384, 768, 1, 1]).\n\tsize mismatch for out_layers.8.0.skip_connection.bias: copying a param with shape torch.Size([960]) from checkpoint, the shape in current model is torch.Size([384]).\n\tsize mismatch for out_layers.8.0.attention_layer.group_norm.weight: copying a param with shape torch.Size([384, 960, 3, 3]) from checkpoint, the shape in current model is torch.Size([384]).\n\tsize mismatch for out_layers.8.0.attention_layer.qkv.weight: copying a param with shape torch.Size([768, 768]) from checkpoint, the shape in current model is torch.Size([1152, 384, 1]).\n\tsize mismatch for out_layers.8.0.attention_layer.qkv.bias: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([1152]).\n\tsize mismatch for out_layers.8.0.attention_layer.encoder_kv.weight: copying a param with shape torch.Size([384]) from checkpoint, the shape in current model is torch.Size([768, 512, 1]).\n\tsize mismatch for out_layers.8.0.attention_layer.encoder_kv.bias: copying a param with shape torch.Size([384]) from checkpoint, the shape in current model is torch.Size([768]).\n\tsize mismatch for out_layers.8.0.attention_layer.proj_out.weight: copying a param with shape torch.Size([384, 384, 3, 3]) from checkpoint, the shape in current model is torch.Size([384, 384, 1]).\n\tsize mismatch for out_layers.8.1.in_layers.0.weight: copying a param with shape torch.Size([384, 960, 1, 1]) from checkpoint, the shape in current model is torch.Size([384]).\n\tsize mismatch for out_layers.8.1.in_layers.2.weight: copying a param with shape torch.Size([384]) from checkpoint, the shape in current model is torch.Size([384, 384, 3, 3]).\n\tsize mismatch for out_layers.8.1.emb_layers.1.weight: copying a param with shape torch.Size([1152, 384, 1]) from checkpoint, the shape in current model is torch.Size([768, 768]).\n\tsize mismatch for out_layers.8.1.emb_layers.1.bias: copying a param with shape torch.Size([1152]) from checkpoint, the shape in current model is torch.Size([768]).\n\tsize mismatch for out_layers.8.1.group_norm.weight: copying a param with shape torch.Size([768, 512, 1]) from checkpoint, the shape in current model is torch.Size([384]).\n\tsize mismatch for out_layers.8.1.group_norm.bias: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([384]).\n\tsize mismatch for out_layers.8.1.out_layers.2.weight: copying a param with shape torch.Size([384, 384, 1]) from checkpoint, the shape in current model is torch.Size([384, 384, 3, 3]).\n\tsize mismatch for out_layers.11.in_layers.2.weight: copying a param with shape torch.Size([384, 576, 3, 3]) from checkpoint, the shape in current model is torch.Size([192, 576, 3, 3]).\n\tsize mismatch for out_layers.11.in_layers.2.bias: copying a param with shape torch.Size([384]) from checkpoint, the shape in current model is torch.Size([192]).\n\tsize mismatch for out_layers.11.emb_layers.1.weight: copying a param with shape torch.Size([768, 768]) from checkpoint, the shape in current model is torch.Size([384, 768]).\n\tsize mismatch for out_layers.11.emb_layers.1.bias: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([384]).\n\tsize mismatch for out_layers.11.group_norm.weight: copying a param with shape torch.Size([384]) from checkpoint, the shape in current model is torch.Size([192]).\n\tsize mismatch for out_layers.11.group_norm.bias: copying a param with shape torch.Size([384]) from checkpoint, the shape in current model is torch.Size([192]).\n\tsize mismatch for out_layers.11.out_layers.2.weight: copying a param with shape torch.Size([384, 384, 3, 3]) from checkpoint, the shape in current model is torch.Size([192, 192, 3, 3]).\n\tsize mismatch for out_layers.11.out_layers.2.bias: copying a param with shape torch.Size([384]) from checkpoint, the shape in current model is torch.Size([192]).\n\tsize mismatch for out_layers.11.skip_connection.weight: copying a param with shape torch.Size([384, 576, 1, 1]) from checkpoint, the shape in current model is torch.Size([192, 576, 1, 1]).\n\tsize mismatch for out_layers.11.skip_connection.bias: copying a param with shape torch.Size([384]) from checkpoint, the shape in current model is torch.Size([192]).\n\tsize mismatch for out_layers.11.attention_layer.group_norm.weight: copying a param with shape torch.Size([384]) from checkpoint, the shape in current model is torch.Size([192]).\n\tsize mismatch for out_layers.11.attention_layer.group_norm.bias: copying a param with shape torch.Size([384]) from checkpoint, the shape in current model is torch.Size([192]).\n\tsize mismatch for out_layers.11.attention_layer.qkv.weight: copying a param with shape torch.Size([1152, 384, 1]) from checkpoint, the shape in current model is torch.Size([576, 192, 1]).\n\tsize mismatch for out_layers.11.attention_layer.qkv.bias: copying a param with shape torch.Size([1152]) from checkpoint, the shape in current model is torch.Size([576]).\n\tsize mismatch for out_layers.11.attention_layer.encoder_kv.weight: copying a param with shape torch.Size([768, 512, 1]) from checkpoint, the shape in current model is torch.Size([384, 512, 1]).\n\tsize mismatch for out_layers.11.attention_layer.encoder_kv.bias: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([384]).\n\tsize mismatch for out_layers.11.attention_layer.proj_out.weight: copying a param with shape torch.Size([384, 384, 1]) from checkpoint, the shape in current model is torch.Size([192, 192, 1]).\n\tsize mismatch for out_layers.11.attention_layer.proj_out.bias: copying a param with shape torch.Size([384]) from checkpoint, the shape in current model is torch.Size([192]).\n\tsize mismatch for out_layers.12.0.in_layers.2.weight: copying a param with shape torch.Size([384, 384, 3, 3]) from checkpoint, the shape in current model is torch.Size([192, 384, 3, 3]).\n\tsize mismatch for out_layers.12.0.in_layers.2.bias: copying a param with shape torch.Size([384]) from checkpoint, the shape in current model is torch.Size([192]).\n\tsize mismatch for out_layers.12.0.emb_layers.1.weight: copying a param with shape torch.Size([768, 768]) from checkpoint, the shape in current model is torch.Size([384, 768]).\n\tsize mismatch for out_layers.12.0.emb_layers.1.bias: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([384]).\n\tsize mismatch for out_layers.12.0.group_norm.weight: copying a param with shape torch.Size([384]) from checkpoint, the shape in current model is torch.Size([192]).\n\tsize mismatch for out_layers.12.0.group_norm.bias: copying a param with shape torch.Size([384]) from checkpoint, the shape in current model is torch.Size([192]).\n\tsize mismatch for out_layers.12.0.out_layers.2.weight: copying a param with shape torch.Size([384, 384, 3, 3]) from checkpoint, the shape in current model is torch.Size([192, 192, 3, 3]).\n\tsize mismatch for out_layers.12.0.out_layers.2.bias: copying a param with shape torch.Size([384]) from checkpoint, the shape in current model is torch.Size([192]).\n\tsize mismatch for out_layers.12.0.skip_connection.weight: copying a param with shape torch.Size([576]) from checkpoint, the shape in current model is torch.Size([192, 384, 1, 1]).\n\tsize mismatch for out_layers.12.0.skip_connection.bias: copying a param with shape torch.Size([576]) from checkpoint, the shape in current model is torch.Size([192]).\n\tsize mismatch for out_layers.12.1.in_layers.0.weight: copying a param with shape torch.Size([192, 576, 3, 3]) from checkpoint, the shape in current model is torch.Size([192]).\n\tsize mismatch for out_layers.12.1.in_layers.2.weight: copying a param with shape torch.Size([384, 768]) from checkpoint, the shape in current model is torch.Size([192, 192, 3, 3]).\n\tsize mismatch for out_layers.12.1.in_layers.2.bias: copying a param with shape torch.Size([384]) from checkpoint, the shape in current model is torch.Size([192]).\n\tsize mismatch for out_layers.12.1.emb_layers.1.weight: copying a param with shape torch.Size([192]) from checkpoint, the shape in current model is torch.Size([384, 768]).\n\tsize mismatch for out_layers.12.1.emb_layers.1.bias: copying a param with shape torch.Size([192]) from checkpoint, the shape in current model is torch.Size([384]).\n\tsize mismatch for out_layers.12.1.group_norm.weight: copying a param with shape torch.Size([192, 192, 3, 3]) from checkpoint, the shape in current model is torch.Size([192]).\n\tsize mismatch for out_layers.12.1.out_layers.2.weight: copying a param with shape torch.Size([192, 576, 1, 1]) from checkpoint, the shape in current model is torch.Size([192, 192, 3, 3])."
     ]
    }
   ],
   "source": [
    "state = ref_model.state_dict()\n",
    "mapped = {ours: state[theirs] for ours, theirs in zip(model.state_dict().keys(), ref_model.state_dict().keys())}\n",
    "\n",
    "model.load_state_dict(mapped)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}