{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [
    {
     "data": {
      "text/plain": "<All keys matched successfully>"
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append(\"./reference\")\n",
    "import torch as t\n",
    "from reference.glide_text2im.download import load_checkpoint\n",
    "from reference.glide_text2im.model_creation import (\n",
    "    create_model_and_diffusion,\n",
    "    model_and_diffusion_defaults,\n",
    ")\n",
    "\n",
    "device = t.device('cpu')\n",
    "options = model_and_diffusion_defaults()\n",
    "options['timestep_respacing'] = '100'\n",
    "ref_model, diffusion = create_model_and_diffusion(**options)\n",
    "ref_model.load_state_dict(load_checkpoint('base', device))\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "from unet import Text2Im\n",
    "from reference.glide_text2im.tokenizer.bpe import get_encoder"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "data": {
      "text/plain": "Text2Im(\n  (time_embed): Sequential(\n    (0): Linear(in_features=192, out_features=768, bias=True)\n    (1): SiLU()\n    (2): Linear(in_features=768, out_features=768, bias=True)\n  )\n  (in_layers): Sequential(\n    (0): Conv2d(3, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (1): ResidualBlock(\n      (in_layers): Sequential(\n        (0): GroupNorm(32, 192, eps=1e-05, affine=True)\n        (1): SiLU()\n        (2): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n      )\n      (emb_layers): Sequential(\n        (0): SiLU()\n        (1): Linear(in_features=768, out_features=384, bias=True)\n      )\n      (group_norm): GroupNorm(32, 192, eps=1e-05, affine=True)\n      (out_layers): Sequential(\n        (0): SiLU()\n        (1): Dropout(p=0.0, inplace=False)\n        (2): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n      )\n    )\n    (2): ResidualBlock(\n      (in_layers): Sequential(\n        (0): GroupNorm(32, 192, eps=1e-05, affine=True)\n        (1): SiLU()\n        (2): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n      )\n      (emb_layers): Sequential(\n        (0): SiLU()\n        (1): Linear(in_features=768, out_features=384, bias=True)\n      )\n      (group_norm): GroupNorm(32, 192, eps=1e-05, affine=True)\n      (out_layers): Sequential(\n        (0): SiLU()\n        (1): Dropout(p=0.0, inplace=False)\n        (2): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n      )\n    )\n    (3): ResidualBlock(\n      (in_layers): Sequential(\n        (0): GroupNorm(32, 192, eps=1e-05, affine=True)\n        (1): SiLU()\n        (2): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n      )\n      (emb_layers): Sequential(\n        (0): SiLU()\n        (1): Linear(in_features=768, out_features=384, bias=True)\n      )\n      (group_norm): GroupNorm(32, 192, eps=1e-05, affine=True)\n      (out_layers): Sequential(\n        (0): SiLU()\n        (1): Dropout(p=0.0, inplace=False)\n        (2): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n      )\n    )\n    (4): DownResidualBlock(\n      (in_layers): Sequential(\n        (0): GroupNorm(32, 192, eps=1e-05, affine=True)\n        (1): SiLU()\n        (2): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n      )\n      (emb_layers): Sequential(\n        (0): SiLU()\n        (1): Linear(in_features=768, out_features=384, bias=True)\n      )\n      (group_norm): GroupNorm(32, 192, eps=1e-05, affine=True)\n      (out_layers): Sequential(\n        (0): SiLU()\n        (1): Dropout(p=0.0, inplace=False)\n        (2): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n      )\n    )\n    (5): ResidualBlock(\n      (in_layers): Sequential(\n        (0): GroupNorm(32, 192, eps=1e-05, affine=True)\n        (1): SiLU()\n        (2): Conv2d(192, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n      )\n      (emb_layers): Sequential(\n        (0): SiLU()\n        (1): Linear(in_features=768, out_features=768, bias=True)\n      )\n      (group_norm): GroupNorm(32, 384, eps=1e-05, affine=True)\n      (out_layers): Sequential(\n        (0): SiLU()\n        (1): Dropout(p=0.0, inplace=False)\n        (2): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n      )\n      (skip_connection): Conv2d(192, 384, kernel_size=(1, 1), stride=(1, 1))\n      (attention_layer): AttentionBlock(\n        (group_norm): GroupNorm(32, 384, eps=1e-05, affine=True)\n        (qkv): Conv1d(384, 1152, kernel_size=(1,), stride=(1,))\n        (attention): QKVAttention()\n        (encoder_kv): Conv1d(512, 768, kernel_size=(1,), stride=(1,))\n        (proj_out): Conv1d(384, 384, kernel_size=(1,), stride=(1,))\n      )\n    )\n    (6): ResidualBlock(\n      (in_layers): Sequential(\n        (0): GroupNorm(32, 384, eps=1e-05, affine=True)\n        (1): SiLU()\n        (2): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n      )\n      (emb_layers): Sequential(\n        (0): SiLU()\n        (1): Linear(in_features=768, out_features=768, bias=True)\n      )\n      (group_norm): GroupNorm(32, 384, eps=1e-05, affine=True)\n      (out_layers): Sequential(\n        (0): SiLU()\n        (1): Dropout(p=0.0, inplace=False)\n        (2): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n      )\n      (attention_layer): AttentionBlock(\n        (group_norm): GroupNorm(32, 384, eps=1e-05, affine=True)\n        (qkv): Conv1d(384, 1152, kernel_size=(1,), stride=(1,))\n        (attention): QKVAttention()\n        (encoder_kv): Conv1d(512, 768, kernel_size=(1,), stride=(1,))\n        (proj_out): Conv1d(384, 384, kernel_size=(1,), stride=(1,))\n      )\n    )\n    (7): ResidualBlock(\n      (in_layers): Sequential(\n        (0): GroupNorm(32, 384, eps=1e-05, affine=True)\n        (1): SiLU()\n        (2): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n      )\n      (emb_layers): Sequential(\n        (0): SiLU()\n        (1): Linear(in_features=768, out_features=768, bias=True)\n      )\n      (group_norm): GroupNorm(32, 384, eps=1e-05, affine=True)\n      (out_layers): Sequential(\n        (0): SiLU()\n        (1): Dropout(p=0.0, inplace=False)\n        (2): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n      )\n      (attention_layer): AttentionBlock(\n        (group_norm): GroupNorm(32, 384, eps=1e-05, affine=True)\n        (qkv): Conv1d(384, 1152, kernel_size=(1,), stride=(1,))\n        (attention): QKVAttention()\n        (encoder_kv): Conv1d(512, 768, kernel_size=(1,), stride=(1,))\n        (proj_out): Conv1d(384, 384, kernel_size=(1,), stride=(1,))\n      )\n    )\n    (8): DownResidualBlock(\n      (in_layers): Sequential(\n        (0): GroupNorm(32, 384, eps=1e-05, affine=True)\n        (1): SiLU()\n        (2): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n      )\n      (emb_layers): Sequential(\n        (0): SiLU()\n        (1): Linear(in_features=768, out_features=768, bias=True)\n      )\n      (group_norm): GroupNorm(32, 384, eps=1e-05, affine=True)\n      (out_layers): Sequential(\n        (0): SiLU()\n        (1): Dropout(p=0.0, inplace=False)\n        (2): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n      )\n    )\n    (9): ResidualBlock(\n      (in_layers): Sequential(\n        (0): GroupNorm(32, 384, eps=1e-05, affine=True)\n        (1): SiLU()\n        (2): Conv2d(384, 576, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n      )\n      (emb_layers): Sequential(\n        (0): SiLU()\n        (1): Linear(in_features=768, out_features=1152, bias=True)\n      )\n      (group_norm): GroupNorm(32, 576, eps=1e-05, affine=True)\n      (out_layers): Sequential(\n        (0): SiLU()\n        (1): Dropout(p=0.0, inplace=False)\n        (2): Conv2d(576, 576, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n      )\n      (skip_connection): Conv2d(384, 576, kernel_size=(1, 1), stride=(1, 1))\n      (attention_layer): AttentionBlock(\n        (group_norm): GroupNorm(32, 576, eps=1e-05, affine=True)\n        (qkv): Conv1d(576, 1728, kernel_size=(1,), stride=(1,))\n        (attention): QKVAttention()\n        (encoder_kv): Conv1d(512, 1152, kernel_size=(1,), stride=(1,))\n        (proj_out): Conv1d(576, 576, kernel_size=(1,), stride=(1,))\n      )\n    )\n    (10): ResidualBlock(\n      (in_layers): Sequential(\n        (0): GroupNorm(32, 576, eps=1e-05, affine=True)\n        (1): SiLU()\n        (2): Conv2d(576, 576, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n      )\n      (emb_layers): Sequential(\n        (0): SiLU()\n        (1): Linear(in_features=768, out_features=1152, bias=True)\n      )\n      (group_norm): GroupNorm(32, 576, eps=1e-05, affine=True)\n      (out_layers): Sequential(\n        (0): SiLU()\n        (1): Dropout(p=0.0, inplace=False)\n        (2): Conv2d(576, 576, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n      )\n      (attention_layer): AttentionBlock(\n        (group_norm): GroupNorm(32, 576, eps=1e-05, affine=True)\n        (qkv): Conv1d(576, 1728, kernel_size=(1,), stride=(1,))\n        (attention): QKVAttention()\n        (encoder_kv): Conv1d(512, 1152, kernel_size=(1,), stride=(1,))\n        (proj_out): Conv1d(576, 576, kernel_size=(1,), stride=(1,))\n      )\n    )\n    (11): ResidualBlock(\n      (in_layers): Sequential(\n        (0): GroupNorm(32, 576, eps=1e-05, affine=True)\n        (1): SiLU()\n        (2): Conv2d(576, 576, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n      )\n      (emb_layers): Sequential(\n        (0): SiLU()\n        (1): Linear(in_features=768, out_features=1152, bias=True)\n      )\n      (group_norm): GroupNorm(32, 576, eps=1e-05, affine=True)\n      (out_layers): Sequential(\n        (0): SiLU()\n        (1): Dropout(p=0.0, inplace=False)\n        (2): Conv2d(576, 576, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n      )\n      (attention_layer): AttentionBlock(\n        (group_norm): GroupNorm(32, 576, eps=1e-05, affine=True)\n        (qkv): Conv1d(576, 1728, kernel_size=(1,), stride=(1,))\n        (attention): QKVAttention()\n        (encoder_kv): Conv1d(512, 1152, kernel_size=(1,), stride=(1,))\n        (proj_out): Conv1d(576, 576, kernel_size=(1,), stride=(1,))\n      )\n    )\n    (12): DownResidualBlock(\n      (in_layers): Sequential(\n        (0): GroupNorm(32, 576, eps=1e-05, affine=True)\n        (1): SiLU()\n        (2): Conv2d(576, 576, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n      )\n      (emb_layers): Sequential(\n        (0): SiLU()\n        (1): Linear(in_features=768, out_features=1152, bias=True)\n      )\n      (group_norm): GroupNorm(32, 576, eps=1e-05, affine=True)\n      (out_layers): Sequential(\n        (0): SiLU()\n        (1): Dropout(p=0.0, inplace=False)\n        (2): Conv2d(576, 576, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n      )\n    )\n    (13): ResidualBlock(\n      (in_layers): Sequential(\n        (0): GroupNorm(32, 576, eps=1e-05, affine=True)\n        (1): SiLU()\n        (2): Conv2d(576, 768, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n      )\n      (emb_layers): Sequential(\n        (0): SiLU()\n        (1): Linear(in_features=768, out_features=1536, bias=True)\n      )\n      (group_norm): GroupNorm(32, 768, eps=1e-05, affine=True)\n      (out_layers): Sequential(\n        (0): SiLU()\n        (1): Dropout(p=0.0, inplace=False)\n        (2): Conv2d(768, 768, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n      )\n      (skip_connection): Conv2d(576, 768, kernel_size=(1, 1), stride=(1, 1))\n      (attention_layer): AttentionBlock(\n        (group_norm): GroupNorm(32, 768, eps=1e-05, affine=True)\n        (qkv): Conv1d(768, 2304, kernel_size=(1,), stride=(1,))\n        (attention): QKVAttention()\n        (encoder_kv): Conv1d(512, 1536, kernel_size=(1,), stride=(1,))\n        (proj_out): Conv1d(768, 768, kernel_size=(1,), stride=(1,))\n      )\n    )\n    (14): ResidualBlock(\n      (in_layers): Sequential(\n        (0): GroupNorm(32, 768, eps=1e-05, affine=True)\n        (1): SiLU()\n        (2): Conv2d(768, 768, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n      )\n      (emb_layers): Sequential(\n        (0): SiLU()\n        (1): Linear(in_features=768, out_features=1536, bias=True)\n      )\n      (group_norm): GroupNorm(32, 768, eps=1e-05, affine=True)\n      (out_layers): Sequential(\n        (0): SiLU()\n        (1): Dropout(p=0.0, inplace=False)\n        (2): Conv2d(768, 768, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n      )\n      (attention_layer): AttentionBlock(\n        (group_norm): GroupNorm(32, 768, eps=1e-05, affine=True)\n        (qkv): Conv1d(768, 2304, kernel_size=(1,), stride=(1,))\n        (attention): QKVAttention()\n        (encoder_kv): Conv1d(512, 1536, kernel_size=(1,), stride=(1,))\n        (proj_out): Conv1d(768, 768, kernel_size=(1,), stride=(1,))\n      )\n    )\n    (15): ResidualBlock(\n      (in_layers): Sequential(\n        (0): GroupNorm(32, 768, eps=1e-05, affine=True)\n        (1): SiLU()\n        (2): Conv2d(768, 768, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n      )\n      (emb_layers): Sequential(\n        (0): SiLU()\n        (1): Linear(in_features=768, out_features=1536, bias=True)\n      )\n      (group_norm): GroupNorm(32, 768, eps=1e-05, affine=True)\n      (out_layers): Sequential(\n        (0): SiLU()\n        (1): Dropout(p=0.0, inplace=False)\n        (2): Conv2d(768, 768, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n      )\n      (attention_layer): AttentionBlock(\n        (group_norm): GroupNorm(32, 768, eps=1e-05, affine=True)\n        (qkv): Conv1d(768, 2304, kernel_size=(1,), stride=(1,))\n        (attention): QKVAttention()\n        (encoder_kv): Conv1d(512, 1536, kernel_size=(1,), stride=(1,))\n        (proj_out): Conv1d(768, 768, kernel_size=(1,), stride=(1,))\n      )\n    )\n  )\n  (middle_layers): Sequential(\n    (0): ResidualBlock(\n      (in_layers): Sequential(\n        (0): GroupNorm(32, 768, eps=1e-05, affine=True)\n        (1): SiLU()\n        (2): Conv2d(768, 768, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n      )\n      (emb_layers): Sequential(\n        (0): SiLU()\n        (1): Linear(in_features=768, out_features=1536, bias=True)\n      )\n      (group_norm): GroupNorm(32, 768, eps=1e-05, affine=True)\n      (out_layers): Sequential(\n        (0): SiLU()\n        (1): Dropout(p=0.0, inplace=False)\n        (2): Conv2d(768, 768, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n      )\n    )\n    (1): AttentionBlock(\n      (group_norm): GroupNorm(32, 768, eps=1e-05, affine=True)\n      (qkv): Conv1d(768, 2304, kernel_size=(1,), stride=(1,))\n      (attention): QKVAttention()\n      (encoder_kv): Conv1d(512, 1536, kernel_size=(1,), stride=(1,))\n      (proj_out): Conv1d(768, 768, kernel_size=(1,), stride=(1,))\n    )\n    (2): ResidualBlock(\n      (in_layers): Sequential(\n        (0): GroupNorm(32, 768, eps=1e-05, affine=True)\n        (1): SiLU()\n        (2): Conv2d(768, 768, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n      )\n      (emb_layers): Sequential(\n        (0): SiLU()\n        (1): Linear(in_features=768, out_features=1536, bias=True)\n      )\n      (group_norm): GroupNorm(32, 768, eps=1e-05, affine=True)\n      (out_layers): Sequential(\n        (0): SiLU()\n        (1): Dropout(p=0.0, inplace=False)\n        (2): Conv2d(768, 768, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n      )\n    )\n  )\n  (out_layers): Sequential(\n    (0): ResidualBlock(\n      (in_layers): Sequential(\n        (0): GroupNorm(32, 1536, eps=1e-05, affine=True)\n        (1): SiLU()\n        (2): Conv2d(1536, 768, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n      )\n      (emb_layers): Sequential(\n        (0): SiLU()\n        (1): Linear(in_features=768, out_features=1536, bias=True)\n      )\n      (group_norm): GroupNorm(32, 768, eps=1e-05, affine=True)\n      (out_layers): Sequential(\n        (0): SiLU()\n        (1): Dropout(p=0.0, inplace=False)\n        (2): Conv2d(768, 768, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n      )\n      (skip_connection): Conv2d(1536, 768, kernel_size=(1, 1), stride=(1, 1))\n      (attention_layer): AttentionBlock(\n        (group_norm): GroupNorm(32, 768, eps=1e-05, affine=True)\n        (qkv): Conv1d(768, 2304, kernel_size=(1,), stride=(1,))\n        (attention): QKVAttention()\n        (encoder_kv): Conv1d(512, 1536, kernel_size=(1,), stride=(1,))\n        (proj_out): Conv1d(768, 768, kernel_size=(1,), stride=(1,))\n      )\n    )\n    (1): ResidualBlock(\n      (in_layers): Sequential(\n        (0): GroupNorm(32, 1536, eps=1e-05, affine=True)\n        (1): SiLU()\n        (2): Conv2d(1536, 768, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n      )\n      (emb_layers): Sequential(\n        (0): SiLU()\n        (1): Linear(in_features=768, out_features=1536, bias=True)\n      )\n      (group_norm): GroupNorm(32, 768, eps=1e-05, affine=True)\n      (out_layers): Sequential(\n        (0): SiLU()\n        (1): Dropout(p=0.0, inplace=False)\n        (2): Conv2d(768, 768, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n      )\n      (skip_connection): Conv2d(1536, 768, kernel_size=(1, 1), stride=(1, 1))\n      (attention_layer): AttentionBlock(\n        (group_norm): GroupNorm(32, 768, eps=1e-05, affine=True)\n        (qkv): Conv1d(768, 2304, kernel_size=(1,), stride=(1,))\n        (attention): QKVAttention()\n        (encoder_kv): Conv1d(512, 1536, kernel_size=(1,), stride=(1,))\n        (proj_out): Conv1d(768, 768, kernel_size=(1,), stride=(1,))\n      )\n    )\n    (2): ResidualBlock(\n      (in_layers): Sequential(\n        (0): GroupNorm(32, 1536, eps=1e-05, affine=True)\n        (1): SiLU()\n        (2): Conv2d(1536, 768, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n      )\n      (emb_layers): Sequential(\n        (0): SiLU()\n        (1): Linear(in_features=768, out_features=1536, bias=True)\n      )\n      (group_norm): GroupNorm(32, 768, eps=1e-05, affine=True)\n      (out_layers): Sequential(\n        (0): SiLU()\n        (1): Dropout(p=0.0, inplace=False)\n        (2): Conv2d(768, 768, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n      )\n      (skip_connection): Conv2d(1536, 768, kernel_size=(1, 1), stride=(1, 1))\n      (attention_layer): AttentionBlock(\n        (group_norm): GroupNorm(32, 768, eps=1e-05, affine=True)\n        (qkv): Conv1d(768, 2304, kernel_size=(1,), stride=(1,))\n        (attention): QKVAttention()\n        (encoder_kv): Conv1d(512, 1536, kernel_size=(1,), stride=(1,))\n        (proj_out): Conv1d(768, 768, kernel_size=(1,), stride=(1,))\n      )\n    )\n    (3): Sequential(\n      (0): ResidualBlock(\n        (in_layers): Sequential(\n          (0): GroupNorm(32, 1344, eps=1e-05, affine=True)\n          (1): SiLU()\n          (2): Conv2d(1344, 768, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n        )\n        (emb_layers): Sequential(\n          (0): SiLU()\n          (1): Linear(in_features=768, out_features=1536, bias=True)\n        )\n        (group_norm): GroupNorm(32, 768, eps=1e-05, affine=True)\n        (out_layers): Sequential(\n          (0): SiLU()\n          (1): Dropout(p=0.0, inplace=False)\n          (2): Conv2d(768, 768, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n        )\n        (skip_connection): Conv2d(1344, 768, kernel_size=(1, 1), stride=(1, 1))\n        (attention_layer): AttentionBlock(\n          (group_norm): GroupNorm(32, 768, eps=1e-05, affine=True)\n          (qkv): Conv1d(768, 2304, kernel_size=(1,), stride=(1,))\n          (attention): QKVAttention()\n          (encoder_kv): Conv1d(512, 1536, kernel_size=(1,), stride=(1,))\n          (proj_out): Conv1d(768, 768, kernel_size=(1,), stride=(1,))\n        )\n      )\n      (1): UpResidualBlock(\n        (in_layers): Sequential(\n          (0): GroupNorm(32, 768, eps=1e-05, affine=True)\n          (1): SiLU()\n          (2): Conv2d(768, 768, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n        )\n        (emb_layers): Sequential(\n          (0): SiLU()\n          (1): Linear(in_features=768, out_features=1536, bias=True)\n        )\n        (group_norm): GroupNorm(32, 768, eps=1e-05, affine=True)\n        (out_layers): Sequential(\n          (0): SiLU()\n          (1): Dropout(p=0.0, inplace=False)\n          (2): Conv2d(768, 768, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n        )\n      )\n    )\n    (4): ResidualBlock(\n      (in_layers): Sequential(\n        (0): GroupNorm(32, 1344, eps=1e-05, affine=True)\n        (1): SiLU()\n        (2): Conv2d(1344, 576, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n      )\n      (emb_layers): Sequential(\n        (0): SiLU()\n        (1): Linear(in_features=768, out_features=1152, bias=True)\n      )\n      (group_norm): GroupNorm(32, 576, eps=1e-05, affine=True)\n      (out_layers): Sequential(\n        (0): SiLU()\n        (1): Dropout(p=0.0, inplace=False)\n        (2): Conv2d(576, 576, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n      )\n      (skip_connection): Conv2d(1344, 576, kernel_size=(1, 1), stride=(1, 1))\n      (attention_layer): AttentionBlock(\n        (group_norm): GroupNorm(32, 576, eps=1e-05, affine=True)\n        (qkv): Conv1d(576, 1728, kernel_size=(1,), stride=(1,))\n        (attention): QKVAttention()\n        (encoder_kv): Conv1d(512, 1152, kernel_size=(1,), stride=(1,))\n        (proj_out): Conv1d(576, 576, kernel_size=(1,), stride=(1,))\n      )\n    )\n    (5): ResidualBlock(\n      (in_layers): Sequential(\n        (0): GroupNorm(32, 1152, eps=1e-05, affine=True)\n        (1): SiLU()\n        (2): Conv2d(1152, 576, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n      )\n      (emb_layers): Sequential(\n        (0): SiLU()\n        (1): Linear(in_features=768, out_features=1152, bias=True)\n      )\n      (group_norm): GroupNorm(32, 576, eps=1e-05, affine=True)\n      (out_layers): Sequential(\n        (0): SiLU()\n        (1): Dropout(p=0.0, inplace=False)\n        (2): Conv2d(576, 576, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n      )\n      (skip_connection): Conv2d(1152, 576, kernel_size=(1, 1), stride=(1, 1))\n      (attention_layer): AttentionBlock(\n        (group_norm): GroupNorm(32, 576, eps=1e-05, affine=True)\n        (qkv): Conv1d(576, 1728, kernel_size=(1,), stride=(1,))\n        (attention): QKVAttention()\n        (encoder_kv): Conv1d(512, 1152, kernel_size=(1,), stride=(1,))\n        (proj_out): Conv1d(576, 576, kernel_size=(1,), stride=(1,))\n      )\n    )\n    (6): ResidualBlock(\n      (in_layers): Sequential(\n        (0): GroupNorm(32, 1152, eps=1e-05, affine=True)\n        (1): SiLU()\n        (2): Conv2d(1152, 576, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n      )\n      (emb_layers): Sequential(\n        (0): SiLU()\n        (1): Linear(in_features=768, out_features=1152, bias=True)\n      )\n      (group_norm): GroupNorm(32, 576, eps=1e-05, affine=True)\n      (out_layers): Sequential(\n        (0): SiLU()\n        (1): Dropout(p=0.0, inplace=False)\n        (2): Conv2d(576, 576, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n      )\n      (skip_connection): Conv2d(1152, 576, kernel_size=(1, 1), stride=(1, 1))\n      (attention_layer): AttentionBlock(\n        (group_norm): GroupNorm(32, 576, eps=1e-05, affine=True)\n        (qkv): Conv1d(576, 1728, kernel_size=(1,), stride=(1,))\n        (attention): QKVAttention()\n        (encoder_kv): Conv1d(512, 1152, kernel_size=(1,), stride=(1,))\n        (proj_out): Conv1d(576, 576, kernel_size=(1,), stride=(1,))\n      )\n    )\n    (7): Sequential(\n      (0): ResidualBlock(\n        (in_layers): Sequential(\n          (0): GroupNorm(32, 960, eps=1e-05, affine=True)\n          (1): SiLU()\n          (2): Conv2d(960, 576, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n        )\n        (emb_layers): Sequential(\n          (0): SiLU()\n          (1): Linear(in_features=768, out_features=1152, bias=True)\n        )\n        (group_norm): GroupNorm(32, 576, eps=1e-05, affine=True)\n        (out_layers): Sequential(\n          (0): SiLU()\n          (1): Dropout(p=0.0, inplace=False)\n          (2): Conv2d(576, 576, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n        )\n        (skip_connection): Conv2d(960, 576, kernel_size=(1, 1), stride=(1, 1))\n        (attention_layer): AttentionBlock(\n          (group_norm): GroupNorm(32, 576, eps=1e-05, affine=True)\n          (qkv): Conv1d(576, 1728, kernel_size=(1,), stride=(1,))\n          (attention): QKVAttention()\n          (encoder_kv): Conv1d(512, 1152, kernel_size=(1,), stride=(1,))\n          (proj_out): Conv1d(576, 576, kernel_size=(1,), stride=(1,))\n        )\n      )\n      (1): UpResidualBlock(\n        (in_layers): Sequential(\n          (0): GroupNorm(32, 576, eps=1e-05, affine=True)\n          (1): SiLU()\n          (2): Conv2d(576, 576, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n        )\n        (emb_layers): Sequential(\n          (0): SiLU()\n          (1): Linear(in_features=768, out_features=1152, bias=True)\n        )\n        (group_norm): GroupNorm(32, 576, eps=1e-05, affine=True)\n        (out_layers): Sequential(\n          (0): SiLU()\n          (1): Dropout(p=0.0, inplace=False)\n          (2): Conv2d(576, 576, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n        )\n      )\n    )\n    (8): ResidualBlock(\n      (in_layers): Sequential(\n        (0): GroupNorm(32, 960, eps=1e-05, affine=True)\n        (1): SiLU()\n        (2): Conv2d(960, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n      )\n      (emb_layers): Sequential(\n        (0): SiLU()\n        (1): Linear(in_features=768, out_features=768, bias=True)\n      )\n      (group_norm): GroupNorm(32, 384, eps=1e-05, affine=True)\n      (out_layers): Sequential(\n        (0): SiLU()\n        (1): Dropout(p=0.0, inplace=False)\n        (2): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n      )\n      (skip_connection): Conv2d(960, 384, kernel_size=(1, 1), stride=(1, 1))\n      (attention_layer): AttentionBlock(\n        (group_norm): GroupNorm(32, 384, eps=1e-05, affine=True)\n        (qkv): Conv1d(384, 1152, kernel_size=(1,), stride=(1,))\n        (attention): QKVAttention()\n        (encoder_kv): Conv1d(512, 768, kernel_size=(1,), stride=(1,))\n        (proj_out): Conv1d(384, 384, kernel_size=(1,), stride=(1,))\n      )\n    )\n    (9): ResidualBlock(\n      (in_layers): Sequential(\n        (0): GroupNorm(32, 768, eps=1e-05, affine=True)\n        (1): SiLU()\n        (2): Conv2d(768, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n      )\n      (emb_layers): Sequential(\n        (0): SiLU()\n        (1): Linear(in_features=768, out_features=768, bias=True)\n      )\n      (group_norm): GroupNorm(32, 384, eps=1e-05, affine=True)\n      (out_layers): Sequential(\n        (0): SiLU()\n        (1): Dropout(p=0.0, inplace=False)\n        (2): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n      )\n      (skip_connection): Conv2d(768, 384, kernel_size=(1, 1), stride=(1, 1))\n      (attention_layer): AttentionBlock(\n        (group_norm): GroupNorm(32, 384, eps=1e-05, affine=True)\n        (qkv): Conv1d(384, 1152, kernel_size=(1,), stride=(1,))\n        (attention): QKVAttention()\n        (encoder_kv): Conv1d(512, 768, kernel_size=(1,), stride=(1,))\n        (proj_out): Conv1d(384, 384, kernel_size=(1,), stride=(1,))\n      )\n    )\n    (10): ResidualBlock(\n      (in_layers): Sequential(\n        (0): GroupNorm(32, 768, eps=1e-05, affine=True)\n        (1): SiLU()\n        (2): Conv2d(768, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n      )\n      (emb_layers): Sequential(\n        (0): SiLU()\n        (1): Linear(in_features=768, out_features=768, bias=True)\n      )\n      (group_norm): GroupNorm(32, 384, eps=1e-05, affine=True)\n      (out_layers): Sequential(\n        (0): SiLU()\n        (1): Dropout(p=0.0, inplace=False)\n        (2): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n      )\n      (skip_connection): Conv2d(768, 384, kernel_size=(1, 1), stride=(1, 1))\n      (attention_layer): AttentionBlock(\n        (group_norm): GroupNorm(32, 384, eps=1e-05, affine=True)\n        (qkv): Conv1d(384, 1152, kernel_size=(1,), stride=(1,))\n        (attention): QKVAttention()\n        (encoder_kv): Conv1d(512, 768, kernel_size=(1,), stride=(1,))\n        (proj_out): Conv1d(384, 384, kernel_size=(1,), stride=(1,))\n      )\n    )\n    (11): Sequential(\n      (0): ResidualBlock(\n        (in_layers): Sequential(\n          (0): GroupNorm(32, 576, eps=1e-05, affine=True)\n          (1): SiLU()\n          (2): Conv2d(576, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n        )\n        (emb_layers): Sequential(\n          (0): SiLU()\n          (1): Linear(in_features=768, out_features=768, bias=True)\n        )\n        (group_norm): GroupNorm(32, 384, eps=1e-05, affine=True)\n        (out_layers): Sequential(\n          (0): SiLU()\n          (1): Dropout(p=0.0, inplace=False)\n          (2): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n        )\n        (skip_connection): Conv2d(576, 384, kernel_size=(1, 1), stride=(1, 1))\n        (attention_layer): AttentionBlock(\n          (group_norm): GroupNorm(32, 384, eps=1e-05, affine=True)\n          (qkv): Conv1d(384, 1152, kernel_size=(1,), stride=(1,))\n          (attention): QKVAttention()\n          (encoder_kv): Conv1d(512, 768, kernel_size=(1,), stride=(1,))\n          (proj_out): Conv1d(384, 384, kernel_size=(1,), stride=(1,))\n        )\n      )\n      (1): UpResidualBlock(\n        (in_layers): Sequential(\n          (0): GroupNorm(32, 384, eps=1e-05, affine=True)\n          (1): SiLU()\n          (2): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n        )\n        (emb_layers): Sequential(\n          (0): SiLU()\n          (1): Linear(in_features=768, out_features=768, bias=True)\n        )\n        (group_norm): GroupNorm(32, 384, eps=1e-05, affine=True)\n        (out_layers): Sequential(\n          (0): SiLU()\n          (1): Dropout(p=0.0, inplace=False)\n          (2): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n        )\n      )\n    )\n    (12): ResidualBlock(\n      (in_layers): Sequential(\n        (0): GroupNorm(32, 576, eps=1e-05, affine=True)\n        (1): SiLU()\n        (2): Conv2d(576, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n      )\n      (emb_layers): Sequential(\n        (0): SiLU()\n        (1): Linear(in_features=768, out_features=384, bias=True)\n      )\n      (group_norm): GroupNorm(32, 192, eps=1e-05, affine=True)\n      (out_layers): Sequential(\n        (0): SiLU()\n        (1): Dropout(p=0.0, inplace=False)\n        (2): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n      )\n      (skip_connection): Conv2d(576, 192, kernel_size=(1, 1), stride=(1, 1))\n    )\n    (13): ResidualBlock(\n      (in_layers): Sequential(\n        (0): GroupNorm(32, 384, eps=1e-05, affine=True)\n        (1): SiLU()\n        (2): Conv2d(384, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n      )\n      (emb_layers): Sequential(\n        (0): SiLU()\n        (1): Linear(in_features=768, out_features=384, bias=True)\n      )\n      (group_norm): GroupNorm(32, 192, eps=1e-05, affine=True)\n      (out_layers): Sequential(\n        (0): SiLU()\n        (1): Dropout(p=0.0, inplace=False)\n        (2): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n      )\n      (skip_connection): Conv2d(384, 192, kernel_size=(1, 1), stride=(1, 1))\n    )\n    (14): ResidualBlock(\n      (in_layers): Sequential(\n        (0): GroupNorm(32, 384, eps=1e-05, affine=True)\n        (1): SiLU()\n        (2): Conv2d(384, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n      )\n      (emb_layers): Sequential(\n        (0): SiLU()\n        (1): Linear(in_features=768, out_features=384, bias=True)\n      )\n      (group_norm): GroupNorm(32, 192, eps=1e-05, affine=True)\n      (out_layers): Sequential(\n        (0): SiLU()\n        (1): Dropout(p=0.0, inplace=False)\n        (2): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n      )\n      (skip_connection): Conv2d(384, 192, kernel_size=(1, 1), stride=(1, 1))\n    )\n    (15): ResidualBlock(\n      (in_layers): Sequential(\n        (0): GroupNorm(32, 384, eps=1e-05, affine=True)\n        (1): SiLU()\n        (2): Conv2d(384, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n      )\n      (emb_layers): Sequential(\n        (0): SiLU()\n        (1): Linear(in_features=768, out_features=384, bias=True)\n      )\n      (group_norm): GroupNorm(32, 192, eps=1e-05, affine=True)\n      (out_layers): Sequential(\n        (0): SiLU()\n        (1): Dropout(p=0.0, inplace=False)\n        (2): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n      )\n      (skip_connection): Conv2d(384, 192, kernel_size=(1, 1), stride=(1, 1))\n    )\n  )\n  (out): Sequential(\n    (0): GroupNorm(32, 192, eps=1e-05, affine=True)\n    (1): Conv2d(192, 6, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n  )\n  (transformer): Transformer(\n    (resblocks): ModuleList(\n      (0): ResidualAttentionBlock(\n        (attn): MultiheadAttention(\n          (c_qkv): Linear(in_features=512, out_features=1536, bias=True)\n          (c_proj): Linear(in_features=512, out_features=512, bias=True)\n          (attention): QKVMultiheadAttention()\n        )\n        (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n        (mlp): MLP(\n          (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n          (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n          (gelu): GELU()\n        )\n        (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n      )\n      (1): ResidualAttentionBlock(\n        (attn): MultiheadAttention(\n          (c_qkv): Linear(in_features=512, out_features=1536, bias=True)\n          (c_proj): Linear(in_features=512, out_features=512, bias=True)\n          (attention): QKVMultiheadAttention()\n        )\n        (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n        (mlp): MLP(\n          (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n          (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n          (gelu): GELU()\n        )\n        (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n      )\n      (2): ResidualAttentionBlock(\n        (attn): MultiheadAttention(\n          (c_qkv): Linear(in_features=512, out_features=1536, bias=True)\n          (c_proj): Linear(in_features=512, out_features=512, bias=True)\n          (attention): QKVMultiheadAttention()\n        )\n        (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n        (mlp): MLP(\n          (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n          (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n          (gelu): GELU()\n        )\n        (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n      )\n      (3): ResidualAttentionBlock(\n        (attn): MultiheadAttention(\n          (c_qkv): Linear(in_features=512, out_features=1536, bias=True)\n          (c_proj): Linear(in_features=512, out_features=512, bias=True)\n          (attention): QKVMultiheadAttention()\n        )\n        (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n        (mlp): MLP(\n          (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n          (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n          (gelu): GELU()\n        )\n        (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n      )\n      (4): ResidualAttentionBlock(\n        (attn): MultiheadAttention(\n          (c_qkv): Linear(in_features=512, out_features=1536, bias=True)\n          (c_proj): Linear(in_features=512, out_features=512, bias=True)\n          (attention): QKVMultiheadAttention()\n        )\n        (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n        (mlp): MLP(\n          (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n          (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n          (gelu): GELU()\n        )\n        (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n      )\n      (5): ResidualAttentionBlock(\n        (attn): MultiheadAttention(\n          (c_qkv): Linear(in_features=512, out_features=1536, bias=True)\n          (c_proj): Linear(in_features=512, out_features=512, bias=True)\n          (attention): QKVMultiheadAttention()\n        )\n        (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n        (mlp): MLP(\n          (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n          (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n          (gelu): GELU()\n        )\n        (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n      )\n      (6): ResidualAttentionBlock(\n        (attn): MultiheadAttention(\n          (c_qkv): Linear(in_features=512, out_features=1536, bias=True)\n          (c_proj): Linear(in_features=512, out_features=512, bias=True)\n          (attention): QKVMultiheadAttention()\n        )\n        (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n        (mlp): MLP(\n          (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n          (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n          (gelu): GELU()\n        )\n        (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n      )\n      (7): ResidualAttentionBlock(\n        (attn): MultiheadAttention(\n          (c_qkv): Linear(in_features=512, out_features=1536, bias=True)\n          (c_proj): Linear(in_features=512, out_features=512, bias=True)\n          (attention): QKVMultiheadAttention()\n        )\n        (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n        (mlp): MLP(\n          (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n          (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n          (gelu): GELU()\n        )\n        (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n      )\n      (8): ResidualAttentionBlock(\n        (attn): MultiheadAttention(\n          (c_qkv): Linear(in_features=512, out_features=1536, bias=True)\n          (c_proj): Linear(in_features=512, out_features=512, bias=True)\n          (attention): QKVMultiheadAttention()\n        )\n        (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n        (mlp): MLP(\n          (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n          (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n          (gelu): GELU()\n        )\n        (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n      )\n      (9): ResidualAttentionBlock(\n        (attn): MultiheadAttention(\n          (c_qkv): Linear(in_features=512, out_features=1536, bias=True)\n          (c_proj): Linear(in_features=512, out_features=512, bias=True)\n          (attention): QKVMultiheadAttention()\n        )\n        (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n        (mlp): MLP(\n          (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n          (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n          (gelu): GELU()\n        )\n        (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n      )\n      (10): ResidualAttentionBlock(\n        (attn): MultiheadAttention(\n          (c_qkv): Linear(in_features=512, out_features=1536, bias=True)\n          (c_proj): Linear(in_features=512, out_features=512, bias=True)\n          (attention): QKVMultiheadAttention()\n        )\n        (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n        (mlp): MLP(\n          (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n          (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n          (gelu): GELU()\n        )\n        (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n      )\n      (11): ResidualAttentionBlock(\n        (attn): MultiheadAttention(\n          (c_qkv): Linear(in_features=512, out_features=1536, bias=True)\n          (c_proj): Linear(in_features=512, out_features=512, bias=True)\n          (attention): QKVMultiheadAttention()\n        )\n        (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n        (mlp): MLP(\n          (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n          (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n          (gelu): GELU()\n        )\n        (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n      )\n      (12): ResidualAttentionBlock(\n        (attn): MultiheadAttention(\n          (c_qkv): Linear(in_features=512, out_features=1536, bias=True)\n          (c_proj): Linear(in_features=512, out_features=512, bias=True)\n          (attention): QKVMultiheadAttention()\n        )\n        (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n        (mlp): MLP(\n          (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n          (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n          (gelu): GELU()\n        )\n        (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n      )\n      (13): ResidualAttentionBlock(\n        (attn): MultiheadAttention(\n          (c_qkv): Linear(in_features=512, out_features=1536, bias=True)\n          (c_proj): Linear(in_features=512, out_features=512, bias=True)\n          (attention): QKVMultiheadAttention()\n        )\n        (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n        (mlp): MLP(\n          (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n          (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n          (gelu): GELU()\n        )\n        (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n      )\n      (14): ResidualAttentionBlock(\n        (attn): MultiheadAttention(\n          (c_qkv): Linear(in_features=512, out_features=1536, bias=True)\n          (c_proj): Linear(in_features=512, out_features=512, bias=True)\n          (attention): QKVMultiheadAttention()\n        )\n        (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n        (mlp): MLP(\n          (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n          (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n          (gelu): GELU()\n        )\n        (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n      )\n      (15): ResidualAttentionBlock(\n        (attn): MultiheadAttention(\n          (c_qkv): Linear(in_features=512, out_features=1536, bias=True)\n          (c_proj): Linear(in_features=512, out_features=512, bias=True)\n          (attention): QKVMultiheadAttention()\n        )\n        (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n        (mlp): MLP(\n          (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n          (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n          (gelu): GELU()\n        )\n        (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n      )\n    )\n  )\n  (final_ln): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n  (token_embedding): Embedding(50257, 512)\n  (transformer_proj): Linear(in_features=512, out_features=768, bias=True)\n)"
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = get_encoder()\n",
    "model = Text2Im(tokenizer)\n",
    "model"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "783 783\n"
     ]
    },
    {
     "data": {
      "text/plain": "[('positional_embedding', 'positional_embedding'),\n ('padding_embedding', 'padding_embedding'),\n ('time_embed.0.weight', 'time_embed.0.weight'),\n ('time_embed.0.bias', 'time_embed.0.bias'),\n ('time_embed.2.weight', 'time_embed.2.weight'),\n ('time_embed.2.bias', 'time_embed.2.bias'),\n ('in_layers.0.weight', 'input_blocks.0.0.weight'),\n ('in_layers.0.bias', 'input_blocks.0.0.bias'),\n ('in_layers.1.in_layers.0.weight', 'input_blocks.1.0.in_layers.0.weight'),\n ('in_layers.1.in_layers.0.bias', 'input_blocks.1.0.in_layers.0.bias'),\n ('in_layers.1.in_layers.2.weight', 'input_blocks.1.0.in_layers.2.weight'),\n ('in_layers.1.in_layers.2.bias', 'input_blocks.1.0.in_layers.2.bias'),\n ('in_layers.1.emb_layers.1.weight', 'input_blocks.1.0.emb_layers.1.weight'),\n ('in_layers.1.emb_layers.1.bias', 'input_blocks.1.0.emb_layers.1.bias'),\n ('in_layers.1.group_norm.weight', 'input_blocks.1.0.out_layers.0.weight'),\n ('in_layers.1.group_norm.bias', 'input_blocks.1.0.out_layers.0.bias'),\n ('in_layers.1.out_layers.2.weight', 'input_blocks.1.0.out_layers.3.weight'),\n ('in_layers.1.out_layers.2.bias', 'input_blocks.1.0.out_layers.3.bias'),\n ('in_layers.2.in_layers.0.weight', 'input_blocks.2.0.in_layers.0.weight'),\n ('in_layers.2.in_layers.0.bias', 'input_blocks.2.0.in_layers.0.bias'),\n ('in_layers.2.in_layers.2.weight', 'input_blocks.2.0.in_layers.2.weight'),\n ('in_layers.2.in_layers.2.bias', 'input_blocks.2.0.in_layers.2.bias'),\n ('in_layers.2.emb_layers.1.weight', 'input_blocks.2.0.emb_layers.1.weight'),\n ('in_layers.2.emb_layers.1.bias', 'input_blocks.2.0.emb_layers.1.bias'),\n ('in_layers.2.group_norm.weight', 'input_blocks.2.0.out_layers.0.weight'),\n ('in_layers.2.group_norm.bias', 'input_blocks.2.0.out_layers.0.bias'),\n ('in_layers.2.out_layers.2.weight', 'input_blocks.2.0.out_layers.3.weight'),\n ('in_layers.2.out_layers.2.bias', 'input_blocks.2.0.out_layers.3.bias'),\n ('in_layers.3.in_layers.0.weight', 'input_blocks.3.0.in_layers.0.weight'),\n ('in_layers.3.in_layers.0.bias', 'input_blocks.3.0.in_layers.0.bias'),\n ('in_layers.3.in_layers.2.weight', 'input_blocks.3.0.in_layers.2.weight'),\n ('in_layers.3.in_layers.2.bias', 'input_blocks.3.0.in_layers.2.bias'),\n ('in_layers.3.emb_layers.1.weight', 'input_blocks.3.0.emb_layers.1.weight'),\n ('in_layers.3.emb_layers.1.bias', 'input_blocks.3.0.emb_layers.1.bias'),\n ('in_layers.3.group_norm.weight', 'input_blocks.3.0.out_layers.0.weight'),\n ('in_layers.3.group_norm.bias', 'input_blocks.3.0.out_layers.0.bias'),\n ('in_layers.3.out_layers.2.weight', 'input_blocks.3.0.out_layers.3.weight'),\n ('in_layers.3.out_layers.2.bias', 'input_blocks.3.0.out_layers.3.bias'),\n ('in_layers.4.in_layers.0.weight', 'input_blocks.4.0.in_layers.0.weight'),\n ('in_layers.4.in_layers.0.bias', 'input_blocks.4.0.in_layers.0.bias'),\n ('in_layers.4.in_layers.2.weight', 'input_blocks.4.0.in_layers.2.weight'),\n ('in_layers.4.in_layers.2.bias', 'input_blocks.4.0.in_layers.2.bias'),\n ('in_layers.4.emb_layers.1.weight', 'input_blocks.4.0.emb_layers.1.weight'),\n ('in_layers.4.emb_layers.1.bias', 'input_blocks.4.0.emb_layers.1.bias'),\n ('in_layers.4.group_norm.weight', 'input_blocks.4.0.out_layers.0.weight'),\n ('in_layers.4.group_norm.bias', 'input_blocks.4.0.out_layers.0.bias'),\n ('in_layers.4.out_layers.2.weight', 'input_blocks.4.0.out_layers.3.weight'),\n ('in_layers.4.out_layers.2.bias', 'input_blocks.4.0.out_layers.3.bias'),\n ('in_layers.5.in_layers.0.weight', 'input_blocks.5.0.in_layers.0.weight'),\n ('in_layers.5.in_layers.0.bias', 'input_blocks.5.0.in_layers.0.bias'),\n ('in_layers.5.in_layers.2.weight', 'input_blocks.5.0.in_layers.2.weight'),\n ('in_layers.5.in_layers.2.bias', 'input_blocks.5.0.in_layers.2.bias'),\n ('in_layers.5.emb_layers.1.weight', 'input_blocks.5.0.emb_layers.1.weight'),\n ('in_layers.5.emb_layers.1.bias', 'input_blocks.5.0.emb_layers.1.bias'),\n ('in_layers.5.group_norm.weight', 'input_blocks.5.0.out_layers.0.weight'),\n ('in_layers.5.group_norm.bias', 'input_blocks.5.0.out_layers.0.bias'),\n ('in_layers.5.out_layers.2.weight', 'input_blocks.5.0.out_layers.3.weight'),\n ('in_layers.5.out_layers.2.bias', 'input_blocks.5.0.out_layers.3.bias'),\n ('in_layers.5.skip_connection.weight',\n  'input_blocks.5.0.skip_connection.weight'),\n ('in_layers.5.skip_connection.bias', 'input_blocks.5.0.skip_connection.bias'),\n ('in_layers.5.attention_layer.group_norm.weight',\n  'input_blocks.5.1.norm.weight'),\n ('in_layers.5.attention_layer.group_norm.bias', 'input_blocks.5.1.norm.bias'),\n ('in_layers.5.attention_layer.qkv.weight', 'input_blocks.5.1.qkv.weight'),\n ('in_layers.5.attention_layer.qkv.bias', 'input_blocks.5.1.qkv.bias'),\n ('in_layers.5.attention_layer.encoder_kv.weight',\n  'input_blocks.5.1.encoder_kv.weight'),\n ('in_layers.5.attention_layer.encoder_kv.bias',\n  'input_blocks.5.1.encoder_kv.bias'),\n ('in_layers.5.attention_layer.proj_out.weight',\n  'input_blocks.5.1.proj_out.weight'),\n ('in_layers.5.attention_layer.proj_out.bias',\n  'input_blocks.5.1.proj_out.bias'),\n ('in_layers.6.in_layers.0.weight', 'input_blocks.6.0.in_layers.0.weight'),\n ('in_layers.6.in_layers.0.bias', 'input_blocks.6.0.in_layers.0.bias'),\n ('in_layers.6.in_layers.2.weight', 'input_blocks.6.0.in_layers.2.weight'),\n ('in_layers.6.in_layers.2.bias', 'input_blocks.6.0.in_layers.2.bias'),\n ('in_layers.6.emb_layers.1.weight', 'input_blocks.6.0.emb_layers.1.weight'),\n ('in_layers.6.emb_layers.1.bias', 'input_blocks.6.0.emb_layers.1.bias'),\n ('in_layers.6.group_norm.weight', 'input_blocks.6.0.out_layers.0.weight'),\n ('in_layers.6.group_norm.bias', 'input_blocks.6.0.out_layers.0.bias'),\n ('in_layers.6.out_layers.2.weight', 'input_blocks.6.0.out_layers.3.weight'),\n ('in_layers.6.out_layers.2.bias', 'input_blocks.6.0.out_layers.3.bias'),\n ('in_layers.6.attention_layer.group_norm.weight',\n  'input_blocks.6.1.norm.weight'),\n ('in_layers.6.attention_layer.group_norm.bias', 'input_blocks.6.1.norm.bias'),\n ('in_layers.6.attention_layer.qkv.weight', 'input_blocks.6.1.qkv.weight'),\n ('in_layers.6.attention_layer.qkv.bias', 'input_blocks.6.1.qkv.bias'),\n ('in_layers.6.attention_layer.encoder_kv.weight',\n  'input_blocks.6.1.encoder_kv.weight'),\n ('in_layers.6.attention_layer.encoder_kv.bias',\n  'input_blocks.6.1.encoder_kv.bias'),\n ('in_layers.6.attention_layer.proj_out.weight',\n  'input_blocks.6.1.proj_out.weight'),\n ('in_layers.6.attention_layer.proj_out.bias',\n  'input_blocks.6.1.proj_out.bias'),\n ('in_layers.7.in_layers.0.weight', 'input_blocks.7.0.in_layers.0.weight'),\n ('in_layers.7.in_layers.0.bias', 'input_blocks.7.0.in_layers.0.bias'),\n ('in_layers.7.in_layers.2.weight', 'input_blocks.7.0.in_layers.2.weight'),\n ('in_layers.7.in_layers.2.bias', 'input_blocks.7.0.in_layers.2.bias'),\n ('in_layers.7.emb_layers.1.weight', 'input_blocks.7.0.emb_layers.1.weight'),\n ('in_layers.7.emb_layers.1.bias', 'input_blocks.7.0.emb_layers.1.bias'),\n ('in_layers.7.group_norm.weight', 'input_blocks.7.0.out_layers.0.weight'),\n ('in_layers.7.group_norm.bias', 'input_blocks.7.0.out_layers.0.bias'),\n ('in_layers.7.out_layers.2.weight', 'input_blocks.7.0.out_layers.3.weight'),\n ('in_layers.7.out_layers.2.bias', 'input_blocks.7.0.out_layers.3.bias'),\n ('in_layers.7.attention_layer.group_norm.weight',\n  'input_blocks.7.1.norm.weight'),\n ('in_layers.7.attention_layer.group_norm.bias', 'input_blocks.7.1.norm.bias'),\n ('in_layers.7.attention_layer.qkv.weight', 'input_blocks.7.1.qkv.weight'),\n ('in_layers.7.attention_layer.qkv.bias', 'input_blocks.7.1.qkv.bias'),\n ('in_layers.7.attention_layer.encoder_kv.weight',\n  'input_blocks.7.1.encoder_kv.weight'),\n ('in_layers.7.attention_layer.encoder_kv.bias',\n  'input_blocks.7.1.encoder_kv.bias'),\n ('in_layers.7.attention_layer.proj_out.weight',\n  'input_blocks.7.1.proj_out.weight'),\n ('in_layers.7.attention_layer.proj_out.bias',\n  'input_blocks.7.1.proj_out.bias'),\n ('in_layers.8.in_layers.0.weight', 'input_blocks.8.0.in_layers.0.weight'),\n ('in_layers.8.in_layers.0.bias', 'input_blocks.8.0.in_layers.0.bias'),\n ('in_layers.8.in_layers.2.weight', 'input_blocks.8.0.in_layers.2.weight'),\n ('in_layers.8.in_layers.2.bias', 'input_blocks.8.0.in_layers.2.bias'),\n ('in_layers.8.emb_layers.1.weight', 'input_blocks.8.0.emb_layers.1.weight'),\n ('in_layers.8.emb_layers.1.bias', 'input_blocks.8.0.emb_layers.1.bias'),\n ('in_layers.8.group_norm.weight', 'input_blocks.8.0.out_layers.0.weight'),\n ('in_layers.8.group_norm.bias', 'input_blocks.8.0.out_layers.0.bias'),\n ('in_layers.8.out_layers.2.weight', 'input_blocks.8.0.out_layers.3.weight'),\n ('in_layers.8.out_layers.2.bias', 'input_blocks.8.0.out_layers.3.bias'),\n ('in_layers.9.in_layers.0.weight', 'input_blocks.9.0.in_layers.0.weight'),\n ('in_layers.9.in_layers.0.bias', 'input_blocks.9.0.in_layers.0.bias'),\n ('in_layers.9.in_layers.2.weight', 'input_blocks.9.0.in_layers.2.weight'),\n ('in_layers.9.in_layers.2.bias', 'input_blocks.9.0.in_layers.2.bias'),\n ('in_layers.9.emb_layers.1.weight', 'input_blocks.9.0.emb_layers.1.weight'),\n ('in_layers.9.emb_layers.1.bias', 'input_blocks.9.0.emb_layers.1.bias'),\n ('in_layers.9.group_norm.weight', 'input_blocks.9.0.out_layers.0.weight'),\n ('in_layers.9.group_norm.bias', 'input_blocks.9.0.out_layers.0.bias'),\n ('in_layers.9.out_layers.2.weight', 'input_blocks.9.0.out_layers.3.weight'),\n ('in_layers.9.out_layers.2.bias', 'input_blocks.9.0.out_layers.3.bias'),\n ('in_layers.9.skip_connection.weight',\n  'input_blocks.9.0.skip_connection.weight'),\n ('in_layers.9.skip_connection.bias', 'input_blocks.9.0.skip_connection.bias'),\n ('in_layers.9.attention_layer.group_norm.weight',\n  'input_blocks.9.1.norm.weight'),\n ('in_layers.9.attention_layer.group_norm.bias', 'input_blocks.9.1.norm.bias'),\n ('in_layers.9.attention_layer.qkv.weight', 'input_blocks.9.1.qkv.weight'),\n ('in_layers.9.attention_layer.qkv.bias', 'input_blocks.9.1.qkv.bias'),\n ('in_layers.9.attention_layer.encoder_kv.weight',\n  'input_blocks.9.1.encoder_kv.weight'),\n ('in_layers.9.attention_layer.encoder_kv.bias',\n  'input_blocks.9.1.encoder_kv.bias'),\n ('in_layers.9.attention_layer.proj_out.weight',\n  'input_blocks.9.1.proj_out.weight'),\n ('in_layers.9.attention_layer.proj_out.bias',\n  'input_blocks.9.1.proj_out.bias'),\n ('in_layers.10.in_layers.0.weight', 'input_blocks.10.0.in_layers.0.weight'),\n ('in_layers.10.in_layers.0.bias', 'input_blocks.10.0.in_layers.0.bias'),\n ('in_layers.10.in_layers.2.weight', 'input_blocks.10.0.in_layers.2.weight'),\n ('in_layers.10.in_layers.2.bias', 'input_blocks.10.0.in_layers.2.bias'),\n ('in_layers.10.emb_layers.1.weight', 'input_blocks.10.0.emb_layers.1.weight'),\n ('in_layers.10.emb_layers.1.bias', 'input_blocks.10.0.emb_layers.1.bias'),\n ('in_layers.10.group_norm.weight', 'input_blocks.10.0.out_layers.0.weight'),\n ('in_layers.10.group_norm.bias', 'input_blocks.10.0.out_layers.0.bias'),\n ('in_layers.10.out_layers.2.weight', 'input_blocks.10.0.out_layers.3.weight'),\n ('in_layers.10.out_layers.2.bias', 'input_blocks.10.0.out_layers.3.bias'),\n ('in_layers.10.attention_layer.group_norm.weight',\n  'input_blocks.10.1.norm.weight'),\n ('in_layers.10.attention_layer.group_norm.bias',\n  'input_blocks.10.1.norm.bias'),\n ('in_layers.10.attention_layer.qkv.weight', 'input_blocks.10.1.qkv.weight'),\n ('in_layers.10.attention_layer.qkv.bias', 'input_blocks.10.1.qkv.bias'),\n ('in_layers.10.attention_layer.encoder_kv.weight',\n  'input_blocks.10.1.encoder_kv.weight'),\n ('in_layers.10.attention_layer.encoder_kv.bias',\n  'input_blocks.10.1.encoder_kv.bias'),\n ('in_layers.10.attention_layer.proj_out.weight',\n  'input_blocks.10.1.proj_out.weight'),\n ('in_layers.10.attention_layer.proj_out.bias',\n  'input_blocks.10.1.proj_out.bias'),\n ('in_layers.11.in_layers.0.weight', 'input_blocks.11.0.in_layers.0.weight'),\n ('in_layers.11.in_layers.0.bias', 'input_blocks.11.0.in_layers.0.bias'),\n ('in_layers.11.in_layers.2.weight', 'input_blocks.11.0.in_layers.2.weight'),\n ('in_layers.11.in_layers.2.bias', 'input_blocks.11.0.in_layers.2.bias'),\n ('in_layers.11.emb_layers.1.weight', 'input_blocks.11.0.emb_layers.1.weight'),\n ('in_layers.11.emb_layers.1.bias', 'input_blocks.11.0.emb_layers.1.bias'),\n ('in_layers.11.group_norm.weight', 'input_blocks.11.0.out_layers.0.weight'),\n ('in_layers.11.group_norm.bias', 'input_blocks.11.0.out_layers.0.bias'),\n ('in_layers.11.out_layers.2.weight', 'input_blocks.11.0.out_layers.3.weight'),\n ('in_layers.11.out_layers.2.bias', 'input_blocks.11.0.out_layers.3.bias'),\n ('in_layers.11.attention_layer.group_norm.weight',\n  'input_blocks.11.1.norm.weight'),\n ('in_layers.11.attention_layer.group_norm.bias',\n  'input_blocks.11.1.norm.bias'),\n ('in_layers.11.attention_layer.qkv.weight', 'input_blocks.11.1.qkv.weight'),\n ('in_layers.11.attention_layer.qkv.bias', 'input_blocks.11.1.qkv.bias'),\n ('in_layers.11.attention_layer.encoder_kv.weight',\n  'input_blocks.11.1.encoder_kv.weight'),\n ('in_layers.11.attention_layer.encoder_kv.bias',\n  'input_blocks.11.1.encoder_kv.bias'),\n ('in_layers.11.attention_layer.proj_out.weight',\n  'input_blocks.11.1.proj_out.weight'),\n ('in_layers.11.attention_layer.proj_out.bias',\n  'input_blocks.11.1.proj_out.bias'),\n ('in_layers.12.in_layers.0.weight', 'input_blocks.12.0.in_layers.0.weight'),\n ('in_layers.12.in_layers.0.bias', 'input_blocks.12.0.in_layers.0.bias'),\n ('in_layers.12.in_layers.2.weight', 'input_blocks.12.0.in_layers.2.weight'),\n ('in_layers.12.in_layers.2.bias', 'input_blocks.12.0.in_layers.2.bias'),\n ('in_layers.12.emb_layers.1.weight', 'input_blocks.12.0.emb_layers.1.weight'),\n ('in_layers.12.emb_layers.1.bias', 'input_blocks.12.0.emb_layers.1.bias'),\n ('in_layers.12.group_norm.weight', 'input_blocks.12.0.out_layers.0.weight'),\n ('in_layers.12.group_norm.bias', 'input_blocks.12.0.out_layers.0.bias'),\n ('in_layers.12.out_layers.2.weight', 'input_blocks.12.0.out_layers.3.weight'),\n ('in_layers.12.out_layers.2.bias', 'input_blocks.12.0.out_layers.3.bias'),\n ('in_layers.13.in_layers.0.weight', 'input_blocks.13.0.in_layers.0.weight'),\n ('in_layers.13.in_layers.0.bias', 'input_blocks.13.0.in_layers.0.bias'),\n ('in_layers.13.in_layers.2.weight', 'input_blocks.13.0.in_layers.2.weight'),\n ('in_layers.13.in_layers.2.bias', 'input_blocks.13.0.in_layers.2.bias'),\n ('in_layers.13.emb_layers.1.weight', 'input_blocks.13.0.emb_layers.1.weight'),\n ('in_layers.13.emb_layers.1.bias', 'input_blocks.13.0.emb_layers.1.bias'),\n ('in_layers.13.group_norm.weight', 'input_blocks.13.0.out_layers.0.weight'),\n ('in_layers.13.group_norm.bias', 'input_blocks.13.0.out_layers.0.bias'),\n ('in_layers.13.out_layers.2.weight', 'input_blocks.13.0.out_layers.3.weight'),\n ('in_layers.13.out_layers.2.bias', 'input_blocks.13.0.out_layers.3.bias'),\n ('in_layers.13.skip_connection.weight',\n  'input_blocks.13.0.skip_connection.weight'),\n ('in_layers.13.skip_connection.bias',\n  'input_blocks.13.0.skip_connection.bias'),\n ('in_layers.13.attention_layer.group_norm.weight',\n  'input_blocks.13.1.norm.weight'),\n ('in_layers.13.attention_layer.group_norm.bias',\n  'input_blocks.13.1.norm.bias'),\n ('in_layers.13.attention_layer.qkv.weight', 'input_blocks.13.1.qkv.weight'),\n ('in_layers.13.attention_layer.qkv.bias', 'input_blocks.13.1.qkv.bias'),\n ('in_layers.13.attention_layer.encoder_kv.weight',\n  'input_blocks.13.1.encoder_kv.weight'),\n ('in_layers.13.attention_layer.encoder_kv.bias',\n  'input_blocks.13.1.encoder_kv.bias'),\n ('in_layers.13.attention_layer.proj_out.weight',\n  'input_blocks.13.1.proj_out.weight'),\n ('in_layers.13.attention_layer.proj_out.bias',\n  'input_blocks.13.1.proj_out.bias'),\n ('in_layers.14.in_layers.0.weight', 'input_blocks.14.0.in_layers.0.weight'),\n ('in_layers.14.in_layers.0.bias', 'input_blocks.14.0.in_layers.0.bias'),\n ('in_layers.14.in_layers.2.weight', 'input_blocks.14.0.in_layers.2.weight'),\n ('in_layers.14.in_layers.2.bias', 'input_blocks.14.0.in_layers.2.bias'),\n ('in_layers.14.emb_layers.1.weight', 'input_blocks.14.0.emb_layers.1.weight'),\n ('in_layers.14.emb_layers.1.bias', 'input_blocks.14.0.emb_layers.1.bias'),\n ('in_layers.14.group_norm.weight', 'input_blocks.14.0.out_layers.0.weight'),\n ('in_layers.14.group_norm.bias', 'input_blocks.14.0.out_layers.0.bias'),\n ('in_layers.14.out_layers.2.weight', 'input_blocks.14.0.out_layers.3.weight'),\n ('in_layers.14.out_layers.2.bias', 'input_blocks.14.0.out_layers.3.bias'),\n ('in_layers.14.attention_layer.group_norm.weight',\n  'input_blocks.14.1.norm.weight'),\n ('in_layers.14.attention_layer.group_norm.bias',\n  'input_blocks.14.1.norm.bias'),\n ('in_layers.14.attention_layer.qkv.weight', 'input_blocks.14.1.qkv.weight'),\n ('in_layers.14.attention_layer.qkv.bias', 'input_blocks.14.1.qkv.bias'),\n ('in_layers.14.attention_layer.encoder_kv.weight',\n  'input_blocks.14.1.encoder_kv.weight'),\n ('in_layers.14.attention_layer.encoder_kv.bias',\n  'input_blocks.14.1.encoder_kv.bias'),\n ('in_layers.14.attention_layer.proj_out.weight',\n  'input_blocks.14.1.proj_out.weight'),\n ('in_layers.14.attention_layer.proj_out.bias',\n  'input_blocks.14.1.proj_out.bias'),\n ('in_layers.15.in_layers.0.weight', 'input_blocks.15.0.in_layers.0.weight'),\n ('in_layers.15.in_layers.0.bias', 'input_blocks.15.0.in_layers.0.bias'),\n ('in_layers.15.in_layers.2.weight', 'input_blocks.15.0.in_layers.2.weight'),\n ('in_layers.15.in_layers.2.bias', 'input_blocks.15.0.in_layers.2.bias'),\n ('in_layers.15.emb_layers.1.weight', 'input_blocks.15.0.emb_layers.1.weight'),\n ('in_layers.15.emb_layers.1.bias', 'input_blocks.15.0.emb_layers.1.bias'),\n ('in_layers.15.group_norm.weight', 'input_blocks.15.0.out_layers.0.weight'),\n ('in_layers.15.group_norm.bias', 'input_blocks.15.0.out_layers.0.bias'),\n ('in_layers.15.out_layers.2.weight', 'input_blocks.15.0.out_layers.3.weight'),\n ('in_layers.15.out_layers.2.bias', 'input_blocks.15.0.out_layers.3.bias'),\n ('in_layers.15.attention_layer.group_norm.weight',\n  'input_blocks.15.1.norm.weight'),\n ('in_layers.15.attention_layer.group_norm.bias',\n  'input_blocks.15.1.norm.bias'),\n ('in_layers.15.attention_layer.qkv.weight', 'input_blocks.15.1.qkv.weight'),\n ('in_layers.15.attention_layer.qkv.bias', 'input_blocks.15.1.qkv.bias'),\n ('in_layers.15.attention_layer.encoder_kv.weight',\n  'input_blocks.15.1.encoder_kv.weight'),\n ('in_layers.15.attention_layer.encoder_kv.bias',\n  'input_blocks.15.1.encoder_kv.bias'),\n ('in_layers.15.attention_layer.proj_out.weight',\n  'input_blocks.15.1.proj_out.weight'),\n ('in_layers.15.attention_layer.proj_out.bias',\n  'input_blocks.15.1.proj_out.bias'),\n ('middle_layers.0.in_layers.0.weight', 'middle_block.0.in_layers.0.weight'),\n ('middle_layers.0.in_layers.0.bias', 'middle_block.0.in_layers.0.bias'),\n ('middle_layers.0.in_layers.2.weight', 'middle_block.0.in_layers.2.weight'),\n ('middle_layers.0.in_layers.2.bias', 'middle_block.0.in_layers.2.bias'),\n ('middle_layers.0.emb_layers.1.weight', 'middle_block.0.emb_layers.1.weight'),\n ('middle_layers.0.emb_layers.1.bias', 'middle_block.0.emb_layers.1.bias'),\n ('middle_layers.0.group_norm.weight', 'middle_block.0.out_layers.0.weight'),\n ('middle_layers.0.group_norm.bias', 'middle_block.0.out_layers.0.bias'),\n ('middle_layers.0.out_layers.2.weight', 'middle_block.0.out_layers.3.weight'),\n ('middle_layers.0.out_layers.2.bias', 'middle_block.0.out_layers.3.bias'),\n ('middle_layers.1.group_norm.weight', 'middle_block.1.norm.weight'),\n ('middle_layers.1.group_norm.bias', 'middle_block.1.norm.bias'),\n ('middle_layers.1.qkv.weight', 'middle_block.1.qkv.weight'),\n ('middle_layers.1.qkv.bias', 'middle_block.1.qkv.bias'),\n ('middle_layers.1.encoder_kv.weight', 'middle_block.1.encoder_kv.weight'),\n ('middle_layers.1.encoder_kv.bias', 'middle_block.1.encoder_kv.bias'),\n ('middle_layers.1.proj_out.weight', 'middle_block.1.proj_out.weight'),\n ('middle_layers.1.proj_out.bias', 'middle_block.1.proj_out.bias'),\n ('middle_layers.2.in_layers.0.weight', 'middle_block.2.in_layers.0.weight'),\n ('middle_layers.2.in_layers.0.bias', 'middle_block.2.in_layers.0.bias'),\n ('middle_layers.2.in_layers.2.weight', 'middle_block.2.in_layers.2.weight'),\n ('middle_layers.2.in_layers.2.bias', 'middle_block.2.in_layers.2.bias'),\n ('middle_layers.2.emb_layers.1.weight', 'middle_block.2.emb_layers.1.weight'),\n ('middle_layers.2.emb_layers.1.bias', 'middle_block.2.emb_layers.1.bias'),\n ('middle_layers.2.group_norm.weight', 'middle_block.2.out_layers.0.weight'),\n ('middle_layers.2.group_norm.bias', 'middle_block.2.out_layers.0.bias'),\n ('middle_layers.2.out_layers.2.weight', 'middle_block.2.out_layers.3.weight'),\n ('middle_layers.2.out_layers.2.bias', 'middle_block.2.out_layers.3.bias'),\n ('out_layers.0.in_layers.0.weight', 'output_blocks.0.0.in_layers.0.weight'),\n ('out_layers.0.in_layers.0.bias', 'output_blocks.0.0.in_layers.0.bias'),\n ('out_layers.0.in_layers.2.weight', 'output_blocks.0.0.in_layers.2.weight'),\n ('out_layers.0.in_layers.2.bias', 'output_blocks.0.0.in_layers.2.bias'),\n ('out_layers.0.emb_layers.1.weight', 'output_blocks.0.0.emb_layers.1.weight'),\n ('out_layers.0.emb_layers.1.bias', 'output_blocks.0.0.emb_layers.1.bias'),\n ('out_layers.0.group_norm.weight', 'output_blocks.0.0.out_layers.0.weight'),\n ('out_layers.0.group_norm.bias', 'output_blocks.0.0.out_layers.0.bias'),\n ('out_layers.0.out_layers.2.weight', 'output_blocks.0.0.out_layers.3.weight'),\n ('out_layers.0.out_layers.2.bias', 'output_blocks.0.0.out_layers.3.bias'),\n ('out_layers.0.skip_connection.weight',\n  'output_blocks.0.0.skip_connection.weight'),\n ('out_layers.0.skip_connection.bias',\n  'output_blocks.0.0.skip_connection.bias'),\n ('out_layers.0.attention_layer.group_norm.weight',\n  'output_blocks.0.1.norm.weight'),\n ('out_layers.0.attention_layer.group_norm.bias',\n  'output_blocks.0.1.norm.bias'),\n ('out_layers.0.attention_layer.qkv.weight', 'output_blocks.0.1.qkv.weight'),\n ('out_layers.0.attention_layer.qkv.bias', 'output_blocks.0.1.qkv.bias'),\n ('out_layers.0.attention_layer.encoder_kv.weight',\n  'output_blocks.0.1.encoder_kv.weight'),\n ('out_layers.0.attention_layer.encoder_kv.bias',\n  'output_blocks.0.1.encoder_kv.bias'),\n ('out_layers.0.attention_layer.proj_out.weight',\n  'output_blocks.0.1.proj_out.weight'),\n ('out_layers.0.attention_layer.proj_out.bias',\n  'output_blocks.0.1.proj_out.bias'),\n ('out_layers.1.in_layers.0.weight', 'output_blocks.1.0.in_layers.0.weight'),\n ('out_layers.1.in_layers.0.bias', 'output_blocks.1.0.in_layers.0.bias'),\n ('out_layers.1.in_layers.2.weight', 'output_blocks.1.0.in_layers.2.weight'),\n ('out_layers.1.in_layers.2.bias', 'output_blocks.1.0.in_layers.2.bias'),\n ('out_layers.1.emb_layers.1.weight', 'output_blocks.1.0.emb_layers.1.weight'),\n ('out_layers.1.emb_layers.1.bias', 'output_blocks.1.0.emb_layers.1.bias'),\n ('out_layers.1.group_norm.weight', 'output_blocks.1.0.out_layers.0.weight'),\n ('out_layers.1.group_norm.bias', 'output_blocks.1.0.out_layers.0.bias'),\n ('out_layers.1.out_layers.2.weight', 'output_blocks.1.0.out_layers.3.weight'),\n ('out_layers.1.out_layers.2.bias', 'output_blocks.1.0.out_layers.3.bias'),\n ('out_layers.1.skip_connection.weight',\n  'output_blocks.1.0.skip_connection.weight'),\n ('out_layers.1.skip_connection.bias',\n  'output_blocks.1.0.skip_connection.bias'),\n ('out_layers.1.attention_layer.group_norm.weight',\n  'output_blocks.1.1.norm.weight'),\n ('out_layers.1.attention_layer.group_norm.bias',\n  'output_blocks.1.1.norm.bias'),\n ('out_layers.1.attention_layer.qkv.weight', 'output_blocks.1.1.qkv.weight'),\n ('out_layers.1.attention_layer.qkv.bias', 'output_blocks.1.1.qkv.bias'),\n ('out_layers.1.attention_layer.encoder_kv.weight',\n  'output_blocks.1.1.encoder_kv.weight'),\n ('out_layers.1.attention_layer.encoder_kv.bias',\n  'output_blocks.1.1.encoder_kv.bias'),\n ('out_layers.1.attention_layer.proj_out.weight',\n  'output_blocks.1.1.proj_out.weight'),\n ('out_layers.1.attention_layer.proj_out.bias',\n  'output_blocks.1.1.proj_out.bias'),\n ('out_layers.2.in_layers.0.weight', 'output_blocks.2.0.in_layers.0.weight'),\n ('out_layers.2.in_layers.0.bias', 'output_blocks.2.0.in_layers.0.bias'),\n ('out_layers.2.in_layers.2.weight', 'output_blocks.2.0.in_layers.2.weight'),\n ('out_layers.2.in_layers.2.bias', 'output_blocks.2.0.in_layers.2.bias'),\n ('out_layers.2.emb_layers.1.weight', 'output_blocks.2.0.emb_layers.1.weight'),\n ('out_layers.2.emb_layers.1.bias', 'output_blocks.2.0.emb_layers.1.bias'),\n ('out_layers.2.group_norm.weight', 'output_blocks.2.0.out_layers.0.weight'),\n ('out_layers.2.group_norm.bias', 'output_blocks.2.0.out_layers.0.bias'),\n ('out_layers.2.out_layers.2.weight', 'output_blocks.2.0.out_layers.3.weight'),\n ('out_layers.2.out_layers.2.bias', 'output_blocks.2.0.out_layers.3.bias'),\n ('out_layers.2.skip_connection.weight',\n  'output_blocks.2.0.skip_connection.weight'),\n ('out_layers.2.skip_connection.bias',\n  'output_blocks.2.0.skip_connection.bias'),\n ('out_layers.2.attention_layer.group_norm.weight',\n  'output_blocks.2.1.norm.weight'),\n ('out_layers.2.attention_layer.group_norm.bias',\n  'output_blocks.2.1.norm.bias'),\n ('out_layers.2.attention_layer.qkv.weight', 'output_blocks.2.1.qkv.weight'),\n ('out_layers.2.attention_layer.qkv.bias', 'output_blocks.2.1.qkv.bias'),\n ('out_layers.2.attention_layer.encoder_kv.weight',\n  'output_blocks.2.1.encoder_kv.weight'),\n ('out_layers.2.attention_layer.encoder_kv.bias',\n  'output_blocks.2.1.encoder_kv.bias'),\n ('out_layers.2.attention_layer.proj_out.weight',\n  'output_blocks.2.1.proj_out.weight'),\n ('out_layers.2.attention_layer.proj_out.bias',\n  'output_blocks.2.1.proj_out.bias'),\n ('out_layers.3.0.in_layers.0.weight', 'output_blocks.3.0.in_layers.0.weight'),\n ('out_layers.3.0.in_layers.0.bias', 'output_blocks.3.0.in_layers.0.bias'),\n ('out_layers.3.0.in_layers.2.weight', 'output_blocks.3.0.in_layers.2.weight'),\n ('out_layers.3.0.in_layers.2.bias', 'output_blocks.3.0.in_layers.2.bias'),\n ('out_layers.3.0.emb_layers.1.weight',\n  'output_blocks.3.0.emb_layers.1.weight'),\n ('out_layers.3.0.emb_layers.1.bias', 'output_blocks.3.0.emb_layers.1.bias'),\n ('out_layers.3.0.group_norm.weight', 'output_blocks.3.0.out_layers.0.weight'),\n ('out_layers.3.0.group_norm.bias', 'output_blocks.3.0.out_layers.0.bias'),\n ('out_layers.3.0.out_layers.2.weight',\n  'output_blocks.3.0.out_layers.3.weight'),\n ('out_layers.3.0.out_layers.2.bias', 'output_blocks.3.0.out_layers.3.bias'),\n ('out_layers.3.0.skip_connection.weight',\n  'output_blocks.3.0.skip_connection.weight'),\n ('out_layers.3.0.skip_connection.bias',\n  'output_blocks.3.0.skip_connection.bias'),\n ('out_layers.3.0.attention_layer.group_norm.weight',\n  'output_blocks.3.1.norm.weight'),\n ('out_layers.3.0.attention_layer.group_norm.bias',\n  'output_blocks.3.1.norm.bias'),\n ('out_layers.3.0.attention_layer.qkv.weight', 'output_blocks.3.1.qkv.weight'),\n ('out_layers.3.0.attention_layer.qkv.bias', 'output_blocks.3.1.qkv.bias'),\n ('out_layers.3.0.attention_layer.encoder_kv.weight',\n  'output_blocks.3.1.encoder_kv.weight'),\n ('out_layers.3.0.attention_layer.encoder_kv.bias',\n  'output_blocks.3.1.encoder_kv.bias'),\n ('out_layers.3.0.attention_layer.proj_out.weight',\n  'output_blocks.3.1.proj_out.weight'),\n ('out_layers.3.0.attention_layer.proj_out.bias',\n  'output_blocks.3.1.proj_out.bias'),\n ('out_layers.3.1.in_layers.0.weight', 'output_blocks.3.2.in_layers.0.weight'),\n ('out_layers.3.1.in_layers.0.bias', 'output_blocks.3.2.in_layers.0.bias'),\n ('out_layers.3.1.in_layers.2.weight', 'output_blocks.3.2.in_layers.2.weight'),\n ('out_layers.3.1.in_layers.2.bias', 'output_blocks.3.2.in_layers.2.bias'),\n ('out_layers.3.1.emb_layers.1.weight',\n  'output_blocks.3.2.emb_layers.1.weight'),\n ('out_layers.3.1.emb_layers.1.bias', 'output_blocks.3.2.emb_layers.1.bias'),\n ('out_layers.3.1.group_norm.weight', 'output_blocks.3.2.out_layers.0.weight'),\n ('out_layers.3.1.group_norm.bias', 'output_blocks.3.2.out_layers.0.bias'),\n ('out_layers.3.1.out_layers.2.weight',\n  'output_blocks.3.2.out_layers.3.weight'),\n ('out_layers.3.1.out_layers.2.bias', 'output_blocks.3.2.out_layers.3.bias'),\n ('out_layers.4.in_layers.0.weight', 'output_blocks.4.0.in_layers.0.weight'),\n ('out_layers.4.in_layers.0.bias', 'output_blocks.4.0.in_layers.0.bias'),\n ('out_layers.4.in_layers.2.weight', 'output_blocks.4.0.in_layers.2.weight'),\n ('out_layers.4.in_layers.2.bias', 'output_blocks.4.0.in_layers.2.bias'),\n ('out_layers.4.emb_layers.1.weight', 'output_blocks.4.0.emb_layers.1.weight'),\n ('out_layers.4.emb_layers.1.bias', 'output_blocks.4.0.emb_layers.1.bias'),\n ('out_layers.4.group_norm.weight', 'output_blocks.4.0.out_layers.0.weight'),\n ('out_layers.4.group_norm.bias', 'output_blocks.4.0.out_layers.0.bias'),\n ('out_layers.4.out_layers.2.weight', 'output_blocks.4.0.out_layers.3.weight'),\n ('out_layers.4.out_layers.2.bias', 'output_blocks.4.0.out_layers.3.bias'),\n ('out_layers.4.skip_connection.weight',\n  'output_blocks.4.0.skip_connection.weight'),\n ('out_layers.4.skip_connection.bias',\n  'output_blocks.4.0.skip_connection.bias'),\n ('out_layers.4.attention_layer.group_norm.weight',\n  'output_blocks.4.1.norm.weight'),\n ('out_layers.4.attention_layer.group_norm.bias',\n  'output_blocks.4.1.norm.bias'),\n ('out_layers.4.attention_layer.qkv.weight', 'output_blocks.4.1.qkv.weight'),\n ('out_layers.4.attention_layer.qkv.bias', 'output_blocks.4.1.qkv.bias'),\n ('out_layers.4.attention_layer.encoder_kv.weight',\n  'output_blocks.4.1.encoder_kv.weight'),\n ('out_layers.4.attention_layer.encoder_kv.bias',\n  'output_blocks.4.1.encoder_kv.bias'),\n ('out_layers.4.attention_layer.proj_out.weight',\n  'output_blocks.4.1.proj_out.weight'),\n ('out_layers.4.attention_layer.proj_out.bias',\n  'output_blocks.4.1.proj_out.bias'),\n ('out_layers.5.in_layers.0.weight', 'output_blocks.5.0.in_layers.0.weight'),\n ('out_layers.5.in_layers.0.bias', 'output_blocks.5.0.in_layers.0.bias'),\n ('out_layers.5.in_layers.2.weight', 'output_blocks.5.0.in_layers.2.weight'),\n ('out_layers.5.in_layers.2.bias', 'output_blocks.5.0.in_layers.2.bias'),\n ('out_layers.5.emb_layers.1.weight', 'output_blocks.5.0.emb_layers.1.weight'),\n ('out_layers.5.emb_layers.1.bias', 'output_blocks.5.0.emb_layers.1.bias'),\n ('out_layers.5.group_norm.weight', 'output_blocks.5.0.out_layers.0.weight'),\n ('out_layers.5.group_norm.bias', 'output_blocks.5.0.out_layers.0.bias'),\n ('out_layers.5.out_layers.2.weight', 'output_blocks.5.0.out_layers.3.weight'),\n ('out_layers.5.out_layers.2.bias', 'output_blocks.5.0.out_layers.3.bias'),\n ('out_layers.5.skip_connection.weight',\n  'output_blocks.5.0.skip_connection.weight'),\n ('out_layers.5.skip_connection.bias',\n  'output_blocks.5.0.skip_connection.bias'),\n ('out_layers.5.attention_layer.group_norm.weight',\n  'output_blocks.5.1.norm.weight'),\n ('out_layers.5.attention_layer.group_norm.bias',\n  'output_blocks.5.1.norm.bias'),\n ('out_layers.5.attention_layer.qkv.weight', 'output_blocks.5.1.qkv.weight'),\n ('out_layers.5.attention_layer.qkv.bias', 'output_blocks.5.1.qkv.bias'),\n ('out_layers.5.attention_layer.encoder_kv.weight',\n  'output_blocks.5.1.encoder_kv.weight'),\n ('out_layers.5.attention_layer.encoder_kv.bias',\n  'output_blocks.5.1.encoder_kv.bias'),\n ('out_layers.5.attention_layer.proj_out.weight',\n  'output_blocks.5.1.proj_out.weight'),\n ('out_layers.5.attention_layer.proj_out.bias',\n  'output_blocks.5.1.proj_out.bias'),\n ('out_layers.6.in_layers.0.weight', 'output_blocks.6.0.in_layers.0.weight'),\n ('out_layers.6.in_layers.0.bias', 'output_blocks.6.0.in_layers.0.bias'),\n ('out_layers.6.in_layers.2.weight', 'output_blocks.6.0.in_layers.2.weight'),\n ('out_layers.6.in_layers.2.bias', 'output_blocks.6.0.in_layers.2.bias'),\n ('out_layers.6.emb_layers.1.weight', 'output_blocks.6.0.emb_layers.1.weight'),\n ('out_layers.6.emb_layers.1.bias', 'output_blocks.6.0.emb_layers.1.bias'),\n ('out_layers.6.group_norm.weight', 'output_blocks.6.0.out_layers.0.weight'),\n ('out_layers.6.group_norm.bias', 'output_blocks.6.0.out_layers.0.bias'),\n ('out_layers.6.out_layers.2.weight', 'output_blocks.6.0.out_layers.3.weight'),\n ('out_layers.6.out_layers.2.bias', 'output_blocks.6.0.out_layers.3.bias'),\n ('out_layers.6.skip_connection.weight',\n  'output_blocks.6.0.skip_connection.weight'),\n ('out_layers.6.skip_connection.bias',\n  'output_blocks.6.0.skip_connection.bias'),\n ('out_layers.6.attention_layer.group_norm.weight',\n  'output_blocks.6.1.norm.weight'),\n ('out_layers.6.attention_layer.group_norm.bias',\n  'output_blocks.6.1.norm.bias'),\n ('out_layers.6.attention_layer.qkv.weight', 'output_blocks.6.1.qkv.weight'),\n ('out_layers.6.attention_layer.qkv.bias', 'output_blocks.6.1.qkv.bias'),\n ('out_layers.6.attention_layer.encoder_kv.weight',\n  'output_blocks.6.1.encoder_kv.weight'),\n ('out_layers.6.attention_layer.encoder_kv.bias',\n  'output_blocks.6.1.encoder_kv.bias'),\n ('out_layers.6.attention_layer.proj_out.weight',\n  'output_blocks.6.1.proj_out.weight'),\n ('out_layers.6.attention_layer.proj_out.bias',\n  'output_blocks.6.1.proj_out.bias'),\n ('out_layers.7.0.in_layers.0.weight', 'output_blocks.7.0.in_layers.0.weight'),\n ('out_layers.7.0.in_layers.0.bias', 'output_blocks.7.0.in_layers.0.bias'),\n ('out_layers.7.0.in_layers.2.weight', 'output_blocks.7.0.in_layers.2.weight'),\n ('out_layers.7.0.in_layers.2.bias', 'output_blocks.7.0.in_layers.2.bias'),\n ('out_layers.7.0.emb_layers.1.weight',\n  'output_blocks.7.0.emb_layers.1.weight'),\n ('out_layers.7.0.emb_layers.1.bias', 'output_blocks.7.0.emb_layers.1.bias'),\n ('out_layers.7.0.group_norm.weight', 'output_blocks.7.0.out_layers.0.weight'),\n ('out_layers.7.0.group_norm.bias', 'output_blocks.7.0.out_layers.0.bias'),\n ('out_layers.7.0.out_layers.2.weight',\n  'output_blocks.7.0.out_layers.3.weight'),\n ('out_layers.7.0.out_layers.2.bias', 'output_blocks.7.0.out_layers.3.bias'),\n ('out_layers.7.0.skip_connection.weight',\n  'output_blocks.7.0.skip_connection.weight'),\n ('out_layers.7.0.skip_connection.bias',\n  'output_blocks.7.0.skip_connection.bias'),\n ('out_layers.7.0.attention_layer.group_norm.weight',\n  'output_blocks.7.1.norm.weight'),\n ('out_layers.7.0.attention_layer.group_norm.bias',\n  'output_blocks.7.1.norm.bias'),\n ('out_layers.7.0.attention_layer.qkv.weight', 'output_blocks.7.1.qkv.weight'),\n ('out_layers.7.0.attention_layer.qkv.bias', 'output_blocks.7.1.qkv.bias'),\n ('out_layers.7.0.attention_layer.encoder_kv.weight',\n  'output_blocks.7.1.encoder_kv.weight'),\n ('out_layers.7.0.attention_layer.encoder_kv.bias',\n  'output_blocks.7.1.encoder_kv.bias'),\n ('out_layers.7.0.attention_layer.proj_out.weight',\n  'output_blocks.7.1.proj_out.weight'),\n ('out_layers.7.0.attention_layer.proj_out.bias',\n  'output_blocks.7.1.proj_out.bias'),\n ('out_layers.7.1.in_layers.0.weight', 'output_blocks.7.2.in_layers.0.weight'),\n ('out_layers.7.1.in_layers.0.bias', 'output_blocks.7.2.in_layers.0.bias'),\n ('out_layers.7.1.in_layers.2.weight', 'output_blocks.7.2.in_layers.2.weight'),\n ('out_layers.7.1.in_layers.2.bias', 'output_blocks.7.2.in_layers.2.bias'),\n ('out_layers.7.1.emb_layers.1.weight',\n  'output_blocks.7.2.emb_layers.1.weight'),\n ('out_layers.7.1.emb_layers.1.bias', 'output_blocks.7.2.emb_layers.1.bias'),\n ('out_layers.7.1.group_norm.weight', 'output_blocks.7.2.out_layers.0.weight'),\n ('out_layers.7.1.group_norm.bias', 'output_blocks.7.2.out_layers.0.bias'),\n ('out_layers.7.1.out_layers.2.weight',\n  'output_blocks.7.2.out_layers.3.weight'),\n ('out_layers.7.1.out_layers.2.bias', 'output_blocks.7.2.out_layers.3.bias'),\n ('out_layers.8.in_layers.0.weight', 'output_blocks.8.0.in_layers.0.weight'),\n ('out_layers.8.in_layers.0.bias', 'output_blocks.8.0.in_layers.0.bias'),\n ('out_layers.8.in_layers.2.weight', 'output_blocks.8.0.in_layers.2.weight'),\n ('out_layers.8.in_layers.2.bias', 'output_blocks.8.0.in_layers.2.bias'),\n ('out_layers.8.emb_layers.1.weight', 'output_blocks.8.0.emb_layers.1.weight'),\n ('out_layers.8.emb_layers.1.bias', 'output_blocks.8.0.emb_layers.1.bias'),\n ('out_layers.8.group_norm.weight', 'output_blocks.8.0.out_layers.0.weight'),\n ('out_layers.8.group_norm.bias', 'output_blocks.8.0.out_layers.0.bias'),\n ('out_layers.8.out_layers.2.weight', 'output_blocks.8.0.out_layers.3.weight'),\n ('out_layers.8.out_layers.2.bias', 'output_blocks.8.0.out_layers.3.bias'),\n ('out_layers.8.skip_connection.weight',\n  'output_blocks.8.0.skip_connection.weight'),\n ('out_layers.8.skip_connection.bias',\n  'output_blocks.8.0.skip_connection.bias'),\n ('out_layers.8.attention_layer.group_norm.weight',\n  'output_blocks.8.1.norm.weight'),\n ('out_layers.8.attention_layer.group_norm.bias',\n  'output_blocks.8.1.norm.bias'),\n ('out_layers.8.attention_layer.qkv.weight', 'output_blocks.8.1.qkv.weight'),\n ('out_layers.8.attention_layer.qkv.bias', 'output_blocks.8.1.qkv.bias'),\n ('out_layers.8.attention_layer.encoder_kv.weight',\n  'output_blocks.8.1.encoder_kv.weight'),\n ('out_layers.8.attention_layer.encoder_kv.bias',\n  'output_blocks.8.1.encoder_kv.bias'),\n ('out_layers.8.attention_layer.proj_out.weight',\n  'output_blocks.8.1.proj_out.weight'),\n ('out_layers.8.attention_layer.proj_out.bias',\n  'output_blocks.8.1.proj_out.bias'),\n ('out_layers.9.in_layers.0.weight', 'output_blocks.9.0.in_layers.0.weight'),\n ('out_layers.9.in_layers.0.bias', 'output_blocks.9.0.in_layers.0.bias'),\n ('out_layers.9.in_layers.2.weight', 'output_blocks.9.0.in_layers.2.weight'),\n ('out_layers.9.in_layers.2.bias', 'output_blocks.9.0.in_layers.2.bias'),\n ('out_layers.9.emb_layers.1.weight', 'output_blocks.9.0.emb_layers.1.weight'),\n ('out_layers.9.emb_layers.1.bias', 'output_blocks.9.0.emb_layers.1.bias'),\n ('out_layers.9.group_norm.weight', 'output_blocks.9.0.out_layers.0.weight'),\n ('out_layers.9.group_norm.bias', 'output_blocks.9.0.out_layers.0.bias'),\n ('out_layers.9.out_layers.2.weight', 'output_blocks.9.0.out_layers.3.weight'),\n ('out_layers.9.out_layers.2.bias', 'output_blocks.9.0.out_layers.3.bias'),\n ('out_layers.9.skip_connection.weight',\n  'output_blocks.9.0.skip_connection.weight'),\n ('out_layers.9.skip_connection.bias',\n  'output_blocks.9.0.skip_connection.bias'),\n ('out_layers.9.attention_layer.group_norm.weight',\n  'output_blocks.9.1.norm.weight'),\n ('out_layers.9.attention_layer.group_norm.bias',\n  'output_blocks.9.1.norm.bias'),\n ('out_layers.9.attention_layer.qkv.weight', 'output_blocks.9.1.qkv.weight'),\n ('out_layers.9.attention_layer.qkv.bias', 'output_blocks.9.1.qkv.bias'),\n ('out_layers.9.attention_layer.encoder_kv.weight',\n  'output_blocks.9.1.encoder_kv.weight'),\n ('out_layers.9.attention_layer.encoder_kv.bias',\n  'output_blocks.9.1.encoder_kv.bias'),\n ('out_layers.9.attention_layer.proj_out.weight',\n  'output_blocks.9.1.proj_out.weight'),\n ('out_layers.9.attention_layer.proj_out.bias',\n  'output_blocks.9.1.proj_out.bias'),\n ('out_layers.10.in_layers.0.weight', 'output_blocks.10.0.in_layers.0.weight'),\n ('out_layers.10.in_layers.0.bias', 'output_blocks.10.0.in_layers.0.bias'),\n ('out_layers.10.in_layers.2.weight', 'output_blocks.10.0.in_layers.2.weight'),\n ('out_layers.10.in_layers.2.bias', 'output_blocks.10.0.in_layers.2.bias'),\n ('out_layers.10.emb_layers.1.weight',\n  'output_blocks.10.0.emb_layers.1.weight'),\n ('out_layers.10.emb_layers.1.bias', 'output_blocks.10.0.emb_layers.1.bias'),\n ('out_layers.10.group_norm.weight', 'output_blocks.10.0.out_layers.0.weight'),\n ('out_layers.10.group_norm.bias', 'output_blocks.10.0.out_layers.0.bias'),\n ('out_layers.10.out_layers.2.weight',\n  'output_blocks.10.0.out_layers.3.weight'),\n ('out_layers.10.out_layers.2.bias', 'output_blocks.10.0.out_layers.3.bias'),\n ('out_layers.10.skip_connection.weight',\n  'output_blocks.10.0.skip_connection.weight'),\n ('out_layers.10.skip_connection.bias',\n  'output_blocks.10.0.skip_connection.bias'),\n ('out_layers.10.attention_layer.group_norm.weight',\n  'output_blocks.10.1.norm.weight'),\n ('out_layers.10.attention_layer.group_norm.bias',\n  'output_blocks.10.1.norm.bias'),\n ('out_layers.10.attention_layer.qkv.weight', 'output_blocks.10.1.qkv.weight'),\n ('out_layers.10.attention_layer.qkv.bias', 'output_blocks.10.1.qkv.bias'),\n ('out_layers.10.attention_layer.encoder_kv.weight',\n  'output_blocks.10.1.encoder_kv.weight'),\n ('out_layers.10.attention_layer.encoder_kv.bias',\n  'output_blocks.10.1.encoder_kv.bias'),\n ('out_layers.10.attention_layer.proj_out.weight',\n  'output_blocks.10.1.proj_out.weight'),\n ('out_layers.10.attention_layer.proj_out.bias',\n  'output_blocks.10.1.proj_out.bias'),\n ('out_layers.11.0.in_layers.0.weight',\n  'output_blocks.11.0.in_layers.0.weight'),\n ('out_layers.11.0.in_layers.0.bias', 'output_blocks.11.0.in_layers.0.bias'),\n ('out_layers.11.0.in_layers.2.weight',\n  'output_blocks.11.0.in_layers.2.weight'),\n ('out_layers.11.0.in_layers.2.bias', 'output_blocks.11.0.in_layers.2.bias'),\n ('out_layers.11.0.emb_layers.1.weight',\n  'output_blocks.11.0.emb_layers.1.weight'),\n ('out_layers.11.0.emb_layers.1.bias', 'output_blocks.11.0.emb_layers.1.bias'),\n ('out_layers.11.0.group_norm.weight',\n  'output_blocks.11.0.out_layers.0.weight'),\n ('out_layers.11.0.group_norm.bias', 'output_blocks.11.0.out_layers.0.bias'),\n ('out_layers.11.0.out_layers.2.weight',\n  'output_blocks.11.0.out_layers.3.weight'),\n ('out_layers.11.0.out_layers.2.bias', 'output_blocks.11.0.out_layers.3.bias'),\n ('out_layers.11.0.skip_connection.weight',\n  'output_blocks.11.0.skip_connection.weight'),\n ('out_layers.11.0.skip_connection.bias',\n  'output_blocks.11.0.skip_connection.bias'),\n ('out_layers.11.0.attention_layer.group_norm.weight',\n  'output_blocks.11.1.norm.weight'),\n ('out_layers.11.0.attention_layer.group_norm.bias',\n  'output_blocks.11.1.norm.bias'),\n ('out_layers.11.0.attention_layer.qkv.weight',\n  'output_blocks.11.1.qkv.weight'),\n ('out_layers.11.0.attention_layer.qkv.bias', 'output_blocks.11.1.qkv.bias'),\n ('out_layers.11.0.attention_layer.encoder_kv.weight',\n  'output_blocks.11.1.encoder_kv.weight'),\n ('out_layers.11.0.attention_layer.encoder_kv.bias',\n  'output_blocks.11.1.encoder_kv.bias'),\n ('out_layers.11.0.attention_layer.proj_out.weight',\n  'output_blocks.11.1.proj_out.weight'),\n ('out_layers.11.0.attention_layer.proj_out.bias',\n  'output_blocks.11.1.proj_out.bias'),\n ('out_layers.11.1.in_layers.0.weight',\n  'output_blocks.11.2.in_layers.0.weight'),\n ('out_layers.11.1.in_layers.0.bias', 'output_blocks.11.2.in_layers.0.bias'),\n ('out_layers.11.1.in_layers.2.weight',\n  'output_blocks.11.2.in_layers.2.weight'),\n ('out_layers.11.1.in_layers.2.bias', 'output_blocks.11.2.in_layers.2.bias'),\n ('out_layers.11.1.emb_layers.1.weight',\n  'output_blocks.11.2.emb_layers.1.weight'),\n ('out_layers.11.1.emb_layers.1.bias', 'output_blocks.11.2.emb_layers.1.bias'),\n ('out_layers.11.1.group_norm.weight',\n  'output_blocks.11.2.out_layers.0.weight'),\n ('out_layers.11.1.group_norm.bias', 'output_blocks.11.2.out_layers.0.bias'),\n ('out_layers.11.1.out_layers.2.weight',\n  'output_blocks.11.2.out_layers.3.weight'),\n ('out_layers.11.1.out_layers.2.bias', 'output_blocks.11.2.out_layers.3.bias'),\n ('out_layers.12.in_layers.0.weight', 'output_blocks.12.0.in_layers.0.weight'),\n ('out_layers.12.in_layers.0.bias', 'output_blocks.12.0.in_layers.0.bias'),\n ('out_layers.12.in_layers.2.weight', 'output_blocks.12.0.in_layers.2.weight'),\n ('out_layers.12.in_layers.2.bias', 'output_blocks.12.0.in_layers.2.bias'),\n ('out_layers.12.emb_layers.1.weight',\n  'output_blocks.12.0.emb_layers.1.weight'),\n ('out_layers.12.emb_layers.1.bias', 'output_blocks.12.0.emb_layers.1.bias'),\n ('out_layers.12.group_norm.weight', 'output_blocks.12.0.out_layers.0.weight'),\n ('out_layers.12.group_norm.bias', 'output_blocks.12.0.out_layers.0.bias'),\n ('out_layers.12.out_layers.2.weight',\n  'output_blocks.12.0.out_layers.3.weight'),\n ('out_layers.12.out_layers.2.bias', 'output_blocks.12.0.out_layers.3.bias'),\n ('out_layers.12.skip_connection.weight',\n  'output_blocks.12.0.skip_connection.weight'),\n ('out_layers.12.skip_connection.bias',\n  'output_blocks.12.0.skip_connection.bias'),\n ('out_layers.13.in_layers.0.weight', 'output_blocks.13.0.in_layers.0.weight'),\n ('out_layers.13.in_layers.0.bias', 'output_blocks.13.0.in_layers.0.bias'),\n ('out_layers.13.in_layers.2.weight', 'output_blocks.13.0.in_layers.2.weight'),\n ('out_layers.13.in_layers.2.bias', 'output_blocks.13.0.in_layers.2.bias'),\n ('out_layers.13.emb_layers.1.weight',\n  'output_blocks.13.0.emb_layers.1.weight'),\n ('out_layers.13.emb_layers.1.bias', 'output_blocks.13.0.emb_layers.1.bias'),\n ('out_layers.13.group_norm.weight', 'output_blocks.13.0.out_layers.0.weight'),\n ('out_layers.13.group_norm.bias', 'output_blocks.13.0.out_layers.0.bias'),\n ('out_layers.13.out_layers.2.weight',\n  'output_blocks.13.0.out_layers.3.weight'),\n ('out_layers.13.out_layers.2.bias', 'output_blocks.13.0.out_layers.3.bias'),\n ('out_layers.13.skip_connection.weight',\n  'output_blocks.13.0.skip_connection.weight'),\n ('out_layers.13.skip_connection.bias',\n  'output_blocks.13.0.skip_connection.bias'),\n ('out_layers.14.in_layers.0.weight', 'output_blocks.14.0.in_layers.0.weight'),\n ('out_layers.14.in_layers.0.bias', 'output_blocks.14.0.in_layers.0.bias'),\n ('out_layers.14.in_layers.2.weight', 'output_blocks.14.0.in_layers.2.weight'),\n ('out_layers.14.in_layers.2.bias', 'output_blocks.14.0.in_layers.2.bias'),\n ('out_layers.14.emb_layers.1.weight',\n  'output_blocks.14.0.emb_layers.1.weight'),\n ('out_layers.14.emb_layers.1.bias', 'output_blocks.14.0.emb_layers.1.bias'),\n ('out_layers.14.group_norm.weight', 'output_blocks.14.0.out_layers.0.weight'),\n ('out_layers.14.group_norm.bias', 'output_blocks.14.0.out_layers.0.bias'),\n ('out_layers.14.out_layers.2.weight',\n  'output_blocks.14.0.out_layers.3.weight'),\n ('out_layers.14.out_layers.2.bias', 'output_blocks.14.0.out_layers.3.bias'),\n ('out_layers.14.skip_connection.weight',\n  'output_blocks.14.0.skip_connection.weight'),\n ('out_layers.14.skip_connection.bias',\n  'output_blocks.14.0.skip_connection.bias'),\n ('out_layers.15.in_layers.0.weight', 'output_blocks.15.0.in_layers.0.weight'),\n ('out_layers.15.in_layers.0.bias', 'output_blocks.15.0.in_layers.0.bias'),\n ('out_layers.15.in_layers.2.weight', 'output_blocks.15.0.in_layers.2.weight'),\n ('out_layers.15.in_layers.2.bias', 'output_blocks.15.0.in_layers.2.bias'),\n ('out_layers.15.emb_layers.1.weight',\n  'output_blocks.15.0.emb_layers.1.weight'),\n ('out_layers.15.emb_layers.1.bias', 'output_blocks.15.0.emb_layers.1.bias'),\n ('out_layers.15.group_norm.weight', 'output_blocks.15.0.out_layers.0.weight'),\n ('out_layers.15.group_norm.bias', 'output_blocks.15.0.out_layers.0.bias'),\n ('out_layers.15.out_layers.2.weight',\n  'output_blocks.15.0.out_layers.3.weight'),\n ('out_layers.15.out_layers.2.bias', 'output_blocks.15.0.out_layers.3.bias'),\n ('out_layers.15.skip_connection.weight',\n  'output_blocks.15.0.skip_connection.weight'),\n ('out_layers.15.skip_connection.bias',\n  'output_blocks.15.0.skip_connection.bias'),\n ('out.0.weight', 'out.0.weight'),\n ('out.0.bias', 'out.0.bias'),\n ('out.1.weight', 'out.2.weight'),\n ('out.1.bias', 'out.2.bias'),\n ('transformer.resblocks.0.attn.c_qkv.weight',\n  'transformer.resblocks.0.attn.c_qkv.weight'),\n ('transformer.resblocks.0.attn.c_qkv.bias',\n  'transformer.resblocks.0.attn.c_qkv.bias'),\n ('transformer.resblocks.0.attn.c_proj.weight',\n  'transformer.resblocks.0.attn.c_proj.weight'),\n ('transformer.resblocks.0.attn.c_proj.bias',\n  'transformer.resblocks.0.attn.c_proj.bias'),\n ('transformer.resblocks.0.ln_1.weight',\n  'transformer.resblocks.0.ln_1.weight'),\n ('transformer.resblocks.0.ln_1.bias', 'transformer.resblocks.0.ln_1.bias'),\n ('transformer.resblocks.0.mlp.c_fc.weight',\n  'transformer.resblocks.0.mlp.c_fc.weight'),\n ('transformer.resblocks.0.mlp.c_fc.bias',\n  'transformer.resblocks.0.mlp.c_fc.bias'),\n ('transformer.resblocks.0.mlp.c_proj.weight',\n  'transformer.resblocks.0.mlp.c_proj.weight'),\n ('transformer.resblocks.0.mlp.c_proj.bias',\n  'transformer.resblocks.0.mlp.c_proj.bias'),\n ('transformer.resblocks.0.ln_2.weight',\n  'transformer.resblocks.0.ln_2.weight'),\n ('transformer.resblocks.0.ln_2.bias', 'transformer.resblocks.0.ln_2.bias'),\n ('transformer.resblocks.1.attn.c_qkv.weight',\n  'transformer.resblocks.1.attn.c_qkv.weight'),\n ('transformer.resblocks.1.attn.c_qkv.bias',\n  'transformer.resblocks.1.attn.c_qkv.bias'),\n ('transformer.resblocks.1.attn.c_proj.weight',\n  'transformer.resblocks.1.attn.c_proj.weight'),\n ('transformer.resblocks.1.attn.c_proj.bias',\n  'transformer.resblocks.1.attn.c_proj.bias'),\n ('transformer.resblocks.1.ln_1.weight',\n  'transformer.resblocks.1.ln_1.weight'),\n ('transformer.resblocks.1.ln_1.bias', 'transformer.resblocks.1.ln_1.bias'),\n ('transformer.resblocks.1.mlp.c_fc.weight',\n  'transformer.resblocks.1.mlp.c_fc.weight'),\n ('transformer.resblocks.1.mlp.c_fc.bias',\n  'transformer.resblocks.1.mlp.c_fc.bias'),\n ('transformer.resblocks.1.mlp.c_proj.weight',\n  'transformer.resblocks.1.mlp.c_proj.weight'),\n ('transformer.resblocks.1.mlp.c_proj.bias',\n  'transformer.resblocks.1.mlp.c_proj.bias'),\n ('transformer.resblocks.1.ln_2.weight',\n  'transformer.resblocks.1.ln_2.weight'),\n ('transformer.resblocks.1.ln_2.bias', 'transformer.resblocks.1.ln_2.bias'),\n ('transformer.resblocks.2.attn.c_qkv.weight',\n  'transformer.resblocks.2.attn.c_qkv.weight'),\n ('transformer.resblocks.2.attn.c_qkv.bias',\n  'transformer.resblocks.2.attn.c_qkv.bias'),\n ('transformer.resblocks.2.attn.c_proj.weight',\n  'transformer.resblocks.2.attn.c_proj.weight'),\n ('transformer.resblocks.2.attn.c_proj.bias',\n  'transformer.resblocks.2.attn.c_proj.bias'),\n ('transformer.resblocks.2.ln_1.weight',\n  'transformer.resblocks.2.ln_1.weight'),\n ('transformer.resblocks.2.ln_1.bias', 'transformer.resblocks.2.ln_1.bias'),\n ('transformer.resblocks.2.mlp.c_fc.weight',\n  'transformer.resblocks.2.mlp.c_fc.weight'),\n ('transformer.resblocks.2.mlp.c_fc.bias',\n  'transformer.resblocks.2.mlp.c_fc.bias'),\n ('transformer.resblocks.2.mlp.c_proj.weight',\n  'transformer.resblocks.2.mlp.c_proj.weight'),\n ('transformer.resblocks.2.mlp.c_proj.bias',\n  'transformer.resblocks.2.mlp.c_proj.bias'),\n ('transformer.resblocks.2.ln_2.weight',\n  'transformer.resblocks.2.ln_2.weight'),\n ('transformer.resblocks.2.ln_2.bias', 'transformer.resblocks.2.ln_2.bias'),\n ('transformer.resblocks.3.attn.c_qkv.weight',\n  'transformer.resblocks.3.attn.c_qkv.weight'),\n ('transformer.resblocks.3.attn.c_qkv.bias',\n  'transformer.resblocks.3.attn.c_qkv.bias'),\n ('transformer.resblocks.3.attn.c_proj.weight',\n  'transformer.resblocks.3.attn.c_proj.weight'),\n ('transformer.resblocks.3.attn.c_proj.bias',\n  'transformer.resblocks.3.attn.c_proj.bias'),\n ('transformer.resblocks.3.ln_1.weight',\n  'transformer.resblocks.3.ln_1.weight'),\n ('transformer.resblocks.3.ln_1.bias', 'transformer.resblocks.3.ln_1.bias'),\n ('transformer.resblocks.3.mlp.c_fc.weight',\n  'transformer.resblocks.3.mlp.c_fc.weight'),\n ('transformer.resblocks.3.mlp.c_fc.bias',\n  'transformer.resblocks.3.mlp.c_fc.bias'),\n ('transformer.resblocks.3.mlp.c_proj.weight',\n  'transformer.resblocks.3.mlp.c_proj.weight'),\n ('transformer.resblocks.3.mlp.c_proj.bias',\n  'transformer.resblocks.3.mlp.c_proj.bias'),\n ('transformer.resblocks.3.ln_2.weight',\n  'transformer.resblocks.3.ln_2.weight'),\n ('transformer.resblocks.3.ln_2.bias', 'transformer.resblocks.3.ln_2.bias'),\n ('transformer.resblocks.4.attn.c_qkv.weight',\n  'transformer.resblocks.4.attn.c_qkv.weight'),\n ('transformer.resblocks.4.attn.c_qkv.bias',\n  'transformer.resblocks.4.attn.c_qkv.bias'),\n ('transformer.resblocks.4.attn.c_proj.weight',\n  'transformer.resblocks.4.attn.c_proj.weight'),\n ('transformer.resblocks.4.attn.c_proj.bias',\n  'transformer.resblocks.4.attn.c_proj.bias'),\n ('transformer.resblocks.4.ln_1.weight',\n  'transformer.resblocks.4.ln_1.weight'),\n ('transformer.resblocks.4.ln_1.bias', 'transformer.resblocks.4.ln_1.bias'),\n ('transformer.resblocks.4.mlp.c_fc.weight',\n  'transformer.resblocks.4.mlp.c_fc.weight'),\n ('transformer.resblocks.4.mlp.c_fc.bias',\n  'transformer.resblocks.4.mlp.c_fc.bias'),\n ('transformer.resblocks.4.mlp.c_proj.weight',\n  'transformer.resblocks.4.mlp.c_proj.weight'),\n ('transformer.resblocks.4.mlp.c_proj.bias',\n  'transformer.resblocks.4.mlp.c_proj.bias'),\n ('transformer.resblocks.4.ln_2.weight',\n  'transformer.resblocks.4.ln_2.weight'),\n ('transformer.resblocks.4.ln_2.bias', 'transformer.resblocks.4.ln_2.bias'),\n ('transformer.resblocks.5.attn.c_qkv.weight',\n  'transformer.resblocks.5.attn.c_qkv.weight'),\n ('transformer.resblocks.5.attn.c_qkv.bias',\n  'transformer.resblocks.5.attn.c_qkv.bias'),\n ('transformer.resblocks.5.attn.c_proj.weight',\n  'transformer.resblocks.5.attn.c_proj.weight'),\n ('transformer.resblocks.5.attn.c_proj.bias',\n  'transformer.resblocks.5.attn.c_proj.bias'),\n ('transformer.resblocks.5.ln_1.weight',\n  'transformer.resblocks.5.ln_1.weight'),\n ('transformer.resblocks.5.ln_1.bias', 'transformer.resblocks.5.ln_1.bias'),\n ('transformer.resblocks.5.mlp.c_fc.weight',\n  'transformer.resblocks.5.mlp.c_fc.weight'),\n ('transformer.resblocks.5.mlp.c_fc.bias',\n  'transformer.resblocks.5.mlp.c_fc.bias'),\n ('transformer.resblocks.5.mlp.c_proj.weight',\n  'transformer.resblocks.5.mlp.c_proj.weight'),\n ('transformer.resblocks.5.mlp.c_proj.bias',\n  'transformer.resblocks.5.mlp.c_proj.bias'),\n ('transformer.resblocks.5.ln_2.weight',\n  'transformer.resblocks.5.ln_2.weight'),\n ('transformer.resblocks.5.ln_2.bias', 'transformer.resblocks.5.ln_2.bias'),\n ('transformer.resblocks.6.attn.c_qkv.weight',\n  'transformer.resblocks.6.attn.c_qkv.weight'),\n ('transformer.resblocks.6.attn.c_qkv.bias',\n  'transformer.resblocks.6.attn.c_qkv.bias'),\n ('transformer.resblocks.6.attn.c_proj.weight',\n  'transformer.resblocks.6.attn.c_proj.weight'),\n ('transformer.resblocks.6.attn.c_proj.bias',\n  'transformer.resblocks.6.attn.c_proj.bias'),\n ('transformer.resblocks.6.ln_1.weight',\n  'transformer.resblocks.6.ln_1.weight'),\n ('transformer.resblocks.6.ln_1.bias', 'transformer.resblocks.6.ln_1.bias'),\n ('transformer.resblocks.6.mlp.c_fc.weight',\n  'transformer.resblocks.6.mlp.c_fc.weight'),\n ('transformer.resblocks.6.mlp.c_fc.bias',\n  'transformer.resblocks.6.mlp.c_fc.bias'),\n ('transformer.resblocks.6.mlp.c_proj.weight',\n  'transformer.resblocks.6.mlp.c_proj.weight'),\n ('transformer.resblocks.6.mlp.c_proj.bias',\n  'transformer.resblocks.6.mlp.c_proj.bias'),\n ('transformer.resblocks.6.ln_2.weight',\n  'transformer.resblocks.6.ln_2.weight'),\n ('transformer.resblocks.6.ln_2.bias', 'transformer.resblocks.6.ln_2.bias'),\n ('transformer.resblocks.7.attn.c_qkv.weight',\n  'transformer.resblocks.7.attn.c_qkv.weight'),\n ('transformer.resblocks.7.attn.c_qkv.bias',\n  'transformer.resblocks.7.attn.c_qkv.bias'),\n ('transformer.resblocks.7.attn.c_proj.weight',\n  'transformer.resblocks.7.attn.c_proj.weight'),\n ('transformer.resblocks.7.attn.c_proj.bias',\n  'transformer.resblocks.7.attn.c_proj.bias'),\n ('transformer.resblocks.7.ln_1.weight',\n  'transformer.resblocks.7.ln_1.weight'),\n ('transformer.resblocks.7.ln_1.bias', 'transformer.resblocks.7.ln_1.bias'),\n ('transformer.resblocks.7.mlp.c_fc.weight',\n  'transformer.resblocks.7.mlp.c_fc.weight'),\n ('transformer.resblocks.7.mlp.c_fc.bias',\n  'transformer.resblocks.7.mlp.c_fc.bias'),\n ('transformer.resblocks.7.mlp.c_proj.weight',\n  'transformer.resblocks.7.mlp.c_proj.weight'),\n ('transformer.resblocks.7.mlp.c_proj.bias',\n  'transformer.resblocks.7.mlp.c_proj.bias'),\n ('transformer.resblocks.7.ln_2.weight',\n  'transformer.resblocks.7.ln_2.weight'),\n ('transformer.resblocks.7.ln_2.bias', 'transformer.resblocks.7.ln_2.bias'),\n ('transformer.resblocks.8.attn.c_qkv.weight',\n  'transformer.resblocks.8.attn.c_qkv.weight'),\n ('transformer.resblocks.8.attn.c_qkv.bias',\n  'transformer.resblocks.8.attn.c_qkv.bias'),\n ('transformer.resblocks.8.attn.c_proj.weight',\n  'transformer.resblocks.8.attn.c_proj.weight'),\n ('transformer.resblocks.8.attn.c_proj.bias',\n  'transformer.resblocks.8.attn.c_proj.bias'),\n ('transformer.resblocks.8.ln_1.weight',\n  'transformer.resblocks.8.ln_1.weight'),\n ('transformer.resblocks.8.ln_1.bias', 'transformer.resblocks.8.ln_1.bias'),\n ('transformer.resblocks.8.mlp.c_fc.weight',\n  'transformer.resblocks.8.mlp.c_fc.weight'),\n ('transformer.resblocks.8.mlp.c_fc.bias',\n  'transformer.resblocks.8.mlp.c_fc.bias'),\n ('transformer.resblocks.8.mlp.c_proj.weight',\n  'transformer.resblocks.8.mlp.c_proj.weight'),\n ('transformer.resblocks.8.mlp.c_proj.bias',\n  'transformer.resblocks.8.mlp.c_proj.bias'),\n ('transformer.resblocks.8.ln_2.weight',\n  'transformer.resblocks.8.ln_2.weight'),\n ('transformer.resblocks.8.ln_2.bias', 'transformer.resblocks.8.ln_2.bias'),\n ('transformer.resblocks.9.attn.c_qkv.weight',\n  'transformer.resblocks.9.attn.c_qkv.weight'),\n ('transformer.resblocks.9.attn.c_qkv.bias',\n  'transformer.resblocks.9.attn.c_qkv.bias'),\n ('transformer.resblocks.9.attn.c_proj.weight',\n  'transformer.resblocks.9.attn.c_proj.weight'),\n ('transformer.resblocks.9.attn.c_proj.bias',\n  'transformer.resblocks.9.attn.c_proj.bias'),\n ('transformer.resblocks.9.ln_1.weight',\n  'transformer.resblocks.9.ln_1.weight'),\n ('transformer.resblocks.9.ln_1.bias', 'transformer.resblocks.9.ln_1.bias'),\n ('transformer.resblocks.9.mlp.c_fc.weight',\n  'transformer.resblocks.9.mlp.c_fc.weight'),\n ('transformer.resblocks.9.mlp.c_fc.bias',\n  'transformer.resblocks.9.mlp.c_fc.bias'),\n ('transformer.resblocks.9.mlp.c_proj.weight',\n  'transformer.resblocks.9.mlp.c_proj.weight'),\n ('transformer.resblocks.9.mlp.c_proj.bias',\n  'transformer.resblocks.9.mlp.c_proj.bias'),\n ('transformer.resblocks.9.ln_2.weight',\n  'transformer.resblocks.9.ln_2.weight'),\n ('transformer.resblocks.9.ln_2.bias', 'transformer.resblocks.9.ln_2.bias'),\n ('transformer.resblocks.10.attn.c_qkv.weight',\n  'transformer.resblocks.10.attn.c_qkv.weight'),\n ('transformer.resblocks.10.attn.c_qkv.bias',\n  'transformer.resblocks.10.attn.c_qkv.bias'),\n ('transformer.resblocks.10.attn.c_proj.weight',\n  'transformer.resblocks.10.attn.c_proj.weight'),\n ('transformer.resblocks.10.attn.c_proj.bias',\n  'transformer.resblocks.10.attn.c_proj.bias'),\n ('transformer.resblocks.10.ln_1.weight',\n  'transformer.resblocks.10.ln_1.weight'),\n ('transformer.resblocks.10.ln_1.bias', 'transformer.resblocks.10.ln_1.bias'),\n ('transformer.resblocks.10.mlp.c_fc.weight',\n  'transformer.resblocks.10.mlp.c_fc.weight'),\n ('transformer.resblocks.10.mlp.c_fc.bias',\n  'transformer.resblocks.10.mlp.c_fc.bias'),\n ('transformer.resblocks.10.mlp.c_proj.weight',\n  'transformer.resblocks.10.mlp.c_proj.weight'),\n ('transformer.resblocks.10.mlp.c_proj.bias',\n  'transformer.resblocks.10.mlp.c_proj.bias'),\n ('transformer.resblocks.10.ln_2.weight',\n  'transformer.resblocks.10.ln_2.weight'),\n ('transformer.resblocks.10.ln_2.bias', 'transformer.resblocks.10.ln_2.bias'),\n ('transformer.resblocks.11.attn.c_qkv.weight',\n  'transformer.resblocks.11.attn.c_qkv.weight'),\n ('transformer.resblocks.11.attn.c_qkv.bias',\n  'transformer.resblocks.11.attn.c_qkv.bias'),\n ('transformer.resblocks.11.attn.c_proj.weight',\n  'transformer.resblocks.11.attn.c_proj.weight'),\n ('transformer.resblocks.11.attn.c_proj.bias',\n  'transformer.resblocks.11.attn.c_proj.bias'),\n ('transformer.resblocks.11.ln_1.weight',\n  'transformer.resblocks.11.ln_1.weight'),\n ('transformer.resblocks.11.ln_1.bias', 'transformer.resblocks.11.ln_1.bias'),\n ('transformer.resblocks.11.mlp.c_fc.weight',\n  'transformer.resblocks.11.mlp.c_fc.weight'),\n ('transformer.resblocks.11.mlp.c_fc.bias',\n  'transformer.resblocks.11.mlp.c_fc.bias'),\n ('transformer.resblocks.11.mlp.c_proj.weight',\n  'transformer.resblocks.11.mlp.c_proj.weight'),\n ('transformer.resblocks.11.mlp.c_proj.bias',\n  'transformer.resblocks.11.mlp.c_proj.bias'),\n ('transformer.resblocks.11.ln_2.weight',\n  'transformer.resblocks.11.ln_2.weight'),\n ('transformer.resblocks.11.ln_2.bias', 'transformer.resblocks.11.ln_2.bias'),\n ('transformer.resblocks.12.attn.c_qkv.weight',\n  'transformer.resblocks.12.attn.c_qkv.weight'),\n ('transformer.resblocks.12.attn.c_qkv.bias',\n  'transformer.resblocks.12.attn.c_qkv.bias'),\n ('transformer.resblocks.12.attn.c_proj.weight',\n  'transformer.resblocks.12.attn.c_proj.weight'),\n ('transformer.resblocks.12.attn.c_proj.bias',\n  'transformer.resblocks.12.attn.c_proj.bias'),\n ('transformer.resblocks.12.ln_1.weight',\n  'transformer.resblocks.12.ln_1.weight'),\n ('transformer.resblocks.12.ln_1.bias', 'transformer.resblocks.12.ln_1.bias'),\n ('transformer.resblocks.12.mlp.c_fc.weight',\n  'transformer.resblocks.12.mlp.c_fc.weight'),\n ('transformer.resblocks.12.mlp.c_fc.bias',\n  'transformer.resblocks.12.mlp.c_fc.bias'),\n ('transformer.resblocks.12.mlp.c_proj.weight',\n  'transformer.resblocks.12.mlp.c_proj.weight'),\n ('transformer.resblocks.12.mlp.c_proj.bias',\n  'transformer.resblocks.12.mlp.c_proj.bias'),\n ('transformer.resblocks.12.ln_2.weight',\n  'transformer.resblocks.12.ln_2.weight'),\n ('transformer.resblocks.12.ln_2.bias', 'transformer.resblocks.12.ln_2.bias'),\n ('transformer.resblocks.13.attn.c_qkv.weight',\n  'transformer.resblocks.13.attn.c_qkv.weight'),\n ('transformer.resblocks.13.attn.c_qkv.bias',\n  'transformer.resblocks.13.attn.c_qkv.bias'),\n ('transformer.resblocks.13.attn.c_proj.weight',\n  'transformer.resblocks.13.attn.c_proj.weight'),\n ('transformer.resblocks.13.attn.c_proj.bias',\n  'transformer.resblocks.13.attn.c_proj.bias'),\n ('transformer.resblocks.13.ln_1.weight',\n  'transformer.resblocks.13.ln_1.weight'),\n ('transformer.resblocks.13.ln_1.bias', 'transformer.resblocks.13.ln_1.bias'),\n ('transformer.resblocks.13.mlp.c_fc.weight',\n  'transformer.resblocks.13.mlp.c_fc.weight'),\n ('transformer.resblocks.13.mlp.c_fc.bias',\n  'transformer.resblocks.13.mlp.c_fc.bias'),\n ('transformer.resblocks.13.mlp.c_proj.weight',\n  'transformer.resblocks.13.mlp.c_proj.weight'),\n ('transformer.resblocks.13.mlp.c_proj.bias',\n  'transformer.resblocks.13.mlp.c_proj.bias'),\n ('transformer.resblocks.13.ln_2.weight',\n  'transformer.resblocks.13.ln_2.weight'),\n ('transformer.resblocks.13.ln_2.bias', 'transformer.resblocks.13.ln_2.bias'),\n ('transformer.resblocks.14.attn.c_qkv.weight',\n  'transformer.resblocks.14.attn.c_qkv.weight'),\n ('transformer.resblocks.14.attn.c_qkv.bias',\n  'transformer.resblocks.14.attn.c_qkv.bias'),\n ('transformer.resblocks.14.attn.c_proj.weight',\n  'transformer.resblocks.14.attn.c_proj.weight'),\n ('transformer.resblocks.14.attn.c_proj.bias',\n  'transformer.resblocks.14.attn.c_proj.bias'),\n ('transformer.resblocks.14.ln_1.weight',\n  'transformer.resblocks.14.ln_1.weight'),\n ('transformer.resblocks.14.ln_1.bias', 'transformer.resblocks.14.ln_1.bias'),\n ('transformer.resblocks.14.mlp.c_fc.weight',\n  'transformer.resblocks.14.mlp.c_fc.weight'),\n ('transformer.resblocks.14.mlp.c_fc.bias',\n  'transformer.resblocks.14.mlp.c_fc.bias'),\n ('transformer.resblocks.14.mlp.c_proj.weight',\n  'transformer.resblocks.14.mlp.c_proj.weight'),\n ('transformer.resblocks.14.mlp.c_proj.bias',\n  'transformer.resblocks.14.mlp.c_proj.bias'),\n ('transformer.resblocks.14.ln_2.weight',\n  'transformer.resblocks.14.ln_2.weight'),\n ('transformer.resblocks.14.ln_2.bias', 'transformer.resblocks.14.ln_2.bias'),\n ('transformer.resblocks.15.attn.c_qkv.weight',\n  'transformer.resblocks.15.attn.c_qkv.weight'),\n ('transformer.resblocks.15.attn.c_qkv.bias',\n  'transformer.resblocks.15.attn.c_qkv.bias'),\n ('transformer.resblocks.15.attn.c_proj.weight',\n  'transformer.resblocks.15.attn.c_proj.weight'),\n ('transformer.resblocks.15.attn.c_proj.bias',\n  'transformer.resblocks.15.attn.c_proj.bias'),\n ('transformer.resblocks.15.ln_1.weight',\n  'transformer.resblocks.15.ln_1.weight'),\n ('transformer.resblocks.15.ln_1.bias', 'transformer.resblocks.15.ln_1.bias'),\n ('transformer.resblocks.15.mlp.c_fc.weight',\n  'transformer.resblocks.15.mlp.c_fc.weight'),\n ('transformer.resblocks.15.mlp.c_fc.bias',\n  'transformer.resblocks.15.mlp.c_fc.bias'),\n ('transformer.resblocks.15.mlp.c_proj.weight',\n  'transformer.resblocks.15.mlp.c_proj.weight'),\n ('transformer.resblocks.15.mlp.c_proj.bias',\n  'transformer.resblocks.15.mlp.c_proj.bias'),\n ('transformer.resblocks.15.ln_2.weight',\n  'transformer.resblocks.15.ln_2.weight'),\n ('transformer.resblocks.15.ln_2.bias', 'transformer.resblocks.15.ln_2.bias'),\n ('final_ln.weight', 'final_ln.weight'),\n ('final_ln.bias', 'final_ln.bias'),\n ('token_embedding.weight', 'token_embedding.weight'),\n ('transformer_proj.weight', 'transformer_proj.weight'),\n ('transformer_proj.bias', 'transformer_proj.bias')]"
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(len(model.state_dict().keys()), len(ref_model.state_dict().keys()))\n",
    "list(zip(model.state_dict().keys(), ref_model.state_dict().keys()))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "data": {
      "text/plain": "<All keys matched successfully>"
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "state = ref_model.state_dict()\n",
    "mapped = {ours: state[theirs] for ours, theirs in zip(model.state_dict().keys(), ref_model.state_dict().keys())}\n",
    "\n",
    "model.load_state_dict(mapped)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}